{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性回归是一种用来对于连续性变量进行预测的办法，下面所说的连续型和离散型是针对结果应变量来说的，而不是针对自变量来说的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1 连续型-应变量(y)\n",
    "回归算法-线性回归、广义线性回归、CART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2 离散型-应变量(y)\n",
    "分类算法-逻辑回归、SVM、决策树、CART、KNN、朴素贝叶斯等等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3 线性回归\n",
    "不独立的两个随机变量，二者之间肯定会存在某种关系：\n",
    "> - 确定性关系：函数关系\n",
    "- 不确定性关系：线性关系、非线性关系\n",
    "\n",
    "相关关系不是因果关系，不相关表示没有**线性**关系，独立表示没有任何关系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#4 协方差和相关系数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4.1 期望\n",
    "$E(X)=\\frac{\\sum_{i=1}^nX}{n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4.2 协方差\n",
    "Cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Cov(X,Y)=E[(X-E(X))*(Y-E(Y))]=\\sum_{i=1}^n(X_i-\\frac{\\sum_{i=1}^nXi}{n})*(Y_i-\\frac{\\sum_{i=1}^nYi}{n})=E[XY]-E[X]E[Y]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设有n个特征，表达式如下$h(\\Theta)=\\Theta_0+\\Theta_1x_1+\\Theta_2x_2+...+\\Theta_nx_n$那么\n",
    "$$\n",
    "Repeat\\{\n",
    "   \\Theta_j := \\Theta_j - \\alpha\\frac{1}{m}\\sum_{i=1}^m(h_{\\Theta}(x^{(i)})-y^{(i)})x_j^{(i)}, j={0,1,...,n}\n",
    "\\}\n",
    "$$\n",
    "\n",
    "$\\Theta_0 := \\Theta_0 - \\alpha\\frac{1}{m}\\sum_{i=1}^m(h_{\\Theta}(x^{(i)})-y^{(i)})x_0^{(i)}$\n",
    "\n",
    "$\\Theta_1 := \\Theta_1 - \\alpha\\frac{1}{m}\\sum_{i=1}^m(h_{\\Theta}(x^{(i)})-y^{(i)})x_1^{(i)}$\n",
    "\n",
    "$\\Theta_2 := \\Theta_2 - \\alpha\\frac{1}{m}\\sum_{i=1}^m(h_{\\Theta}(x^{(i)})-y^{(i)})x_2^{(i)}$\n",
    "\n",
    "![images](images/06/19.png)\n",
    "这是什么意思，首先给$\\Theta_0,\\Theta_1,...\\Theta_n$设置初始值，基本都是1，然后中间部分一样，这个值就是用这些初始值带入m个向量，可以得到m个值，用这m个值分别减去它们对应的y值，然后用这个值分别乘以对应记录的当前向量特征的值($\\Theta_0$就是$X_0$，$X_0$没有，就是1，$\\Theta_1$就是当前记录的第一个特征的值)然后将这m个值求和,然后乘以步长，乘以$\\frac{1}{m}$,这个值我们可以叫他$\\delta$。对于$\\Theta_0$来说，他的下一个值就是$\\Theta_0-\\delta$，对于$\\Theta_1$来说，他的下一个值就是$\\Theta_1-\\delta$...直到前后两次的高度差小于一定范围"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4.3 相关系数\n",
    "Cor,取值范围$[-1,+1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\rho_{xy}=\\frac{Cov(X,Y)}{\\sqrt{D(X)}\\sqrt{D(Y)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#5 线性回归的解法\n",
    "解决线性回归的办法主要有最小二乘法和梯度下降法。\n",
    "- 最小二乘法可以计算出解析解，但是计算量很大，如果特征超过10000，那么效率会很低，因为涉及到矩阵的计算；\n",
    "- 梯度下降法可以求出数值解，适合于大数据量的求解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##5.1 数值优化\n",
    "- 解析解:精确解\n",
    "- 数值解：差不多的解，导致损失函数最小的那个解\n",
    "\n",
    "在机器学习中，只有线性回归能得到解析解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##5.2 线性回归的损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L(\\Theta)=\\sum_{i=1}^n(h_\\Theta(x_i)-y_i)^2$,我们需要使得L最小，其中x，y已知，需要求解h,其中$h_\\Theta(x_i)=\\Theta_1x_i+\\Theta_2$,我们上面只是考虑的一个特征的情况，最终我们需要求解$\\Theta_1$和$\\Theta_2$，使得损失函数最小.\n",
    "\n",
    "$L(\\Theta)=\\sum_{i=1}^n(\\Theta_1x_i+\\Theta_2-y_i)^2$.带入后，得到上述方程，那么画出曲线之后，会是一个开口向上的抛物面\n",
    "![images](images/06/14.png)\n",
    "\n",
    "每个样本距离线性回归曲线的误差，是独立并且具有相同的分布，通常认为服从均值为0方差为$\\Theta^2$的高斯分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##5.3 最小二乘法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\omega=\\frac{\\sum_{i=1}^my_i(x_i-\\overline{x})}{\\sum_{i=1}^mx_i^2-\\frac{1}{m}(\\sum_{i=1}^mx_i)^2}$\n",
    "\n",
    "$b=\\frac{1}{m}\\sum_{i=1}^m(y_i-\\omega{x}_i)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##5.4 梯度下降法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度方向是函数值变化最快的方向\n",
    "- 全微分：考虑所有自变量变化时，函数值的变化情况\n",
    "- 偏微分：假设其它自变量不变，考虑一个变量变化，函数值的变化情况\n",
    "\n",
    "$\\Delta{y}=\\sum_{i=1}^n\\frac{\\partial}{\\partial{x_i}}\\Delta{x_i}$\n",
    "- 梯度向量:所有变量偏微分组成的向量。梯度方向由L(θ)对θ的偏导数确定,所以我们需要沿着负梯度方向往下走\n",
    "\n",
    "梯度下降法的思路就是找到负梯度方向，然后不停的以一个很小的间隔去向它移动，直到前后两次的高度差小于一定范围的时候，停止，这个时候可以求出m和b,这就涉及到一个问题，这个很小的间隔怎么选取，我们管这个参数叫做$\\alpha$，一般定义为0.001，这个参数太大了，会在底部不停震荡，如果太小，那么迭代次数太多"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##5.5 多变量线性回归 - 多重线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###5.5.1 梯度下降法的一般形式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设有n个特征，表达式如下$h(\\Theta)=\\Theta_0+\\Theta_1x_1+\\Theta_2x_2+...+\\Theta_nx_n$那么\n",
    "$$\n",
    "Repeat\\{\n",
    "   \\Theta_j := \\Theta_j - \\alpha\\frac{1}{m}\\sum_{i=1}^m(h_{\\Theta}(x^{(i)})-y^{(i)})x_j^{(i)}, j={0,1,...,n}\n",
    "\\}\n",
    "$$\n",
    "\n",
    "$\\Theta_0 := \\Theta_0 - \\alpha\\frac{1}{m}\\sum_{i=1}^m(h_{\\Theta}(x^{(i)})-y^{(i)})x_0^{(i)}$\n",
    "\n",
    "$\\Theta_1 := \\Theta_1 - \\alpha\\frac{1}{m}\\sum_{i=1}^m(h_{\\Theta}(x^{(i)})-y^{(i)})x_1^{(i)}$\n",
    "\n",
    "$\\Theta_2 := \\Theta_2 - \\alpha\\frac{1}{m}\\sum_{i=1}^m(h_{\\Theta}(x^{(i)})-y^{(i)})x_2^{(i)}$\n",
    "\n",
    "这是什么意思，首先给$\\Theta_0,\\Theta_1,...\\Theta_n$设置初始值，基本都是1，然后中间部分一样，这个值就是用这些初始值带入m个向量，可以得到m个值，用这m个值分别减去它们对应的y值，然后用这个值分别乘以对应记录的当前向量特征的值($\\Theta_0$就是$X_0$，$X_0$没有，就是1，$\\Theta_1$就是当前记录的第一个特征的值)然后将这m个值求和,然后乘以步长，乘以$\\frac{1}{m}$,这个值我们可以叫他$\\delta$。对于$\\Theta_0$来说，他的下一个值就是$\\Theta_0-\\delta$，对于$\\Theta_1$来说，他的下一个值就是$\\Theta_1-\\delta$...直到前后两次的高度差小于一定范围"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###5.5.2 标准方程的一般形式\n",
    "![images](images/06/16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_1$ : 房子大小, $x_2$ : 卧室数量, $x_3$ : 楼层数量, $x_4$ : 房龄, y : 房子的价格，则有：\n",
    "\n",
    "$x_1=\\begin{bmatrix}\n",
    "2104\\\\\n",
    "1416\\\\\n",
    "1534\\\\\n",
    "852\\\\\n",
    "...\n",
    "\\end{bmatrix}$\n",
    "$x_2=\\begin{bmatrix}\n",
    "5\\\\\n",
    "3\\\\\n",
    "3\\\\\n",
    "2\\\\\n",
    "...\n",
    "\\end{bmatrix}$\n",
    "$x_3=\\begin{bmatrix}\n",
    "1\\\\\n",
    "2\\\\\n",
    "2\\\\\n",
    "1\\\\\n",
    "...\n",
    "\\end{bmatrix}$\n",
    "$x_4=\\begin{bmatrix}\n",
    "45\\\\\n",
    "40\\\\\n",
    "30\\\\\n",
    "36\\\\\n",
    "...\n",
    "\\end{bmatrix}$\n",
    "$y=\\begin{bmatrix}\n",
    "460\\\\\n",
    "232\\\\\n",
    "315\\\\\n",
    "178\\\\\n",
    "...\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$h_{\\Theta}(X)=\\Theta_0+\\Theta_1x_1+\\Theta_2x_2+...+\\Theta_nx_n$,这时，使用线性代数表示更为方便。我们假设，$x_0=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x=\\begin{bmatrix}\n",
    "x_1\\\\\n",
    "x_2\\\\\n",
    "x_3\\\\\n",
    "...\\\\\n",
    "x_n\n",
    "\\end{bmatrix}$\n",
    "$\\theta=\\begin{bmatrix}\n",
    "\\theta_1\\\\\n",
    "\\theta_2\\\\\n",
    "\\theta_3\\\\\n",
    "...\\\\\n",
    "\\theta_n\n",
    "\\end{bmatrix}$\n",
    "$h_{\\Theta}(X)=\\Theta^TX$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设训练数据有m个，特征有n个，则有(下面讲解假设$x_0=[x_0^1,x_0^2,...,x_0^n]=[1,1,...,1]$)\n",
    "\n",
    "$X=\\begin{bmatrix}\n",
    "x_0^T\\\\\n",
    "x_1^T\\\\\n",
    "x_2^T\\\\\n",
    "...\\\\\n",
    "x_n^T\n",
    "\\end{bmatrix}=\n",
    "\\begin{bmatrix}\n",
    "1&x_1^1&x_2^1&...&x_n^1\\\\\n",
    "1&x_1^2&x_2^2&...&x_n^2\\\\\n",
    "\\vdots&\\vdots&\\vdots&\\vdots&\\vdots\\\\\n",
    "1&x_1^m&x_2^m&...&x_n^m\n",
    "\\end{bmatrix}\n",
    "$m * (n+1)的矩阵\n",
    "$\\theta=\\begin{bmatrix}\n",
    "\\theta_0\\\\\n",
    "\\theta_1\\\\\n",
    "\\theta_2\\\\\n",
    "\\theta_3\\\\\n",
    "...\\\\\n",
    "\\theta_n\n",
    "\\end{bmatrix}$(n+1) * 1的矩阵\n",
    "$y=\\begin{bmatrix}\n",
    "y_0\\\\\n",
    "y_1\\\\\n",
    "y_2\\\\\n",
    "y_3\\\\\n",
    "...\\\\\n",
    "y_n\n",
    "\\end{bmatrix}$(n+1) * 1的矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由线性代数运算，可得：$\\Theta=(X^TX)^{-1}X^Ty$,也就是X的转置乘以X，然后取逆矩阵，在乘以X的转置，在乘以y\n",
    "\n",
    "但是有个问题，如果X的转置乘以X，不能求逆矩阵怎么办，也就是说它不是一个正矩阵(行列相等)怎么办？\n",
    "- 导致无法求逆矩阵的情况有两个\n",
    "    - 变量之间存在线性关系\n",
    "    - 变量数量大于记录数\n",
    "\n",
    "解决这个问题，需要依靠岭回归、Lasso回归或者弹性网来解决"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}