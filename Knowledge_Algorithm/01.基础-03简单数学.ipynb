{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 简单数学\n",
    "包括微积分、线性代数和矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1 微积分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1.1 导数\n",
    "导数的定义：$f'(a)=\\lim\\limits_{h\\to{0}}\\frac{f(a+h)-f(a)}{h}$,h是变化率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1.2 常见导数\n",
    "$(x^a)'=ax^{a-1}$\n",
    "\n",
    "$(e^x)'=e^x$\n",
    "\n",
    "$(a^x)'=ln(a)a^x$\n",
    "\n",
    "$(ln(x))'=\\frac{1}{x}$\n",
    "\n",
    "$\\frac{d}{dx}sin(x)=cos(x)$\n",
    "\n",
    "$\\frac{d}{dx}cos(x)=-sin(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1.3 导数法则\n",
    "加法法则：$(\\alpha{f}+\\beta{g})'=\\alpha{f}'+\\beta{g}'$\n",
    "\n",
    "乘法法则：$(fg)'=f'g+fg'$\n",
    "\n",
    "除法法则：$(\\frac{f}{g})'=\\frac{f'g-fg'}{g^2}$\n",
    "\n",
    "复合函数：$f(x)=h(g(x)), f'(x)=h'(g(x)) * g'(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算$f(x)=x^4+sin(x^2)-ln(x)e^x+7$的导数\n",
    "\n",
    "$f'(x)=4x^3+cos(x^2) * 2x - \\frac{1}{x}e^x - e^xln(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1.4 梯度和Hession矩阵(以下都假设函数连续可导)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1.4.1 一阶导数和梯度\n",
    "一元函数$f'(x)$,x为标量\n",
    "\n",
    "多元函数$\\nabla{f(X)}=\\frac{\\partial{f(X)}}{\\partial{X}}=\\begin{bmatrix}\n",
    "\\frac{\\partial{f(X)}}{\\partial{x_1}}\\\\\n",
    "\\frac{\\partial{f(X)}}{\\partial{x_2}}\\\\\n",
    "\\vdots\\\\\n",
    "\\frac{\\partial{f(X)}}{\\partial{x_n}}\n",
    "\\end{bmatrix}$,X为矢量,求偏导的时候，需要把其它的变量当成常数,这就是梯度，对矢量求导，也就是求梯度的过程\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###1.4.2 二阶导数和Hession矩阵\n",
    "一元函数的二阶导数:$f''(x)$\n",
    "\n",
    "多元函数的二阶导数：$\\nabla^2f(X)=\\begin{bmatrix}\n",
    "\\frac{\\partial^2{f(X)}}{\\partial{x_1}{\\partial{x_1}}}&\\frac{\\partial^2{f(X)}}{\\partial{x_1}{\\partial{x_2}}}&\\dots&\\frac{\\partial^2{f(X)}}{\\partial{x_1}{\\partial{x_n}}}\\\\\n",
    "\\frac{\\partial^2{f(X)}}{\\partial{x_2}{\\partial{x_1}}}&\\frac{\\partial^2{f(X)}}{\\partial{x_2}{\\partial{x_2}}}&\\dots&\\frac{\\partial^2{f(X)}}{\\partial{x_2}{\\partial{x_n}}}\\\\\n",
    "\\vdots&\\vdots&\\vdots&\\vdots\\\\\n",
    "\\frac{\\partial^2{f(X)}}{\\partial{x_n}{\\partial{x_1}}}&\\frac{\\partial^2{f(X)}}{\\partial{x_n}{\\partial{x_2}}}&\\dots&\\frac{\\partial^2{f(X)}}{\\partial{x_n}{\\partial{x_n}}}\n",
    "\\end{bmatrix}$,这就是Hession矩阵,对称矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1.5 二次型的矩阵表示\n",
    "二次型就是二次多项式，可以简单使用矩阵$X^TAX$标识，比如$x_1^2+x_2^2$可以表示为$\\begin{bmatrix}\n",
    "x_1\\\\\n",
    "x_2\n",
    "\\end{bmatrix}^T$A$\\begin{bmatrix}\n",
    "x_1\\\\\n",
    "x_2\n",
    "\\end{bmatrix}$,A是一个单位矩阵，这样写法比较简单\n",
    "\n",
    "那么二次型求梯度，则有$\\frac{\\partial{X^TAX}}{\\partial{X}}=2AX$,这样求梯度会比直接对二项式求梯度简单"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1.6 泰勒级数与极值\n",
    "f(x)在$x_k$处做泰勒级数展开，有$f(x_k+\\delta)\\approx{f(x_k)}+f'(x_k)\\delta+\\frac{f''(x_k)}{2!}\\delta^2+...+\\frac{f^{(n)}(x_k)}{n!}\\delta^n+R_n(x)$，其中$\\delta$是一个很小的量，相当于$x_k$在周围很小的一个范围展开，如果$x_k=0$，也就是说在0处展开，则有$f(x)=f(0)+f'(0)x+\\frac{f''(0)}{2!}x^2+...+\\frac{f^{(n)}(0)}{n!}x^n+o(x^n)$，这个叫做Maclaurin公式.利用泰勒级数展开，可以求e的值\n",
    "\n",
    "令$f(x)=e^x,\\because (e^x)'=e^x, \\therefore f(x)=e^0+e^0x+\\frac{e^0}{2!}x^2+...+\\frac{e^0}{n!}x^n$,\n",
    "\n",
    "$\\therefore e^x=1+x+\\frac{x^2}{2!}+...\\frac{x^n}{n!}+R_n$，令x=1，则有\n",
    "\n",
    "$e=1+1+\\frac{1}{2!}+\\frac{1}{3!}+...+\\frac{1}{n!}$\n",
    "\n",
    "如果做加减乘除之类的算法，硬件可以直接算,如果计算sinx，或者cosx，或者$e^{0.01}$，只能通过泰勒级数展开来做"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一阶导数如果等于0，那么这个点可能是极值点\n",
    "\n",
    "称满足$f'(x_k)=0$的点为平稳点(候选点)，此时如果还有:\n",
    "- $f''(x_k) \\ge 0, x_k$为一严格局部极小点，反之为严格局部最大点\n",
    "- $f''(x_k)=0$，可能是一个鞍点(saddle point)\n",
    "\n",
    "总结，一阶导数等于0，可能有三种情况：\n",
    "- 局部最小值点，如果$f''(x_k) \\ge 0$，那么就是局部最小点\n",
    "- 局部最大值点，如果$f''(x_k) \\le 0$，那么就是局部最大点\n",
    "- 鞍点，如果$如果$f''(x_k) = 0$,那么就是鞍点，这个时候就要考虑三阶导数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入为矢量的泰勒级数展开$f(X_k+\\Delta)\\approx{f(X_k)}+\\nabla^Tf(X_k)\\Delta+\\frac{1}{2}\\Delta^T\\nabla^2f(X_k)\\Delta$,其中$X,\\Delta$都是矢量\n",
    "\n",
    "称满足$\\nabla^Tf(X_k)=0$的点为平稳点(候选点)，此时如果还有\n",
    "- $\\nabla^2f(X_k) \\succ 0,X_k$为一严格局部极小点，反之是严格局部最大点\n",
    "- 如果$\\nabla^2f(X_k)$为不定矩阵，那么就是一个鞍点\n",
    "\n",
    "由于$\\nabla^2f(X_k)$的结果是一个Hession矩阵，$\\succ 0$的意思就是这个Hession矩阵的所有特征值都是大于0的，这就是正定矩阵(那么这个函数就是凸函数)，如果是$\\nabla^2f(X_k) \\succeq 0$，那么就是半正定矩阵，也就是所有特征值都是大于等于0的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1.7 梯度下降法\n",
    "为了寻找极值点，我们需要对函数求一阶导数，但是很多情况下，函数的一阶导数等于0，根本无法求解出来，或者说求解起来极其复杂。那么我们就只能使用梯度下降法来求解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###1.7.1 为何梯度指向了函数最大增加的方向\n",
    "我们知道泰勒级数展开为$f(X_k+\\Delta)\\approx{f(X_k)}+\\nabla^Tf(X_k)\\Delta+\\frac{1}{2}\\Delta^T\\nabla^2f(X_k)\\Delta$，$\\Delta$相当于一个方向，那么我们选择一个方向往前走，当$\\Delta=\\nabla{f(X)}$的时候，即当$\\Delta$等于f(x)的梯度的时候，$\\nabla^Tf(X_k)\\Delta=\\nabla^Tf(X_k) * \\nabla{f(X_K)}$，相当于两个向量求内积,根据内积公式$a^Tb=|a|*|b|*cos\\theta$，当两个向量是平行同方向的时候最大，所以梯度执行了函数最大增加的方向。那么当两个向量是平行反方向的时候，也就是选择负梯度，那么就是函数下降最快的方向"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1.8 $\\Gamma$函数\n",
    "$\\Gamma(x)=(x-1) \\bullet \\Gamma(x-1) \\Rightarrow \\frac{\\Gamma(x)}{\\Gamma(x-1)}=x-1$\n",
    "\n",
    "$\\Gamma(x)=\\int_0^{+\\infty}t^{x-1}e^{-t}dt=(x-1)!$\n",
    "\n",
    "$\\Gamma$函数是阶乘在实数上的推广"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2. 矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2.1 特征值与特征向量\n",
    "如果给定一个方阵A，特征Ax=$\\lambda$x，A是个矩阵，x是个向量，$\\lambda$是个标量,如果等式成立，那么$\\lambda$就称为方阵A的特征值，x就称为A的特征向量。从几何意义上来说，Ax相当于在坐标系上对于原始向量x进行的旋转，如果旋转以后的向量跟原始向量在一个一条直线上，或者是同方向，或者是反方向，那么x就是A的特征向量，$\\lambda$就是对特征向量进行了缩放"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "举例：给定一个矩阵A=$\\begin{bmatrix}\n",
    "4&1\\\\\n",
    "1&4\n",
    "\\end{bmatrix}$,对于$x_1=\\begin{bmatrix}\n",
    "1\\\\\n",
    "0\n",
    "\\end{bmatrix}$,则有$Ax_1=\\begin{bmatrix}\n",
    "5\\\\\n",
    "0\n",
    "\\end{bmatrix}$;对于$Ax_3=\\begin{bmatrix}\n",
    "1\\\\\n",
    "1\n",
    "\\end{bmatrix}$，则有$Ax_3=5\\begin{bmatrix}\n",
    "1\\\\\n",
    "1\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2.2 特征分解\n",
    "对于$Ax_i=\\lambda{x_i}$，如果所有的特征值都不相同，则对应的所有的特征向量线性无关。此时，A可以被对角化为$A=V\\wedge V^{-1}$，其中V是一个可逆矩阵,$V=[x_1,x_2,...,x_n]$，$\\wedge=Diag(\\lambda_1,...,\\lambda_n)$。有一类矩阵一定可以被对角化，那就是对称矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "具体的应用就是PCA降维，PCA的本质就是协方差矩阵的想死对角化，KL变换。降维的目的是将多行的向量，降维少量行数的向量，那么目标就是同一行内的数据要尽量的分散，不同行之间的数据要尽量的减少相关性。这样才能更好的分辨原有数据.\n",
    "\n",
    "给定一个矩阵$X \\in R^{m*n}$，例如$X=\\begin{bmatrix}\n",
    "a_{11}&a_{12}&\\dots&a_{1n}\\\\\n",
    "a_{21}&a_{22}&\\dots&a_{2n}\\\\\n",
    "\\vdots&\\vdots&\\vdots&\\vdots\\\\\n",
    "a_{m1}&a_{mn2}&\\dots&a_{mn}\n",
    "\\end{bmatrix}$，选择k<m个正交基进行降维的同时又尽量保留原始的信息。即使得A变换到这组基后，使得行向量间的协方差为0，而每个行向量的方差尽可能大。0均值化后的协方差矩阵为$C_x=\\frac{1}{n}XX^T=\\begin{bmatrix}\n",
    "\\frac{1}{n}\\sum_{i=1}^na_i^2&\\frac{1}{n}\\sum_{i=1}^na_ib_i\\\\\n",
    "\\frac{1}{n}\\sum_{i=1}^na_ib_i&\\frac{1}{n}\\sum_{i=1}^nb_i^2\n",
    "\\end{bmatrix}$，那么我们就希望非对角线上的数都是0，所以a,b必须是垂直的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设变换矩阵为$Y=QX$，并先假设Q是方阵，则有$C_Y=\\frac{1}{n}YY^T=QC_XQ^T$，$\\because C_X=U\\wedge U^T \\Rightarrow \\wedge=U^TC_XU$，当$Q=U^T$的时候，$C_Y$是一个对角矩阵.\n",
    "\n",
    "PCA的本质就是协方差矩阵的对角化，对角化后我们把特征值大的保留下来，特征值小的丢弃，这样就可以实现降维的效果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "举例说明PCA降维：\n",
    "\n",
    "假设$X=\\begin{bmatrix}\n",
    "-1&-1&0&2&0\\\\\n",
    "-2&0&0&1&1\\\\\n",
    "\\end{bmatrix}$，需要讲X降维1行\n",
    "\n",
    "求解：首先求出协方差矩阵$C_X=\\begin{bmatrix}\n",
    "\\frac{6}{5}&\\frac{4}{5}\\\\\n",
    "\\frac{4}{5}&\\frac{6}{5}\n",
    "\\end{bmatrix}$,然后可以求出$C_X$的特征值为$\\lambda_1=2,\\lambda_2=\\frac{2}{5}$，特征向量为$\\begin{bmatrix}\n",
    "\\frac{1}{\\sqrt{2}}\\\\\n",
    "\\frac{1}{\\sqrt{2}}\n",
    "\\end{bmatrix}$,$\\begin{bmatrix}\n",
    "-\\frac{1}{\\sqrt{2}}\\\\\n",
    "\\frac{1}{\\sqrt{2}}\n",
    "\\end{bmatrix}$，因此U=$\\begin{bmatrix}\n",
    "\\frac{1}{\\sqrt{2}}&-\\frac{1}{\\sqrt{2}}\\\\\n",
    "\\frac{1}{\\sqrt{2}}&\\frac{1}{\\sqrt{2}}\n",
    "\\end{bmatrix}$，那么$U_T=\\begin{bmatrix}\n",
    "\\frac{1}{\\sqrt{2}}&\\frac{1}{\\sqrt{2}}\\\\\n",
    "-\\frac{1}{\\sqrt{2}}&\\frac{1}{\\sqrt{2}}\n",
    "\\end{bmatrix}$，这样就对角化了$C_Y$，降维的时候取他的第一行\n",
    "\n",
    "降维$\\begin{bmatrix}\n",
    "\\frac{1}{\\sqrt{2}}&\\frac{1}{\\sqrt{2}}\n",
    "\\end{bmatrix}X=\\begin{bmatrix}\n",
    "-\\frac{3}{\\sqrt{2}}&-\\frac{1}{\\sqrt{2}}&0&\\frac{3}{\\sqrt{2}}&\\frac{1}{\\sqrt{2}}\n",
    "\\end{bmatrix}$,此时可验证$C_Y=2=\\lambda_1$，根据$\\lambda$的大小排序，如果降维为m，那么就取最大的m个$\\lambda$"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}