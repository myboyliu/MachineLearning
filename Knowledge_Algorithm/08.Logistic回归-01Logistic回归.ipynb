{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#逻辑回归与最大似然"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "逻辑(Logistic)回归解决的是分类问题(应变量为离散型变量，自变量即可以是连续型，也可以是离散型)，最大似然是一种通用的解决概率问题的思想，两者之间的关系是线性回归和最小二乘法的关系。逻辑回归得到的是一个分割面，可以把数据一分为二"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1. 基本概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0-1事件就是一个事件发生的概率只有两种可能，那么如果我们假设1发生的概率为p，那么0发生的概率就是1-p，用一个统一个公式来表示，就是$P\\{X=x\\}=p^x(1-p)^{1-x}$,定义事件1发生的几率为$odds=\\frac{p}{1-p}$,对数几率z=ln(odds),那么可以推到出如下公式：$P\\{X=x\\}=\\frac{1}{1+e^{-z}}$\n",
    "\n",
    "伯努利分布是指数族分布，对于伯努利分布$Bernouli(\\varphi)$,其中$y \\in \\{0, 1\\}$，$\\therefore$Bernouli分布函数可以写成如下形式\n",
    "\n",
    "$P\\{X=x\\}=p^y(1-p)^{1-y}=e^{ln(p^y(1-p)^{1-y})}=e^{lnp^y+ln(1-p)^{1-y}}=e^{ylnp+(1-y)ln(1-p)}$\n",
    "\n",
    "$=e^{ylnp+ln(1-p)-yln(1-p)}=e^{y[lnp-ln(1-p)]+ln(1-p)}=e^{yln\\frac{p}{1-p}+ln(1-p)}$，推到这里，可以看到伯努利分布是指数族的分布\n",
    "\n",
    "令$\\varphi = ln\\frac{p}{1-p}$，有$e^{\\varphi}=\\frac{p}{1-p}, \\therefore p=\\frac{e^{\\varphi}}{1+e^{\\varphi}}$，$\\therefore p=\\frac{1}{1+e^{-\\varphi}}$，这里我们推出了Sigmoid函数.这是因为Logistic模型对问题的前置概率估计其实就是伯努利分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2. 逻辑回归曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD6CAYAAAC1W2xyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJDA1lb0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113cb9198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "lineX = np.linspace(-6,6, 15)\n",
    "y=1/(1+np.e**(-lineX))\n",
    "\n",
    "plt.figure(1, facecolor='white')\n",
    "plt.plot(lineX, y, 'r--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3 似然函数与损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3.1 Logistic公式\n",
    "$h_{\\theta}(x)=g(\\theta^Tx)=\\frac{1}{1+e^{-\\theta^Tx}}$\n",
    "\n",
    "$g'(x)=(\\frac{1}{1+e^{-x}})'=\\frac{e^{-x}}{(1+e^{-x})^2}=\\frac{1}{1+e^{-x}} \\bullet \\frac{e^{-x}}{1+e^{-x}}$\n",
    "\n",
    "$=\\frac{1}{1+e^{-x}} \\bullet (1-\\frac{1}{1+e^{-x}})=g(x) \\bullet (1-g(x))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3.2 似然函数\n",
    "假定$\\begin{cases}\n",
    "P(y=1|x;\\theta)=h_{\\theta}(x)\\\\\n",
    "P(y=0|x;\\theta)=1-h_{\\theta}(x)\n",
    "\\end{cases}$，那么就有$p(y|x;\\theta)=(h_{\\theta}(x))^y(1-h_{\\theta}(x))^{1-y}$\n",
    "\n",
    "那么似然函数$L(\\theta)=p(\\overrightarrow{y}|X;\\theta)=\\prod_{i=1}^m[h_{\\theta}(x^{(i)})]^{y^{(i)}} \\bullet [1-h_{\\theta}(x^{(i)})]^{1-y^{(i)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3.3 对数似然函数\n",
    "$\\because L(\\theta)==\\prod_{i=1}^m[h_{\\theta}(x^{(i)})]^{y^{(i)}} \\bullet [1-h_{\\theta}(x^{(i)})]^{1-y^{(i)}}$\n",
    "\n",
    "$\\Rightarrow \\ell(\\theta)=logL(\\theta)=\\sum_{i=1}^my^{(i)}logh_{\\theta}(x^{(i)})+(1-y^{(i)})log(1-h_{\\theta}(x^{(i)}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3.4 对数似然求偏导\n",
    "$\\frac{\\partial{\\ell(\\theta)}}{\\partial{\\theta_j}}=\\sum_{i=1}^m[y^{(i)} \\bullet \\frac{1}{h_{\\theta}(x^{(i)})} \\bullet \\frac{\\partial{h_{\\theta}(x^{(i)})}}{\\partial{\\theta_j}} + (1-y^{(i)}) \\bullet -\\frac{1}{1-h_{\\theta}(x^{(i)})} \\bullet \\frac{\\partial{h_{\\theta}(x^{(i)})}}{\\partial{\\theta_j}}]=\\sum_{i=1}^m[\\frac{y^{(i)}}{h_{\\theta}(x^{(i)})}-\\frac{1-y^{(i)}}{1-h_{\\theta}(x^{(i)})}] \\bullet \\frac{\\partial{h_{\\theta}(x^{(i)})}}{\\partial{\\theta_j}}$\n",
    "\n",
    "$\\because h_{\\theta}(x^{(i)})=g(\\theta^Tx^{(i)}), \\therefore \\frac{\\partial{\\ell(\\theta)}}{\\partial{\\theta_j}}=\\sum_{i=1}^m[\\frac{y^{(i)}}{g(\\theta^Tx^{(i)})}-\\frac{1-y^{(i)}}{1-g(\\theta^Tx^{(i)})}] \\bullet \\frac{\\partial{g(\\theta^Tx^{(i)})}}{\\partial{\\theta_j}}$\n",
    "\n",
    "$\\because g'(z)=g(z) \\bullet (1-g(z)), \\therefore \\frac{\\partial{\\ell(\\theta)}}{\\partial{\\theta_j}}=\\sum_{i=1}^m[\\frac{y^{(i)}}{g(\\theta^Tx^{(i)})}-\\frac{1-y^{(i)}}{1-g(\\theta^Tx^{(i)})}] \\bullet g(\\theta^Tx^{(i)}) \\bullet [1-g(\\theta^Tx^{(i)})] \\bullet \\frac{\\partial{\\theta^Tx^{(i)}}}{\\partial{\\theta_j}}$\n",
    "\n",
    "$\\Rightarrow \\frac{\\partial{\\ell(\\theta)}}{\\partial{\\theta_j}}=\\sum_{i=1}^m[y^{(i)}(1-g(\\theta^Tx^{(i)})) - (1-y^{(i)})g(\\theta^Tx^{(i)})] \\bullet x_j^{(i)}$\n",
    "\n",
    "$\\Rightarrow \\frac{\\partial{\\ell(\\theta)}}{\\partial{\\theta_j}}=\\sum_{i=1}^m[y^{(i)} - g(\\theta^Tx^{(i)})] \\bullet x_j^{(i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3.5 参数学习\n",
    "$\\theta_j:=\\theta_j+\\alpha\\sum_{i=1}^m(y^{(i)}-h_{\\theta}(x^{(i)}))x_j^{(i)}$,有了这个，就可以进行参数学习了。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3.6 总结\n",
    "我们发现这个式子和线性回归的形式是完全一样的，如果我们定义$h_{\\theta}(x)=\\theta{X}$，那么就是线性回归，如果我们定义$h_{\\theta}(x)=\\frac{1}{1+e^{-\\theta{x}}}$，那么就是logistic回归。\n",
    "\n",
    "Logistic回归中，我们假定模型服从的是二项分布，利用最大似然估计进行推导的；线性回归我们假定模型服从高斯分布，利用最大似然估计推导的；正是因为二项分布和高斯分布都是指数族分布，所以它们才能得到一样的参数学习法则\n",
    "\n",
    "其实Logistic回归是一个广义的线性模型，这是因为$logit(p)=log\\frac{p}{1-p}=log\\frac{h_{\\theta}(x)}{1-h_{\\theta}(x)}=log(\\frac{\\frac{1}{1+e^{-\\theta^T{x}}}}{\\frac{e^{-\\theta^T{x}}}{1+e^{-\\theta^T{x}}}})=loge^{-\\theta^T{x}}=\\theta^Tx$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#4 Softmax回归\n",
    "\n",
    "利用Logistic回归来进行多分类，就是Softmax回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4.1 定义\n",
    "设样本为一个m行的记录$X=\\{\\overrightarrow{x_1},\\overrightarrow{x_2},...,\\overrightarrow{x_n}\\}$, 共有K的类别，那么存在这么K个$\\Theta$向量$\\overrightarrow{\\theta_1},\\overrightarrow{\\theta_2},...,\\overrightarrow{\\theta_K}$。令$Z=\\Theta^TX$，且设$\\varphi=\\frac{1}{1+e^{-Z}}$，则有$\\varphi=\\frac{1}{1+e^{-\\Theta^TX}}$\n",
    "\n",
    "如果K=2，那么就是Logistic回归。分类为$y \\in \\{0, 1\\}$，带入$\\varphi$函数后，总能求的一个0~1之间的值，我们用0.5做分界点，大于0.5的就是分类1，否则就是分类0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果K>2，那么可以令$C_i=e^{\\overrightarrow{\\theta_i}^T\\overrightarrow{x_i}}$，则第i行的概率为$p(c=k|x;\\theta)=\\frac{e^{\\theta_k^Tx}}{\\sum_{i=1}^Ke^{\\theta_i^Tx}}, k=1,2,...,K$\n",
    "\n",
    "似然函数为$L(\\theta)=\\prod_{i=1}^m\\prod_{k=1}^Kp(c=k|x^{(i)};\\theta)^{y_k^{(i)}}=\\prod_{i=1}^m\\prod_{k=1}^K[\\frac{e^{\\theta_k^Tx}}{\\sum_{i=1}^Ke^{\\theta_i^Tx}}]^{y_k^{(i)}}$\n",
    "\n",
    "对数似然：$J_m(\\theta)=lnL(\\theta)=\\sum_{i=1}^m\\sum_{k=1}^Ky_k^{(i)} \\bullet (\\theta_k^Tx^{(i)}-ln\\sum_{i=1}^Ke^(\\theta_i^Tx^{(i)}))$\n",
    "\n",
    "$J(\\theta)=\\sum_{k=1}^Ky_k \\bullet (\\theta_k^Tx-ln\\sum_{i=1}^Ke^{\\theta_i^Tx})$\n",
    "\n",
    "随机梯度：$\\frac{\\partial{J(\\theta)}}{\\partial{\\theta_k}}=(y_k-p(y_k|x;\\theta)) \\bullet x$"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}