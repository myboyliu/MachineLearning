{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#线性回归\n",
    "线性回归是一种用来对于连续性变量进行预测的办法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1 问题的引出\n",
    "![images](images/03/03.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_1$ : 房子大小, $x_2$ : 卧室数量, $x_3$ : 楼层数量, $x_4$ : 房龄, y : 房子的价格，则有：\n",
    "\n",
    "$x_1=\\begin{bmatrix}\n",
    "2104\\\\\n",
    "1416\\\\\n",
    "1534\\\\\n",
    "852\\\\\n",
    "...\n",
    "\\end{bmatrix}$\n",
    "$x_2=\\begin{bmatrix}\n",
    "5\\\\\n",
    "3\\\\\n",
    "3\\\\\n",
    "2\\\\\n",
    "...\n",
    "\\end{bmatrix}$\n",
    "$x_3=\\begin{bmatrix}\n",
    "1\\\\\n",
    "2\\\\\n",
    "2\\\\\n",
    "1\\\\\n",
    "...\n",
    "\\end{bmatrix}$\n",
    "$x_4=\\begin{bmatrix}\n",
    "45\\\\\n",
    "40\\\\\n",
    "30\\\\\n",
    "36\\\\\n",
    "...\n",
    "\\end{bmatrix}$\n",
    "$y=\\begin{bmatrix}\n",
    "460\\\\\n",
    "232\\\\\n",
    "315\\\\\n",
    "178\\\\\n",
    "...\n",
    "\\end{bmatrix}$\n",
    "$h_{\\Theta}(X)=\\Theta_0+\\Theta_1x_1+\\Theta_2x_2+...+\\Theta_nx_n$,这时，使用线性代数表示更为方便。我们假设，$x_0=1$\n",
    "$x=\\begin{bmatrix}\n",
    "x_1\\\\\n",
    "x_2\\\\\n",
    "x_3\\\\\n",
    "...\\\\\n",
    "x_n\n",
    "\\end{bmatrix}$\n",
    "$\\theta=\\begin{bmatrix}\n",
    "\\theta_1\\\\\n",
    "\\theta_2\\\\\n",
    "\\theta_3\\\\\n",
    "...\\\\\n",
    "\\theta_n\n",
    "\\end{bmatrix}$\n",
    "$h_{\\Theta}(X)=\\Theta^TX$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于样本大概符合线性关系的情况，我们总是可以写出公式$Y=\\Theta^TX+\\varepsilon$，Y是实际的结果，$\\Theta^TX$是计算出来的结果，$\\varepsilon$是误差。根据中心极限定理，我们知道每个样本距离线性回归曲线的误差，是独立并且具有相同的分布，通常认为服从高斯分布$N-(0, \\sigma^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2 线性回归的似然函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于$\\varepsilon$符合高斯分布$N-(0, \\sigma^2)$，那么就有$P(\\varepsilon)=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{\\varepsilon^2}{2\\sigma^2}}$，并且$\\varepsilon=Y-\\Theta^TX$，则有\n",
    "$$\n",
    "P(y^{(i)}|x^{(i)};\\theta)=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(y^{(i)}-\\theta^Tx^{(i)})^2}{2\\sigma^2}}\n",
    "$$\n",
    "那么它的似然函数就是它的联合概率密度，则有$L(\\theta)=\\prod_{i=1}^mP(y^{(i)}|x^{(i)};\\theta)=\\prod_{i=1}^m\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(y^{(i)}-\\theta^Tx^{(i)})^2}{2\\sigma^2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来求其对数似然\n",
    "\n",
    "$\\ell(\\theta)=logL(\\theta)=log\\prod_{i=1}^m\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(y^{(i)}-\\theta^Tx^{(i)})^2}{2\\sigma^2}}=\\sum_{i=1}^mlog\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(y^{(i)}-\\theta^Tx^{(i)})^2}{2\\sigma^2}}$\n",
    "\n",
    "$\\Rightarrow \\ell(\\theta)=mlog\\frac{1}{\\sqrt{2\\pi}\\sigma}-\\frac{1}{\\sigma^2} \\bullet \\frac{1}{2}\\sum_{i=1}^m(y^{(i)}-\\theta^Tx^{(i)})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于我们来说需要求最大似然，由于$=mlog\\frac{1}{\\sqrt{2\\pi}\\sigma}$和$\\frac{1}{\\sigma^2}$都是定值，所以我们只要求$\\frac{1}{2}\\sum_{i=1}^m(y^{(i)}-\\theta^Tx^{(i)})^2$的最小值就行了，也就有了\n",
    "$$\n",
    "J(\\theta)=\\frac{1}{2}\\sum_{i=1}^m(y^{(i)}-\\theta^Tx^{(i)})^2\n",
    "$$\n",
    "这就是损失函数，也可以写成\n",
    "$$\n",
    "J(\\theta)=\\frac{1}{2}\\sum_{i=1}^m(h_{\\theta}(x^{(i)})-y^{(i)})^2\n",
    "$$\n",
    "发现里面是期望值与真实值的差值的平方，这不就是最小二乘吗。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以误差符合高斯分布，那就是最小二乘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3. $\\theta$的解法\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3.1 最小二乘法\n",
    "首先解释以下损失函数中的各个符号的意义\n",
    "- $y^{(i)}$表示样本第i行的期望值\n",
    "- $x^{(i)}$表示样本第i行的特征值，是一个行向量$x_{i1},x_{i2},...,x_{in}$\n",
    "- $h_{\\theta}(x^{(i)})$是第i行计算出来的实际值\n",
    "\n",
    "所以损失函数也可以写成$J(\\theta)=\\frac{1}{2}(X\\Theta-Y)^T(X\\Theta-Y)$，其中X是$m*n$的样本，$\\Theta$是一个$m*1$的列向量，Y也是一个$m*1$的列向量\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推导：\n",
    "\n",
    "$J(\\Theta)=\\frac{1}{2}(\\Theta^TX^T-Y^T)(X\\Theta-Y)=\\frac{1}{2}(\\Theta^TX^TX\\Theta-\\Theta^TX^TY-Y^TX\\Theta+Y^TY)$，因为我们希望找到$J(\\Theta)$的最小值，那么需要求出$\\frac{\\partial{J(\\Theta)}}{\\partial{\\Theta}}$，$\\because X^TX$一定是对称矩阵，根据线性代数的法则$\\frac{\\partial{\\Theta^TA\\Theta}}{\\partial{\\Theta}}=2A\\Theta$，则有\n",
    "\n",
    "$\\frac{\\partial{J(\\Theta)}}{\\partial{\\Theta}}=\\frac{1}{2}(2X^TX\\Theta-X^TY-(Y^TX)^T+0)$\n",
    "\n",
    "$\\frac{\\partial{J(\\Theta)}}{\\partial{\\Theta}}=\\frac{1}{2}(2X^TX\\Theta-2X^TY)$\n",
    "\n",
    "$\\therefore \\frac{\\partial{J(\\Theta)}}{\\partial{\\Theta}}=X^TX\\Theta-X^TY$，现在我们令其等于0，那么就有$X^TX\\Theta = X^TY$，如果$X^TX$是可逆的，那么$\\Theta=(X^TX)^{-1} \\bullet X^TY$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果$X^TX$不可逆或者为了防止过拟合，可以增加$\\lambda$扰动，有$\\Theta=(X^TX+\\lambda{I})^{-1} \\bullet X^TY$。由于$X^TX$一定是一个半正定矩阵，那么加上一个$\\lambda{I}$，则一定是一个正定矩阵，正定矩阵一定是可逆的\n",
    "\n",
    "损失函数$J(\\Theta)$一定是一个凸函数，开口向上的，所以它有极小值，这是因为它的二次项$\\Theta^TX^XT\\Theta$，它的一阶导数是$2X^TX\\Theta$，二阶导数就是$2X^TX$，这个一定是半正定的,半正定的函数一定是凸函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3.2 梯度下降法\n",
    "梯度方向是函数值变化最快的方向\n",
    "- 全微分：考虑所有自变量变化时，函数值的变化情况\n",
    "- 偏微分：假设其它自变量不变，考虑一个变量变化，函数值的变化情况\n",
    "\n",
    "$\\Delta{y}=\\sum_{i=1}^n\\frac{\\partial}{\\partial{x_i}}\\Delta{x_i}$\n",
    "- 梯度向量:所有变量偏微分组成的向量。梯度方向由L(θ)对θ的偏导数确定,所以我们需要沿着负梯度方向往下走\n",
    "\n",
    "梯度下降法的思路就是找到负梯度方向，然后不停的以一个很小的间隔去向它移动，直到前后两次的高度差小于一定范围的时候，停止，这个时候可以求出m和b,这就涉及到一个问题，这个很小的间隔怎么选取，我们管这个参数叫做$\\alpha$，一般定义为0.001，这个参数太大了，会在底部不停震荡，如果太小，那么迭代次数太多"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 批量梯度下降算法:BGD。拿到所有样本进行梯度下降\n",
    "- 随机梯度下降算法:SGD,优先选择.每次拿到一个样本就开始梯度下降\n",
    "- mini-batch梯度下降算法:拿到一部分样本进行梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设有n个特征，表达式如下$h(\\Theta)=\\Theta_0+\\Theta_1x_1+\\Theta_2x_2+...+\\Theta_nx_n$那么\n",
    "$$\n",
    "Repeat\\{\n",
    "   \\Theta_j := \\Theta_j - \\alpha\\frac{1}{m}\\sum_{i=1}^m(h_{\\Theta}(x^{(i)})-y^{(i)})x_j^{(i)}, j={0,1,...,n}\n",
    "\\}\n",
    "$$\n",
    "\n",
    "$\\Theta_0 := \\Theta_0 - \\alpha\\frac{1}{m}\\sum_{i=1}^m(h_{\\Theta}(x^{(i)})-y^{(i)})x_0^{(i)}$\n",
    "\n",
    "$\\Theta_1 := \\Theta_1 - \\alpha\\frac{1}{m}\\sum_{i=1}^m(h_{\\Theta}(x^{(i)})-y^{(i)})x_1^{(i)}$\n",
    "\n",
    "$\\Theta_2 := \\Theta_2 - \\alpha\\frac{1}{m}\\sum_{i=1}^m(h_{\\Theta}(x^{(i)})-y^{(i)})x_2^{(i)}$\n",
    "\n",
    "![images](images/03/01.png)\n",
    "这是什么意思，首先给$\\Theta_0,\\Theta_1,...\\Theta_n$设置初始值，基本都是1，然后中间部分一样，这个值就是用这些初始值带入m个向量，可以得到m个值，用这m个值分别减去它们对应的y值，然后用这个值分别乘以对应记录的当前向量特征的值($\\Theta_0$就是$X_0$，$X_0$没有，就是1，$\\Theta_1$就是当前记录的第一个特征的值)然后将这m个值求和,然后乘以步长，乘以$\\frac{1}{m}$,这个值我们可以叫他$\\delta$。对于$\\Theta_0$来说，他的下一个值就是$\\Theta_0-\\delta$，对于$\\Theta_1$来说，他的下一个值就是$\\Theta_1-\\delta$...直到前后两次的高度差小于一定范围"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面三幅图分别是order为3，6，9的情况下，梯度下降法(红色)和最小二乘法(绿色)的拟合曲线\n",
    "![images](images/03/04.png)\n",
    "![images](images/03/05.png)\n",
    "![images](images/03/06.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#4 线性回归的复杂度惩罚因子\n",
    "对于线性回归来说，n个样本必然能找到一个最高阶为n-1次方的函数，使得求的的曲线经过所有点，但是这样存在两个问题\n",
    "- 过拟合\n",
    "- 所求的$\\Theta$系数过大\n",
    "\n",
    "基于此，需要将目标函数增加平方和损失$J(\\Theta)=\\frac{1}{2}\\sum_{i=1}^m(h_{\\Theta}(x^{(i)})-y^{(i)})^2+\\lambda\\sum_{j=1}^n\\Theta_j^2$，新加的东西叫做正则项，由于是取的平方，所以也叫做L2正则；在线性回归中，这个叫做岭(Ridge)回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#5 线性回归的扩展\n",
    "前面已经提到过，线性回归模型的基本特征就是，模型是参数向量$\\alpha={\\alpha_0,\\alpha_1,\\alpha_2,...,\\alpha_n}$的线性函数，这极大地限制了模型的适用性。这里使用基函数（basis function）对上面的线性模型进行拓展，即：线性回归模型是一组输入变量x的非线性基函数的线性组合，在数学上其形式:$y(x,\\alpha)=\\alpha_0+\\sum_{i=1}^M\\alpha_i\\Phi_i(x)$",
    "\n",
    "其中$\\Phi_i(x)$叫做基函数，总共的基函数数目为M个"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##5.1 $\\Phi_i(x) = x_i$\n",
    "这里取M=n，那么就能够得到$y(x,\\alpha)=\\alpha_0+\\sum_{i=1}^M\\alpha_i\\Phi_i(x)=\\alpha_0+\\alpha_1x_1+\\alpha_2x_2+...+\\alpha_n*x_n$,这个就是多变量线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##5.2 $\\Phi_i(x) = x^i$\n",
    "这里就是典型的多项式曲线拟合（Polynomial Curve Fitting），其中M是输入变量x的最高次数，同时也是模型的复杂度。多项式曲线拟合的一个缺陷就是这个模型是一个输入变量的全局函数，这使得对输入空间的一个区间的改动会影响到其他的区间，这个问题可以通过将输入空间切分成不同的区域，对不同的区域各自在各自的区域中做曲线拟合来解决，这个方法就是经典的样条函数（spline function）。$y(x,\\alpha)=\\alpha_0+\\sum_{i=1}^M\\alpha_i\\Phi_i(x)=\\sum_{i=0}^M\\alpha_ix^i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#5.3 $\\Phi_i(x)=e^{-\\frac{(x_i-\\mu_i)^2}{2s^2}}$\n",
    "这里$\\mu_i$表示的是该基函数在输入空间中的位置，参数s控制的是基函数的空间尺度，这个基函数往往会被称为高斯基函数（Gaussian basis function）。需要强调一点，这里的高斯函数并不是输入空间的概率分布，其并没有概率意义。由于每个基函数前面都有一个比例系数ωj，所以，对参数的归一化（coefficient normalization）并不是必需的。 $y(x,\\alpha)=\\alpha_0+\\sum_{i=1}^M\\alpha_i\\Phi_i(x)=\\alpha_0\\sum_{i=1}^M\\alpha_ie^{-\\frac{(x_i-\\mu_i)^2}{2s^2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##5.4 $\\Phi_i(x)=\\sigma(\\frac{x_i-\\mu_i}{s})$\n",
    "这个基函数叫做sigmoid函数，其定义为：$\\sigma(a)=\\frac{1}{1+e^{-a}}$,与这个函数拥有相同效果的一个函数叫做tach函数，其定义为：$tanh(a)=2\\sigma(a)-1$.在神经网络中，这两个基函数有着非常广泛的应用。其实从根本上来讲，神经网络的每一层，也是一些基函数的线性组合，神经网络之所以能处理非线性问题，其根本也是由于采用了合适的非线性基函数。$y(x,\\alpha)=\\alpha_0+\\sum_{i=1}^M\\alpha_i\\Phi_i(x)=\\alpha_0\\sum_{i=1}^M\\alpha_i\\frac{1}{1+e^{-a}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "真实值： 2 3 -1\n预测值： [ 1.99114639  3.00703745 -0.60936282]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJ/N/YY/GoI63J7EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10fbc5f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.optimize import leastsq\n",
    "\n",
    "def residual(t, x, y):\n",
    "    return y - (t[0] * x ** 2 + t[1] * x + t[2])\n",
    "\n",
    "x = np.linspace(-2, 2, 50)\n",
    "A, B, C = 2, 3, -1\n",
    "y = (A * x ** 2 + B * x + C) + np.random.rand(len(x))*0.75\n",
    "\n",
    "t = leastsq(residual, [0, 0, 0], args=(x, y))\n",
    "theta = t[0]\n",
    "print('真实值：', A, B, C)\n",
    "print('预测值：', theta)\n",
    "y_hat = theta[0] * x ** 2 + theta[1] * x + theta[2]\n",
    "plt.plot(x, y, 'r-', linewidth=2, label=u'Actual')\n",
    "plt.plot(x, y_hat, 'g-', linewidth=2, label=u'Predict')\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#6 四种回归的比较"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##6.1 误差平方和-MSE\n",
    "$\\varphi=\\frac{1}{m}\\sum_{i=1}^m(y_i-\\hat{y_i})^2$\n",
    "\n",
    "如果MSE趋近于0，那么说明在训练集上拟合程度很好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##6.2 线性回归、岭回归、Lasso回归和弹性网的比较"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###6.2.1 线性回归\n",
    "![images](images/03/07.png)\n",
    "图中画出了从1阶到8阶的拟合曲线，发现阶数越大，拟合程度越好，当全部拟合的时候，$R^2=1$。但是我们发现，8阶的曲线其实出现了一些过拟合的现象"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###6.2.2 岭回归\n",
    "![images](images/03/08.png)。过拟合现象降低了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###6.2.3 Lasso回归\n",
    "![images](images/03/09.png)。过拟合现象降低了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###6.2.4 弹性网\n",
    "![images](images/03/10.png)。过拟合现象降低了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##6.3 一些概念\n",
    "对于m个样本$(\\overrightarrow{x_1},y_1), (\\overrightarrow{x_2},y_2),...,(\\overrightarrow{x_m},y_m)$，某模型的估计值为$(\\overrightarrow{x_1},\\hat{y_1}), (\\overrightarrow{x_2},\\hat{y_2}),...,(\\overrightarrow{x_m},\\hat{y_m})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "样本的总平方和TSS(Total Sum of Squares):$TSS=\\sum_{i=1}^m(y_i-\\overline{y})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "残差平方和RSS(Residual Sum of Squares):$RSS=\\sum_{i=1}^m(\\hat{y_i}-y_i)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$R^2=1-\\frac{RSS}{TSS}$\n",
    "\n",
    "$R^2$越大，拟合效果越好。最优值为1；若模型预测为随机值，$R^2$可能为负；若预测值恒为样本期望，$R^2=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ESS(Expalined Sum of Squares):$ESS=\\sum_{i=1}^m(\\hat{y_i}-\\overline{y})^2$\n",
    "\n",
    "只有在无偏估计时才有$TSS=ESS+RSS$；否则$TSS \\ge ESS+RSS$\n",
    "\n",
    "ESS又称为回归平方和SSR(Sum of Squares for Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}