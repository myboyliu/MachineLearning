{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#支持向量机"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1. 概述\n",
    "支持向量机的三种情况：线性可分、近似线性可分、非线性可分.不仅可以做二分类，也可以做多分类\n",
    "- 线性可分：在空间中可以找到一条直线，或一个平面，或一个超平面，把样本可以完美的分开，一类在一边，另一类在另一边，也就是$\\Theta{X} + b$\n",
    "- 近似线性可分：在空间中可以找到一条直线，或一个平面，或一个超平面，把样本可以近似的分开\n",
    "- 非线性可分：在空间中可以找到一条曲线，或一个曲面，把样本可以完美的分开\n",
    "\n",
    "逻辑回归是不能进行非线性可分的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2. 重要概念\n",
    "![images](images/05/01.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2.1 点到直线的距离\n",
    "假设存在一个点$(x_0,y_0)$，以及一条直线$f(x,y)=Ax+By+C$，那么这个点到这条线的距离就是$\\frac{|Ax_0+By_0+C|}{\\sqrt{A^2+B^2}}$。如果$Ax_0+By_0+C > 0$，说明$(x_0,y_0)$位于法向$(A,B)$的正向，反之位于负向。\n",
    "\n",
    "$\\frac{|Ax_0+By_0+C|}{\\sqrt{A^2+B^2}}=\\frac{A}{\\sqrt{A^2+B^2}}x_0+\\frac{B}{\\sqrt{A^2+B^2}}y_0+\\frac{C}{\\sqrt{A^2+B^2}}$，令$A'=\\frac{A}{\\sqrt{A^2+B^2}},B'=\\frac{B}{\\sqrt{A^2+B^2}},C'=\\frac{C}{\\sqrt{A^2+B^2}},\\Rightarrow A'x_0+B'y_0+C'$，使用向量表示，有如下形式\n",
    "\n",
    "$\\Rightarrow \\begin{bmatrix}\n",
    "A'&B'\n",
    "\\end{bmatrix} \\bullet \\begin{bmatrix}\n",
    "x_0\\\\\n",
    "y_0\n",
    "\\end{bmatrix} + C$\n",
    "\n",
    "如果是n为的，那么$\\begin{bmatrix}\n",
    "A'&B'\n",
    "\\end{bmatrix}$就是$\\overrightarrow{\\omega}$，$\\begin{bmatrix}\n",
    "x_0\\\\\n",
    "y_0\n",
    "\\end{bmatrix}$就是$\\overrightarrow{x}$\n",
    "\n",
    "所以点到直线的距离就是$\\alpha(x_0,L)=\\frac{\\overrightarrow{\\omega} \\bullet \\overrightarrow{x} + C}{||\\omega||_2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2.2 概念\n",
    "如上图所示，在空间中存在9个点，我们可以找到很多的平面来把它们完美的分为两组，比如中间的两个实线所在的平面，这两个面可以向左右进行平移，当与某个点相交后，可以形成两个不同的虚面，(蓝色的形成两个虚面，红色的形成两个虚面)，那么蓝色的两个虚面的距离和，就叫做D，同样两个红色虚面的距离和，也叫做D，那么使得D最大的分割面，就是最好的分割面,虚面叫做支撑平面，跟支撑平面相交的点，就叫做支撑向量,D叫做间隔.分割面的公式:$\\vec{w}\\bullet\\vec{x}+b=0$.两个支撑平面的公式：$\\vec{w}\\bullet\\vec{x}+b=-1$,$\\vec{w}\\bullet\\vec{x}+b=1$\n",
    "\n",
    "w是平面的法向量(垂直于平面的向量)，-1和+1是分类标签(逻辑回归的分类标签是0和1).假设x'是支撑平面上的一点，那么支撑平面到分割平面的距离，就是这个点x'到分割平面的距离，那么就是:$\\frac{\\vec{w}x^{'}+b}{||w||}=\\frac{1}{||w||}$,那么$D=\\frac{2}{||w||}$，也就是2除以w的模长(w的模长就是w的长度)但是这里有个约束条件，那么就是样本点带入wx+b的公式中，要么结果是<=-1的，要么就是>=1的，总结一下就有如下约束$y*(\\vec{w}\\bullet{x}+b)>=1$,x就是样本点的坐标，等于1的时候，样本点正好落在支撑平面上,所以我们需要求解的问题，就变成了$argmax(D)=argmax(\\frac{2}{||w||})=argmin(\\frac{1}{2}||w||)=argmin(\\frac{1}{2}||w||^2)$，最后一步是为了将来好做处理\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2.3 问题给出\n",
    "- 假设给定一个特征空间上的训练数据集$T=\\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\\}$，其中$x_i \\in R^n, y_i \\in \\{-1,+1\\},i=1,2,...,N$\n",
    "- $x_i$为第i个实例(若n>1,$x_i$为向量)\n",
    "- $y_i$为$x_i$的类标记；当$y_i=+1$时，称$x_i$为正例；当$y_i=-1$时，称$x_i$为负例\n",
    "- $(x_i,y_i)$称为样本点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2.4 线性可分支持向量机\n",
    "- 给定线性可分训练数据集，通过间隔最大化得到的分离超平面为$y(x)=\\omega^T\\Phi(x)+b$\n",
    "- 相应的分类决策函数$f(x)=sign(\\omega^T\\Phi(x)+b)$，该决策函数称为线性可分支持向量机。\n",
    "- $\\varphi(x)$是某个确定的特征空间转换函数，它的作用是将x映射到(更高的)维度，最简单的有$\\Phi(x)=x$\n",
    "- 求解分离超平面问题可以等价为求解相应的凸二次规划问题\n",
    "\n",
    "简单的说，就是首先你要把所有样本都分开，其次你要在所有这些可以把样本都分开的线里面找到一个距离样本最远的直线，所以就是最大最小问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2.5 符号整理\n",
    "- 分割平面:$y(x)=\\omega^T\\Phi(x)+b$\n",
    "- 训练集:$x_1,x_2,...,x_n$\n",
    "- 目标值:$y_1,y_2,...,y_n, y_i \\in \\{-1,+1\\}$\n",
    "- 新数据的分类:sign[y(x)]\n",
    "- $y(x_i)$表示第i个样本的预测值，$y_i$表示的是第i个样本的真实值\n",
    "\n",
    "所以就有$\\begin{cases}\n",
    "y(x_i) > 0 \\Rightarrow y_i = +1\\\\\n",
    "y(x_i) < 0 \\Rightarrow y_i = -1\n",
    "\\end{cases} \\Rightarrow y_i \\bullet y(x_i) > 0$\n",
    "\n",
    "那么我们的目标函数就是\n",
    "$$\n",
    "argmax_{\\omega,b}\\{\\frac{1}{||\\omega||}min_i[y_i \\bullet (\\omega^T \\bullet \\Phi(x_i)+b)]\\}\n",
    "$$\n",
    "\n",
    "我们知道2.2中有约束$y*(\\vec{w}\\bullet{x}+b)>=1$，所以新的目标函数就是\n",
    "$$\n",
    "argmax_{\\omega,b}\\frac{1}{||\\omega||}\n",
    "$$\n",
    "\n",
    "整理一下就有了$\\begin{cases}\n",
    "min(\\frac{1}{2}||w||^2) \\\\\n",
    "y(w^Tx+b) >=1\n",
    "\\end{cases}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3. 解法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3.1 拉格朗日乘子法\n",
    "要找函数$z=f(x,y)$在条件$\\varphi(x,y)=0$下可能的极值点，那么我们可以构造函数$F(x,y)=f(x,y)+\\lambda\\varphi(x,y)$,其中$\\lambda$为某一常数，也叫做拉格朗日乘子。令$\\frac{\\partial{F}}{\\partial{x}}=0,\\frac{\\partial{F}}{\\partial{y}}=0,\\frac{\\partial{F}}{\\partial{\\lambda}}=0$,\n",
    "$\\begin{cases} \n",
    "f_x(x,y)+\\lambda\\varphi_x(x,y)=0\\\\ \n",
    "f_y(x,y)+\\lambda\\varphi_y(x,y)=0\n",
    "\\end{cases}$\n",
    ",$\\varphi(x,y)=0$,解出$x,y,\\lambda$,其中x,y就是可能的极值点。举例说明\n",
    "\n",
    "例题：将正数12分成x,y,z三个正数，使得$\\mu_{max}=x^3y^2z$为最大，求解x,y,z以及$\\mu_{max}$\n",
    "\n",
    "解：由题目可知$x+y+z=12$,所以$\\varphi(x,y,z)=x+y+z-12=0$,令$F(x,y,z)=x^3y^2z+\\lambda(x+y+z-12)$，则\n",
    "$\\begin{cases}\n",
    "\\frac{\\partial{F}}{\\partial{x}}=F_x^{'}=3y^2zx^2+\\lambda=0\\\\\n",
    "\\frac{\\partial{F}}{\\partial{y}}=F_y^{'}=2x^3zy+\\lambda=0\\\\\n",
    "\\frac{\\partial{F}}{\\partial{z}}=F_z^{'}=x^3y^2+\\lambda=0\\\\\n",
    "x+y+z=12\n",
    "\\end{cases}\n",
    "$\n",
    "$\\Rightarrow 2x=3y,y=2z$\n",
    "\n",
    "解得唯一驻点是(6,4,2)，故$\\mu_{max}=6^3*4^2*2=6912$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3.2 广义拉格朗日乘子法\n",
    "假设要求的极值函数为F(x),其中的条件有m个不等式:$[f_1(x)\\leq 0,f_2(x)\\leq 0,...,f_m(x)\\leq 0]$,p个等式:$[h_1(x)=0,h_2(x)=0,...,h_p(x)=0]$，那么可以分别给定m个不等式因子$\\lambda_{m}$，以及p个等式因子$\\nu_{p}$,形成广义拉格朗日函数为:\n",
    "$L(x,y,\\lambda,\\nu)=F(x)+\\sum_{i=1}^p\\nu_ih_i(x)+\\sum_{j=1}^m\\lambda_jf_j(x)$,\n",
    "\n",
    "$\\because h(x)=0 \\Rightarrow \\sum_{j=1}^p\\nu_jh_j(x)=0$\n",
    "\n",
    "$\\therefore L(x,y,\\lambda,\\nu)=L(x,y,\\lambda)=F(x)+\\sum_{i=1}^m\\lambda_if_i(x)$\n",
    "\n",
    "$\\because \\lambda > 0, f(x)\\leq 0$\n",
    "\n",
    "$\\therefore L(x,y,\\lambda)\\leq F(x)$\n",
    "\n",
    "KKT条件，就是使得$L(x,y,\\lambda)=F(x)$的条件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3.3 初始问题求解-拉格朗日函数\n",
    "最开始的问题，我们的目标是\n",
    "$\\begin{cases}\n",
    "min(\\frac{1}{2}||w||^2) \\\\\n",
    "y(w^Tx+b) >=1\n",
    "\\end{cases}\n",
    "$.通过上面的拉格朗日乘子法，我们就可以求解了。\n",
    "\n",
    "$$\\because y(w^Tx+b) >=1, \\therefore 1-y(w^Tx+b)\\leq 0$$\n",
    "\n",
    "加入拉格朗日因子$\\alpha$,变化为$-\\alpha(y(w^Tx+b)-1)\\leq 0$，那么拉格朗日函数就是$L(w,b,\\alpha)=\\frac{1}{2}||w||^2-\\alpha(y(w^Tx+b)-1)=\\frac{1}{2}||w||^2-\\sum_{i=1}^n\\alpha_i(y_i(w^Tx_i+b)-1)$,其中$x_i$就是第i个样本,$y_i$就是第i个样本对应的分类,$\\alpha_i$就是第i个样本对应的拉格朗日乘子,$\\alpha_i\\geq 0$，那么我们我们要求的是一个极小极大问题：\n",
    "\n",
    "$min_{w,b}max_{\\alpha_i}L(w,b,\\alpha)=min_{w,b}max_{\\alpha_i}[\\frac{1}{2}||w||^2-\\sum_{i=1}^n\\alpha_i(y_i(w^Tx_i+b)-1)]$,展开有\n",
    "\n",
    "$min_{w,b}max_{\\alpha_i}L(w,b,\\alpha)=[\\frac{1}{2}||w||^2-\\sum_{i=1}^n\\alpha_iy_i(w^Tx_i+b)+\\sum_{i=1}^n\\alpha_i],\\alpha_i\\geq 0$\n",
    "\n",
    "这是一个凸规划问题，意义是先对$\\alpha$求偏导数，令其等于0消掉$\\alpha$，然后再对w和b求偏导数以求得L的最小值。但是直接求解由难度，我们需要引入拉格朗日对偶函数来解决这个问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3.4 拉格朗日对偶函数\n",
    "$min_{w,b}max_{\\alpha_i}L(w,b,\\alpha)=max_{\\alpha_i}min_{w,b}L(w,b,\\alpha)$,这样我们可以先对w，b求偏导数，令其结果为0，消掉w,b，然后在对$\\alpha$求L的最大值。有：\n",
    "\n",
    "$\\frac{\\partial{L}}{\\partial{w}}=0 \\Rightarrow \\frac{1}{2}*2||w||-\\sum_{i=1}^n\\alpha_iy_ix_i=0 \\Rightarrow w=\\sum_{i=1}^n\\alpha_iy_ix_i$\n",
    "\n",
    "$\\frac{\\partial{L}}{\\partial{b}}=0 \\Rightarrow \\sum_{i=1}^n\\alpha_iy_i=0$\n",
    "\n",
    "这是两个非常重要的等式，然后我们展开原来的式子:\n",
    "\n",
    "$min_{w,b}max_{\\alpha_i}L(w,b,\\alpha)=max_{\\alpha_i}min_{w,b}L(w,b,\\alpha)=max_{\\alpha_i}min_{w,b}(\\frac{1}{2}||w||^2-\\sum_{i=1}^n\\alpha_iy_iw^Tx_i-\\sum_{i=1}^n\\alpha_iy_ib+\\sum_{i=1}^n\\alpha_i)$\n",
    "\n",
    "$\\therefore max_{\\alpha_i}min_{w,b}L(w,b,\\alpha)=max_{\\alpha_i}min_{w,b}(\\frac{1}{2}||w||^2-w^T\\sum_{i=1}^n\\alpha_iy_ix_i-b\\sum_{i=1}^n\\alpha_iy_i+\\sum_{i=1}^n\\alpha_i)$\n",
    "\n",
    "将结果带入上式，有:\n",
    "\n",
    "$max_{\\alpha_i}L(\\alpha)=max_{\\alpha_i}(\\frac{1}{2}||w||^2-w^T*w-b*0+\\sum_{i=1}^n\\alpha_i)=max_{\\alpha_i}(\\sum_{i=1}^n\\alpha_i-\\frac{1}{2}||w||^2)$\n",
    "\n",
    "$=max_{\\alpha_i}(\\sum_{i=1}^n\\alpha_i-\\frac{1}{2}(\\sum_{i=1}^n\\alpha_iy_ix_i)^2)$\n",
    "\n",
    "$=max_{\\alpha_i}(\\sum_{i=1}^n\\alpha_i-\\frac{1}{2}\\sum_{i,j=1}^n\\alpha_i\\alpha_jy_iy_jx_ix_j),\\alpha_i\\geq 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3.5 求解\n",
    "$\\because min_{w,b}max_{\\alpha_i}L(w,b,\\alpha)=min\\frac{1}{2}||w||^2$\n",
    "\n",
    "$\\because L(w,b,\\alpha)=\\frac{1}{2}||w||^2-\\sum_{i=1}^n\\alpha_i(y_i(w^Tx_i+b)-1),\\alpha_i\\geq 0,y_i(w^Tx_i+b)\\geq 1$\n",
    "\n",
    "$\\therefore$ 只有每一个$\\alpha_i(y_i(w^Tx_i+b)-1)$都是0，结果才能为0从而达到才能取得最大值\n",
    "\n",
    "这样我们就能得到KKT条件:\n",
    "$\\begin{cases}\n",
    "\\alpha_i\\geq 0 \\\\\n",
    "y_i(w^Tx_i+b)-1\\geq 0 \\\\\n",
    "\\alpha_i(y_i(w^Tx_i+b)-1)=0\n",
    "\\end{cases}$\n",
    "\n",
    "根据KKT条件3，我们可以得出这样一个结论$\\alpha_i=0$或者$y_i(w^Tx_i+b)=1$\n",
    "\n",
    "所以如果一个样本是支持向量，则其对应的拉格朗日系数非零；如果一个样本不是支持向量，则其对应的拉格朗日系数一定为0。也就是说我们要找的支撑平面只跟支撑向量有关"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#4. SMO算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4.1 求解目标\n",
    "无论，我们是线性可分、近似线性可分或者是分线性可分，都可以通过加入松弛变量，或者是核函数，来映射成求$max_\\alpha(\\sum_{i=1}^n\\alpha_i-\\frac{1}{2}\\sum_{i,j=1}^n\\alpha_i\\alpha_jy_iy_jK(x_i^T,x_j)),0\\leq \\alpha_i\\leq C$,要解决的是在参数${\\alpha_1,\\alpha_2,...,\\alpha_n}$上求最大值的问题，至于$x_i$和$y_i$都是已知数。C由我们预先设定，也是已知数.所以我们需要求的其实就是所有的$\\alpha_i$使得上面的式子最大，求出以后，找到b，最后求出w.所以我们的思路必然是对于每个$\\alpha_i$求偏导数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4.2 固定一个参数带来的问题\n",
    "如果我们固定除$\\alpha_1$之外的所有参数，然后在$\\alpha_1$上求极值，会有问题。因为如果别的参数固定了，那么a1也就固定了，这是因为有一个条件约束,$\\because \\sum_{i=1}^n\\alpha_iy_i=0, \\therefore \\alpha_1y_1=-\\sum_{i=2}^n\\alpha_iy_i$,所以这个思路有问题，我们只能最少选择两个参数进行优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4.3 伪代码\n",
    "重复下面的过程知道收敛{\n",
    "- 选择两个拉格朗日乘子$\\alpha_i$和$\\alpha_j$\n",
    "- 固定其它拉格朗日乘子，只对$\\alpha_i$和$\\alpha_j$进行优化，会得到一个$\\alpha$的向量w($\\alpha$)\n",
    "- 根据优化后的$\\alpha_i$和$\\alpha_j$，更新截距b的值\n",
    "}\n",
    "\n",
    "迭代的停止条件是$\\alpha_i$和$\\alpha_j$基本没有改变，或者总得迭代次数达到了迭代次数上限"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4.4 选择$\\alpha_i$和$\\alpha_j$\n",
    "对于$\\alpha_i$的选择，主要根据启发法。启发法主要是根据KKT条件来选择，需要选择那些违反KKT条件的$\\alpha_i$选择出来,我们根据KKT条件，可以得出结论：\n",
    "\n",
    "$\\begin{cases}\n",
    "y_i(w^Tx_i+b) > 1 &\\alpha_i=0 \\\\\n",
    "y_i(w^Tx_i+b) < 1 &\\alpha_i=C \\\\\n",
    "y_i(w^Tx_i+b) = 1 &0<\\alpha_i<C\n",
    "\\end{cases}$\n",
    "\n",
    ",两个支撑平面外的点，符合要求1；对于支撑平面之间的点，符合要求2；对于落在支撑平面上的点，符合要求3；如果我们的样本不满足上面的三个条件，那么就是违反了KKT的条件,一般情况下，我们会根据是否违反KKT条件来选择$\\alpha_i$，那么$\\alpha_j$的选择就是随机选择一个\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4.5 优化\n",
    "既然选择了$\\alpha_i$和$\\alpha_j$，固定了其它的$\\alpha$，那么我们的对偶函数就变成了一个关于$\\alpha_i$和$\\alpha_j$的二元二次方程，具体推导如下：\n",
    "\n",
    "假设$K_{ij}=K(x_i^T,x_j)$\n",
    "$\\sum_{i,j=1}^n\\alpha_i\\alpha_jy_iy_jK_{ij}=\\alpha_1y_1\\sum_{j=1}^n\\alpha_jy_jK_{1j}+\\alpha_2y_2\\sum_{j=1}^n\\alpha_jy_jK_{2j}+\\sum_{i=3}^n\\sum_{j=1}^n\\alpha_i\\alpha_jy_iy_jK_{ij} \n",
    "$\n",
    "\n",
    "$=\\alpha_1y_1\\alpha_1y_1K_{11}+\\alpha_1y_1\\alpha_2y_2K_{12}+\\alpha_1y_1\\sum_{i=3}^n\\alpha_iy_iK_{1n}+\\alpha_2y_2\\alpha_1y_1K_{21}+\\alpha_2y_2\\alpha_2y_2K_{22}+\\alpha_2y_2\\sum_{i=3}^n\\alpha_iy_iK_{2n}+\\alpha_1y_1\\sum_{i=3}^n\\alpha_iy_iK_{n1}+\\alpha_2y_2\\sum_{i=3}^n\\alpha_iy_iK_{n2}$\n",
    "\n",
    "$+\\sum_{i=3}^n\\sum_{j=3}^n\\alpha_i\\alpha_jK_{ij}$\n",
    "\n",
    "$\\because y \\in \\{-1,1\\}, \\therefore y_1^2 = 1$\n",
    "\n",
    "$\\therefore \\Rightarrow K_{11}\\alpha_1^2+K_{22}\\alpha_2^2+2K_{12}y_1y_2\\alpha_1\\alpha_2+y_1\\alpha_1(\\sum_{i=3}^n\\alpha_iy_iK_{1n}+\\sum_{i=3}^n\\alpha_iy_iK_{n1})+y_2\\alpha_2(\\sum_{i=3}^n\\alpha_iy_iK_{2n}+\\sum_{i=3}^n\\alpha_iy_iK_{n2})+\\sum_{i=3}^n\\sum_{j=3}^n\\alpha_i\\alpha_jK_{ij}$\n",
    "\n",
    "$\\therefore \\psi=\\sum_{i=1}^n\\alpha_i-\\frac{1}{2}(K_{11}\\alpha_1^2+K_{22}\\alpha_2^2+2K_{12}y_1y_2\\alpha_1\\alpha_2+y_1\\alpha_1(\\sum_{i=3}^n\\alpha_iy_iK_{1n}+\\sum_{i=3}^n\\alpha_iy_iK_{n1})+y_2\\alpha_2(\\sum_{i=3}^n\\alpha_iy_iK_{2n}+\\sum_{i=3}^n\\alpha_iy_iK_{n2})+\\sum_{i=3}^n\\sum_{j=3}^n\\alpha_i\\alpha_jK_{ij})$\n",
    "\n",
    "$=\\alpha_1+\\alpha_2+\\sum_{i=3}^n\\alpha_i-\\frac{1}{2}K_{11}\\alpha_1^2-\\frac{1}{2}K_{22}\\alpha_2^2-K_{12}y_1y_2\\alpha_1\\alpha_2-\\nu_1y_1\\alpha_1-\\nu_2y_2\\alpha_2-\\varphi_{constant}$\n",
    "\n",
    "$\\psi_{min}=min_{\\alpha_1,\\alpha_2}(\\frac{1}{2}K_{11}\\alpha_1^2+\\frac{1}{2}K_{22}\\alpha_2^2+K_{11}y_1y_2\\alpha_1\\alpha_2+\\nu_1y_1\\alpha_1+\\nu_2y_2\\alpha_2+\\varphi_{constant}-\\alpha_1-\\alpha_2）$\n",
    "$\\because \\alpha_1y_2+\\alpha_2y_2=-\\sum_{i=3}^n\\alpha_iy_i, \\because y_1y_1=1$\n",
    "\n",
    "$\\therefore \\alpha_1y_1y_1+\\alpha_2y_2y_1=-y_1\\sum_{i=3}^n\\alpha_iy_i$\n",
    "\n",
    "$\\therefore \\Rightarrow \\alpha_1+y_1y_2\\alpha_2=-y_1\\sum_{i=3}^n\\alpha_iy_i$\n",
    "\n",
    "设 $S=y_1y_2$, $\\omega=-y_1\\sum_{i=3}^n\\alpha_iy_i, \\therefore \\alpha_1+S\\alpha_2=\\omega=\\alpha_1^*+S\\alpha_2^*$, $\\alpha_i^*$是经过优化后的新的$\\alpha_i$的值\n",
    "\n",
    "$\\therefore \\psi=\\frac{1}{2}K_{11}(\\omega-S\\alpha_2)^2+\\frac{1}{2}K_{22}\\alpha_2^2+K_{12}y_1y_2(\\omega-S\\alpha_2)\\alpha_2+\\nu_1y_1(\\omega-S\\alpha_2)+\\nu_2y_2\\alpha_2+\\varphi_{constant}-(\\omega-S\\alpha_2)-\\alpha_2$\n",
    "\n",
    "$=\\frac{1}{2}K_{11}(\\omega-S\\alpha_2)^2+\\frac{1}{2}K_{22}\\alpha_2^2+K_{12}S(\\omega-S\\alpha_2)\\alpha_2-\\omega+S\\alpha_2+\\nu_1y_1(\\omega-S\\alpha_2)+\\nu_2y_2\\alpha_2+\\varphi_{constant}-\\alpha_2$\n",
    "设$\\frac{\\partial{\\psi}}{\\partial{\\alpha_2}}=0$\n",
    "\n",
    "$-K_{11}\\omega{S}+K_{11}S^2\\alpha_2+K_{22}\\alpha_2+K_{12}S\\omega-2K_{12}S^2\\alpha^2+S-\\nu_1y_1S+\\nu_2y_2-1=0$\n",
    "$\\because S^2=1$\n",
    "\n",
    "$-SK_{11}(\\omega-S\\alpha_2)+K_{22}\\alpha_2+K_{12}S\\omega-2K_{12}\\alpha_2-\\nu_1y_1S+S+\\nu_2y_2-1=0$\n",
    "\n",
    "$\\alpha_2(K_{11}+K_{22}-2K_{12})=S(K_{11}-K_{12})\\omega+y_2(\\nu_1-\\nu_2)+1-S$\n",
    "\n",
    "我们需要的是不停的迭代$\\alpha_i$，所以我们需要找到$\\alpha_2$与$\\alpha_2^*$之间的关系，将$\\omega=\\alpha_1^*+S\\alpha_2^*$代入，则：\n",
    "\n",
    "$S(K_{11}-K_{12})\\omega=S(K_{11}-K_{12})(\\alpha_1^*+S\\alpha_2^*)$\n",
    "\n",
    "$=S(K_{11}\\alpha_1^*+K_{11}S\\alpha_2^*-K_{12}\\alpha_1^*-K_{12}S\\alpha_2^*)$\n",
    "\n",
    "$=SK_{11}\\alpha_1^*+K_{11}S^2\\alpha_2^*-SK_{12}\\alpha_1^*-K_{12}S^2\\alpha_2^*$\n",
    "\n",
    "$=SK_{11}\\alpha_1^*-SK_{12}\\alpha_1^*+K_{11}\\alpha_2^*-K_{12}\\alpha_2^*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\because \\nu_1=\\mu_1+b^*-y_1\\alpha_1^*K_{11}-y_2\\alpha_2^*K_{21}, \\nu_2=\\mu_2+b^*-y_2\\alpha_2^*K_{12}-y_2\\alpha_2^*K_{22}$\n",
    "\n",
    "代入$\\nu_1-\\nu_2$\n",
    "\n",
    "$\\mu_1+b^*-y_1\\alpha_1^*K_{11}-y_2\\alpha_2^*K_{21}-(\\mu_2+b^*-y_2\\alpha_2^*K_{12}-y_2\\alpha_2^*K_{22})$\n",
    "\n",
    "$=\\mu_1+b^*-y_1\\alpha_1^*K_{11}-y_2\\alpha_2^*K_{21}-\\mu_2-b^*+y_2\\alpha_2^*K_{12}+y_2\\alpha_2^*K_{22}$\n",
    "\n",
    "$=\\mu_1-\\mu_2-y_1\\alpha_1^*K_{11}-y_2\\alpha_2^*K_{21}+y_2\\alpha_2^*K_{12}+y_2\\alpha_2^*K_{22}$\n",
    "\n",
    "$\\therefore y_2(\\nu_1-\\nu_2)=y_2(\\mu_1-\\mu_2-y_1\\alpha_1^*K_{11}-y_2\\alpha_2^*K_{21}+y_1\\alpha_1^*K_{12}+y_2\\alpha_2^*K_{22})$\n",
    "\n",
    "$=y_2\\mu_1-y_2\\mu_2-S\\alpha_1^*K_{11}-K_{21}\\alpha_2^*+S\\alpha_1^*K_{12}+K_{22}\\alpha_2^*$\n",
    "\n",
    "$S(K_{11}-K_{12})\\omega+y_2(\\nu_1-\\nu_2)+1-S=SK_{11}\\alpha_1^*-SK_{12}\\alpha_1^*+K_{11}\\alpha_2^*-K_{12}\\alpha_2^*+y_2\\mu_1-y_2\\mu_2-S\\alpha_1^*K_{11}-K_{21}\\alpha_2^*+S\\alpha_1^*K_{12}+K_{22}\\alpha_2^*+1-S$\n",
    "\n",
    "$=\\alpha_1^*(SK_{11}-SK_{12}-SK_{11}+SK_{12})+\\alpha_2^*(K_{11}-K_{12}-K_{21}+K_{22})+y_2\\mu_1-y_2\\mu_2+1-S$\n",
    "\n",
    "$\\alpha_2^*(K_{11}-K_{12}-K_{21}+K_{22})+y_2\\mu_1-y_2\\mu_2+y_2y_2-y_1y_2$\n",
    "\n",
    "$\\therefore \\alpha_2^{new,unclipped}=\\alpha_2^{old}+\\frac{y_2(E_1-E_2)}{\\eta}$\n",
    "\n",
    "其中，$\\eta=K_{11}-2K_{12}+K_{22}$, $E_i=f(x_i)-y_i$\n",
    "\n",
    "$\\because 0\\leq \\alpha_i\\leq C$\n",
    "\n",
    "$\\therefore$如果$y_1y_2=-1$,则$max=(0, \\alpha_i^{old}-\\alpha_j^{old})$;$min=(C,C+\\alpha_i^{old}-\\alpha_j^{old})$\n",
    "\n",
    "如果$y_1y_2=1$,则$max=(0, \\alpha_i^{old}+\\alpha_j^{old}-C)$;$min=(C, \\alpha_i^{old} + \\alpha_j^{old})$\n",
    "\n",
    "根据这两个条件，可以真正求出一个$\\alpha_i^{new, clipped}$,这个就是我们优化后的$\\alpha_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4.6 具体做法\n",
    "设定样本数量为m,初始化一个长度为m的数组$\\alpha$，元素都是0。初始化b为0\n",
    "整体是一个循环，次数就是设置的默认迭代次数，循环内部分为4步：\n",
    "- 选择i\n",
    "- 选择j\n",
    "- 更新$\\alpha_i$,$\\alpha_j$\n",
    "- 更新b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###4.6.1 选择i\n",
    "选择i的过程主要是根据KKT条件，我们根据6.4的推导，可以知道我们的KKT条件如下：\n",
    "\n",
    "$\\begin{cases}\n",
    "y_i(w^Tx_i+b) > 1 &\\alpha_i=0 \\\\\n",
    "y_i(w^Tx_i+b) < 1 &\\alpha_i=C \\\\\n",
    "y_i(w^Tx_i+b) = 1 &0<\\alpha_i<C\n",
    "\\end{cases}$\n",
    "\n",
    "循环这m个样本去选择违反KKT条件的i：\n",
    "- 如果$ 0<\\alpha_i<C $,那么找到第一条$y_i(w^Tx_i+b) != 1$的记录，返回i\n",
    "- 如果$\\alpha_i=0$，那么找到第一条$y_i(w^Tx_i+b) < 1$的记录，返回i\n",
    "- 如果$\\alpha_i=C$，那么找到第一条$y_i(w^Tx_i+b) < 1$的记录，返回i\n",
    "\n",
    "如果没有找到违反KKT条件的i，那么说明所有的$\\alpha$都优化好了，那么就退出循环"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###4.6.2 选择j\n",
    "选择j很简单，就是选择任意一个非i的index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###4.6.3 优化$\\alpha_i$和$\\alpha_j$\n",
    "这一步主要计算$\\alpha_j^{new,clipped}$与$\\alpha_i$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####4.6.3.1 核函数计算-$K_{ij}$\n",
    "有固定的公式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####4.6.3.2 计算$E_i$和$E_j$\n",
    "$f(x_i)$:$\\sum_{p=1}^m\\alpha_py_pK_{pi}+b^{old}, f(x_j)$:$\\sum_{p=1}^m\\alpha_py_pK_{pj}+b^{old}$\n",
    "\n",
    "$E_i=f(x_i)-y_i,E_j=f(x_j)-y_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####4.6.3.3 计算$\\eta$\n",
    "$\\eta=K_{ii}-2K_{ij}+K_{jj}$\n",
    "\n",
    "$\\alpha_j^{new,unclipped}=\\alpha_j^{old}+\\frac{y_j(\\sum_{p=1}^m\\alpha_py_pK_{pi}+b^{old}-(\\sum_{p=1}^m\\alpha_py_pK_{pj}+b^{old}))}{K_{ii}-2K_{ij}+K_{jj}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####4.6.3.4 调整$\\alpha_j$\n",
    "如果$y_1y_2=-1$,则$max=(0, \\alpha_i^{old}-\\alpha_j^{old})$;$min=(C,C+\\alpha_i^{old}-\\alpha_j^{old})$\n",
    "\n",
    "如果$y_1y_2=1$,则$max=(0, \\alpha_i^{old}+\\alpha_j^{old}-C)$;$min=(C, \\alpha_i^{old} + \\alpha_j^{old})$\n",
    "\n",
    "可以计算出$\\alpha_j^{new,clipped}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####4.6.3.5 计算$\\alpha_i$\n",
    "$\\alpha_i^{new}=\\alpha_i^{old}+(\\alpha_j^{old}-\\alpha_j^{new,clipped})*y_i*y_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###4.6.4 更新$b^{new}$\n",
    "$b_i=b^{old}+y_i-f(x_i)$,$b_j=b^{old}+y_j-f(x_j)$",
    "\n",
    "$\\begin{cases}\n",
    "b^{new}=b_i &0<\\alpha_i<C \\\\\n",
    "b^{new}=b_j &0<\\alpha_j<C \\\\\n",
    "b^{new}=\\frac{b_i+b_j}{2} &otherwise\n",
    "\\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###4.6.5 计算w\n",
    "迭代完成后，会得到一个新的$\\alpha$数组，其中大部分项都是0，只有极个别的几个元素不是0，这几个元素就是支撑向量对应的$\\alpha$,那么$w=\\sum_{i=1}^m\\alpha_iy_ix_i$\n",
    "\n",
    "$\\therefore$最后的分割面就是$\\sum_{i=1}^nw_ix_i+b=0$,支撑平面就是$\\sum_{i=1}^nw_ix_i+b=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#5 举例说明\n",
    "给定3个数据点：正例点$x_1=(3,3)$，$x_2=(4,3)$，负例点$x_3=(1,1)$，求线性可分支撑向量机\n",
    "\n",
    "目标函数$min_\\alpha\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^n\\alpha_i\\alpha_jy_iy_j(x_i \\bullet x_j) - \\sum_{i=1}^n\\alpha_i$，根据题目可以得出完整的样本点应该是$(3,3,1),(4,3,1),(1,1,-1)$\n",
    "\n",
    "带入目标函数有$\\frac{1}{2}(18\\alpha_1^2+25\\alpha_2^2+2\\alpha_3^2+42\\alpha_1\\alpha_2-12\\alpha_1\\alpha_3-14\\alpha_2\\alpha_2)-\\alpha_1-\\alpha_2-\\alpha_3$，并且$\\alpha_1+\\alpha_2-\\alpha_3=0$，此时，将$\\alpha_3=\\alpha_1+\\alpha_2$带入目标函数，得到一个关于$\\alpha_1,\\alpha_2$的函数如下：\n",
    "\n",
    "$s(\\alpha_1,\\alpha_2)=4\\alpha_1^2+\\frac{13}{2}\\alpha_2^2+10\\alpha_1\\alpha_2-2\\alpha_1-2\\alpha_2$，对$\\alpha_1,\\alpha_2$分别求偏导数令其结果为0，可以得到一个关于$\\alpha_1,\\alpha_2$的二元一次方程组$\\begin{cases}\n",
    "4\\alpha_1+5\\alpha_2=1\\\\\n",
    "10\\alpha_1+13\\alpha_2=2\n",
    "\\end{cases}$，求解得$\\alpha_1=1.5,\\alpha_2=-1$。但是此时不满足条件$\\alpha_2 \\geq 0$，所以只能在边界值上得到最小.在$\\alpha_1=\\frac{1}{4},\\alpha_2=0$时达到最小，此时$\\alpha_3=\\frac{1}{4}$\n",
    "\n",
    "$\\omega*=\\sum_{i=1}^N\\alpha_i^*y_ix_i,b^*=y_i-\\sum_{i=1}^N\\alpha_i^*y_i(x_i,x_j)$，得到$\\omega_1=\\omega_2=0.5,b=-2$，因此分离超平面为$\\frac{1}{2}x_1+\\frac{1}{2}x_2-2=0$"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}