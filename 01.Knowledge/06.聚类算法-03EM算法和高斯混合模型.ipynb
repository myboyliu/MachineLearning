{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#EM(期望最大)算法与高斯混合模型(GMM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1 概述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1.1 概述\n",
    "EM算法本质上来说是一种寻找问题最优解的优化算法，高斯混合模型是EM算法的一个典型的应用场景。\n",
    "常见的寻找问题最优解的方法：\n",
    "- EM算法\n",
    "- 梯度下降法 - 线性回归\n",
    "- 坐标上升法 - 支持向量机\n",
    "- 极大似然估计 - 逻辑回归\n",
    "\n",
    "EM算法是一个无监督算法，一种聚类的算法。这个算法思想简单，但是推导复杂。它的本质就是极大似然估计法求解未知参数的最优解。极大似然估计是求解参数的最优解，EM算法对其进行了推广，能够叫它处理更复杂的问题，这些问题中多了一些隐变量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1.2 应用场景\n",
    "- 高斯混合模型\n",
    "- K-Means聚类\n",
    "- HMM-隐马尔可夫模型\n",
    "\n",
    "比如观察人的身高，符合正态分布，那么参数就有两个$(\\mu,\\sigma^2)$,我们现在有100个男性的身高和100个女性的身高,那么求解均值和标准差的过程就是估计高斯分布的参数,可以直接根据公式$\\hat{\\mu}=\\frac{1}{n}\\sum_{i=1}^nx_i=\\overline{x}$,$\\hat{\\sigma}^2=\\frac{1}{n}\\sum_{i=1}^n(x_i-\\overline{x})^2$.\n",
    "\n",
    "上述问题中，我们是性别已知的，但是如果我们收集了200个人的身高，而性别未知，又该如何处理？这样就涉及到了两个分布、两个分布的参数.那么首先需要先性别估计，然后才能估计高斯分布.这个时候就需要用到EM算法，性别估计的参数就是隐变量\n",
    "\n",
    "这个时候，对于每一个样本或者你抽取到的人，就有两个东西需要猜测或者估计的了，一是这个人是男的还是女的？二是男生和女生\n",
    "对应的身高的高斯分布的参数是多少？\n",
    "\n",
    "只有当我们知道了哪些人属于同一个高斯分布的时候，我们才能够对这个分布的参数作出靠谱的预测，例如刚开始的最大似然所说的\n",
    "，但现在两种高斯分布的人混在一块了，我们又不知道哪些人属于第一个高斯分布，哪些属于第二个，所以就没法估计这两个分布的\n",
    "参数。反过来，只有当我们对这两个分布的参数作出了准确的估计的时候，才能知道到底哪些人属于第一个分布，那些人属于第二个\n",
    "分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1.3 EM算法和坐标上升法的对比\n",
    "EM算法就是这样，假设我们想估计知道A和B两个参数，在开始状态下二者都是未知的，但如果知道了A的信息就可以得到B的信息，反过来知道了B也就得到了A。可以考虑首先赋予A某种初值，以此得到B的估计值，然后从B的当前值出发，重新估计A的取值，这个过程一直持续到收敛为止。\n",
    "\n",
    "这里把每个人的完整描述看做是三元组$y_i=\\{x_i,z_{i1},z_{i2}\\}$，其中，$x_i$是第i个样本的观测值，也就是对应的这个人的身高，是可以观测到的值。$z_{i1}$和$z_{i2}$表示男生和女生这两个高斯分布中哪个被用来产生值$x_i$，就是说这两个值标\n",
    "记这个人到底是男生还是女生（的身高分布产生的）。这两个值我们是不知道的，是隐含变量。确切的说，$z_{ij}$在$x_i$由第j\n",
    "个高斯分布产生时值为1，否则为0。例如一个样本的观测值为1.8，然后他来自男生的那个高斯分布，那么我们可以将这个样本表示为{1.8, 1, 0}。如果$z_{i1}$和$z_{i2}$的值已知，也就是说每个人我已经标记为男生或者女生了，那么我们就可以利用上面说的最大似然算法来估计他们各自高斯分布的参数。但是它们未知，因此我们只能用EM算法\n",
    "\n",
    "- 坐标上升法：假设我们想估计知道A和B两个参数，在开始状态下二者都是未知的，但如果知道了A的信息就可以得到B的信息，反过来知道了B也就得到了A\n",
    "- EM算法：首先赋予A初始值，以此得到B的估计值，然后从B的当前值触发，重新估计A的取值，这个过程一直持续到收敛为止\n",
    "\n",
    "EM算法本质上就是一个坐标上升法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1.4 EM算法和K-Means算法的对比\n",
    "K-Means算法是一种聚类算法，它最后得到的结果会把样本唯一的分到一类中，但无法给出某个样本属于该簇的后验概率。而EM算法最后得到的是这个样本属于任何一种分类的概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1.5 混合高斯模型\n",
    "随机变量X是有K个高斯分布混合而成，取各个高斯分布的概率为$\\pi_1,\\pi_2,...,\\pi_K$，第i个高斯分布的均值为$\\mu_i$，方差为$\\sum_i$。若观测到随机 变量X的一系列样本$x_1,x_2,...,x_n$，试估计参数$\\pi,\\mu,\\sum$。\n",
    "\n",
    "高斯分布是指数族分布，但是混合高斯模型就不是指数族分布了。\n",
    "\n",
    "若$x_1,x_2,...,x_n$中，每一个都是一个n维的向量，那么$\\mu_i$也是一个n维的向量，方差$\\sum_i$就是一个$n*n$的协方差矩阵(对称的，半正定的矩阵)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2 混合高斯模型解法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2.1 似然函数与对数似然\n",
    "- 似然函数:$L_{\\pi,\\mu,\\sum}(x)=\\prod_{i=1}^N\\prod_{k=1}^K\\pi_kN(x_i|\\mu_k,\\sum_k)$\n",
    "- 对数似然:$\\ell_{\\pi,\\mu,\\sum}(x)=\\sum_{i=1}^Nlog[\\sum_{k=1}^K\\pi_kN(x_i|\\mu_k,\\sum_k)]$\n",
    "\n",
    "如果给定了$\\mu_k,\\sum_k$的时候，算第i个样本$x_i$属于第k个高斯分布的概率就是$N(x_i|\\mu_k,\\sum_k)$\n",
    "\n",
    "对于在对数函数里面又有加和，无法直接用求导解方程的办法直接求得最大值。为了解决这个问题，我们分成两步"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2.2 第一步-估算数据来自哪个组份\n",
    "估计数据由每个组份生成的概率:对于每个样本$x_i$,它由第k个组份生成的概率为$\\gamma(i,k)=\\frac{\\pi_kN(x_i|\\mu_k,\\sum_k)}{\\sum_{j=1}^K\\pi_jN(x_i|\\mu_j,\\sum_j)}$\n",
    "\n",
    "上式中的$\\mu$和$\\sum$也是待估计的值，因此采样迭代法: 在计算$\\gamma(i,k)$时假定$\\mu$和$\\sum$已知;\n",
    "- 需要先验给定$\\mu$和$\\sum$。\n",
    "- $\\gamma(i,k)$亦可看成组份k在生成数据$x_i$时所做的贡献"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2.3 第二步-估计每个组份的参数\n",
    "对于所有的样本点，对于组份k而言，可看做生成了$\\{\\gamma(i,k)x_i|i=1,2,...,N\\}$这些点。组份k是一个标准的高斯分布，利用上面的结论\n",
    "$$\\begin{cases}\n",
    "N_k=\\sum_{i=1}^N\\gamma(i,k)\\\\\n",
    "\\mu_k=\\frac{1}{N}\\sum_{i=1}^N\\gamma(i,k)x_i\\\\\n",
    "\\sum_k=\\frac{1}{N_k}\\sum_{i=1}^N\\gamma(i,k)(x_i-\\mu_k)(x_i-\\mu_k)^T\\\\\n",
    "\\pi_k=\\frac{N_k}{N}=\\frac{1}{N}\\sum_{i=1}^N\\gamma(i,k)\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3. EM算法思想\n",
    "对于未知性别的估计，高斯概率密度函数要求知道两个参数，就可以知道具体的性别；极大似然要求知道性别，就能知道高斯分布参数。这就陷入了死循环。所以就引入了EM算法。\n",
    "\n",
    "假定有训练集$\\{x^{(1)},x^{(2)},...,x^{(m)}\\}$,包含m个独立样本，希望从中找到该组数据的模型p(x,z)的参数。其中x是已经观察到的参数，z是未观测到的参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3.1 通过最大似然估计建立目标函数\n",
    "- 似然函数$L(\\theta)=\\prod_p(x;\\theta)$\n",
    "- 对数似然$\\ell(\\theta)=\\sum_{i=1}^mlogp(x;\\theta)=\\sum_{i=1}^mlog\\sum_zp(x,z;\\theta)$\n",
    "\n",
    "z是隐随机变量，不方便直接找到参数估计。策略:计算$\\ell(\\theta)$下界，求该下界的最大值; 重复该过程，直到收敛到局部最大值\n",
    "\n",
    "本质上，我们需要使得上述式子最大化，但是可以看到里面有“和的对数”，求导后形式会非常复杂（自己可以想象下$log(f_1(x)+ f_2(x)+ f_3(x)+…)$复合函数的求导），所以很难求解得到未知参数z和$\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3.2 Jenson不等式\n",
    "令$Q_i$是z的某一个分布，其中$Q_i > 0$，则对数似然函数为：$\\ell(\\theta)=\\sum_{i=1}^mlogp(x;\\theta)=\\sum_{i=1}^mlog\\sum_zp(x,z;\\theta)=\\sum_ilog\\sum_{z^{{i}}}Q_i(z^{(i)})*\\frac{p(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})}$\n",
    "\n",
    "对于一个随机变量X：\n",
    "- 如果f是凸函数，那么$E[f(X)]>=f(E[X])$\n",
    "- 如果f是凹函数，那么$E[f(X)]<=f(E[X])$\n",
    "\n",
    "E就是期望\n",
    "\n",
    "我们知道$E[f(X)]=\\sum\\limits_i^np_iz_i$, 其中$\\sum\\limits_i^np_i=1$，其含义就是对于函数f(x)上每个可能取值的点的一个加权平均，那么f(E[X])的含义是，对于可能取值的X的加权平均，然后在求解函数值,$logY$是一个凹函数，如下所示的就是凹函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWwAAAD6CAYAAACF131TAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAGjVJREFUeJzt3Xt0VeWdxvFvREFFAihgGESYaitUQCwHCfdguUqdBVap\nQJEqGhFhVal1phYdFRy8tS5nGWxjiXJrCVoRCgjEYuSqkFgBcawwI1C5mZhwx1z3/PErcos5B5Jz\n3r3PeT5rZRHO2Ym/9brz5OXd7yXJ8zwPERHxvfNcFyAiIpFRYIuIBIQCW0QkIBTYIiIBocAWEQkI\nBbaISEAosEVEAkKBLSISEApsEZGAOL82v1mTJk1o3bp1bX5LEZG4t337dgoLC8NeV6uB3bp1a/Ly\n8mrzW4qIxL1QKBTRdRoSEREJCAW2iEhAKLBFRAJCgS0iEhAKbBGRgFBgi4gEhAJbRCQganUetohI\nQjl0CPLz4ehRuOmmqP/nFNgiIpEoL4eDB+HSS6G4GHr2hE8+Ac+Dtm0V2CIizuzcCR98YB/r11tP\n+pZbYNYsaNQI2rWD226DLl2gc+eYlKTAFhHZvx82bICCAhgxwl676SbYsgXq1YMf/ADuuQf697f3\nkpJg7tyYl6nAFpHEtHAh/PnP1nv+9FN77dJLYfhwC+T//m9o2BDat4e6dd3W+k8KbBGJbwUFsHYt\nrFkD778PS5fCxRfDunX2eZcuMHLkiaGNpCT7uhtvdFt3FRTYIhI/Kivt4/zz4e234ec/h61b7b26\ndaFTJwvwVq3g8cfhv/7rREAHgAJbRILr6FEbez7eg167FrKyYMgQaNrUZm/cfTd06wahEFx44Ymv\nrVfPXd3nSIEtIsGxZw+UlEDr1rBjB1x9tU23AwvnW26BFi3s76EQLFjgrNRoUGCLiH9t3gyrV5/o\nPX/+Odx5p/Wir7wSHnnExp27doXLLnNdbdQpsEXEH8rL4cMPYdcuGDrUXvvxj20MOiUFuneHCRNO\nPAxMSoInnnBXrwMKbBFxZ9MmWLIE3nvPetKHD0OTJjYGnZQEr71mYf2v/xqoh4PRos2fRCQ2Sktt\nWGPqVPscYOZM+NWv4B//gDvugHnz4OOPT4Rzt27wne8orP8poh72vn37GDhwIH/729+iXY+IxJPt\n22H2bOtBr11rszrAVgx26gS/+AX8+7/bjA4JK6LAfuihhzh27Fi0axGRIPv6a1s1mJsLAwbYQpQd\nO+DRR6FDB5te17s39Oplwx4AzZs7LTlowgb2ihUrqF+/PikpKbGoR0SC5OhReO45C+l162zKXVIS\nXHKJBXa3blBYmBAzOGKh2sAuLS3lySef5K233mLIkCFVXpOZmUlmZiYABQUFtV+hiPiD59mMjeXL\nbdVgerotPsnIgJYtYdw4SEuzbUcbN7avueAChXUtqjawn376ae6//34aNWr0rdekp6eTnp4OQCgU\nqt3qRMS9xYth/nzIybEtRwF++EML7Dp1bNjjoovc1pggqp0l8s4775CRkUFaWhofffQRd999d6zq\nEhEXSkpgxQqYMsV61ADZ2fDGG/aQcNo02LYN3nnnxNcorGMmyfOO/1+pXlpaGrm5udVeEwqFyMvL\nq426RCRWduywbUaXL4eVK+HYMds8ads22ySpqAiSk+01iYpIszPiedjhwlpEAmLPHpv//L//a3/f\nsMGm1+3YYTM5Fi60kG7Vyt6/9FKFtU/o/4JIvCsvh1WrbCx62TJbmALwm9/AxIkwaJCNTbds6bZO\nCUuBLRKPCgth7147d/DYMZsXnZRkc6BHjYJ+/eC66+za+vXtQ3xPgS0SDzzPdrZbtMh60uvW2Q52\na9ZAgwb2ILFjR5sfLYGlwBYJqtLSE2cNjhoFc+bY56EQ/Od/wo9+dOLaHj1iX5/UOgW2SJDs2mU9\n6EWL4N134f/+z/bhGD7cFq0MHqzl3nFMgS0SBB98APfdB8c3YGvdGn72sxO73g0e7KoyiSEFtojf\nHD5sc6IXLbIg/vGP4fLL7aTvp5+2oY7vf19bjiYgBbaIH3ge/OlP8PrrsHSp7XzXqJHtcgfWo169\n2mmJ4p4CW8SV/fvtxJVevay3PHWqLVi55x47TLZ7d9s8SeSfFNgisVRUZCd5v/GGbaZUrx58+aXt\nx7F0qT0wPE8HQUnVFNgisfLKK7YFaXm5DXH8/Odw660W2gAtWjgtT/xPgS0SDfv22Zakb7xhZxb+\n8Idwww22Z8ett9rOd3poKGdJgS1SW0pK4A9/sJBeuRIqK+Gaa2zWB9hS8OPLwUXOgQJbpCa++MIW\nr/TqZTvaTZ5sJ6w8+qj1pK+9Vj1pqTUKbJGzdfCg9aJnzLCe9BVX2G53derYfh46AVyiRI+jRc5G\nRgakpMCYMbav9OTJ8Ne/nuhFK6wlitTDFqnOli3Wk77rLmjTxlYYjh5tH126aLhDYkqBLXK6ggL4\n4x/tVJYPP7Sx6e9/3wK7Tx/7EHFAgS1ysq+/hquugkOHbOrdiy/C7bdDs2auKxNRYEsC8zzbBW/G\nDDvfcPlyuPBCePllm37Xrp3rCkVOocCWxPPFFxbSM2fCZ5/ZsvChQ20edb16MHKk6wpFqqRZIpIY\nDh06sYBl+XKYNMn27Zg+3c4+nDPnxBJxEZ9SYEt827wZxo61cM7KsteGDYPPP4fcXJv9kZzstESR\nSGlIROKP59nClpdesoUtF15oR2j17GnvX3KJDqOVQFJgS/w4dMhOCE9KsgUu//gHPPus9aIvu8x1\ndSI1psCWYPM8WLPGAnrhQpvtkZICc+faqsM6dVxXKFJrFNgSTEeP2oPCjAzYuBEaNoR77z2x8jAl\nxW19IlGgwJZgKSuzY7O+/NICun17yMyEESOgfn3X1YlElQJb/K+iwo7PeuklWyb+l7/YiS0ffwxt\n22o/D0kYmtYn/lVUBM8/D9/7HvzoRzb0ccMNNm4Ntr+HwloSiAJb/Od4IP/ud/DLX9pZh9nZsGOH\nHQygkJYEpcAWf/A8G/bo2dPCGWyMeuNGm0s9bJiNXYskMAW2uOV5Nh3vhhtg0CDrRZ//z0crl10G\nHTq4rU/ER/TQUdy69VZ48034znfglVfgjjugbl3XVYn4knrYElvl5fCnP8GRI/b3UaNs57y//x3u\nvlthLVINBbbERlmZbb7Utq3NmZ47114fMsR61efrH3si4SiwJboqK222x3e/awfXNmhgQyB33um6\nMpHAiSiwi4qKyMnJobCwMNr1SLyorLQ/k5JsCXlKCixaBPn5dljAeeoriJytsD81e/bsYfDgwaxf\nv54+ffpQUFAQi7okqA4ftsUuV11lBwMkJdnKxHXrYPBgzaEWqYGwA4dbtmzhhRdeIDU1leLiYj78\n8EMGDBgQi9okSA4csKXjL7wAX30F/frBwYPWs27UyHV1InEhbGD37dsXgJUrV7J+/Xoee+yxqBcl\nAXPggE3LKyqyXvSkSZCa6roqkbgT0UCi53lkZ2dzwQUXUOe0/YUzMzMJhUKEQiENlySSwkKYNcs+\nb9jQQjo/38apFdYiUZHkecc3bgjv0UcfpV27dvzkJz+p8v1QKEReXl6tFSc+VFYG06bB44/bCS/b\nt8MVV7iuSiTQIs3OsD3sZ555hpkzZwKwf/9+Gmk8MnHl5EDHjvDAA9C5s+3zobAWiZmwgZ2ens6s\nWbPo1asXFRUV9O/fPxZ1id989ZVNxyspgQULYNkyuPZa11WJJJSwDx0bN25MTk5OLGoRvzl0CGbP\nhrFjbSOmnBz4wQ+gXj3XlYkkJK1ekDNVVtoDxWuugXHjYP16e71rV4W1iEMKbDnVhg3Qvbvt73HF\nFbbgpUsX11WJCNpeVU5WVmbbnZaUwKuvWmhrCbmIb+inMdGVlNg0vZISO9Hlrbfgs8/gZz9TWIv4\njH4iE5Xn2SKXdu3g/vtt5gfA9ddDcrLb2kSkSgrsRPTpp3DTTXDzzbYP9dtv25mJIuJrGsNORHff\nDZs3w29/C+PH63BbkYBQYCeCykp47TXrUTdtCtOnQ+PG0KyZ68pE5CxoSCTeffSRnUg+ZozN/ACb\nX62wFgkcBXa8qqy0vam7dIFdu+zUl1/+0nVVIlIDCux49eSTMHEiDBxo49UjRui0F5GA0xh2vCkr\ns4eIY8dCixb2gFFBLRIX1MOOFyUltu3pgAFQUWFHc91zj8JaJI4osOPB//yPjVW/+CK0b2+BLSJx\nR4EdZJ4Hv/89dOoEu3fbysUXX4S6dV1XJiJRoDHsIDt6FJ55Bnr2hBkzbBhEROKWAjuI1qyxXnX9\n+rBqFTRvro2aRBKAfsqDpKwMfvUr61E//7y91qKFwlokQaiHHRTbttlc6g0bbKregw+6rkhEYkyB\nHQR/+YuF9fnnw+uv2yEDIpJw9G/pILj6aujRAzZtUliLJDAFtl+tWwcPP2xT99q2tT2rW7Z0XZWI\nOKTA9puKCpg82R4svv46FBa6rkhEfEKB7Sc7d0KfPvDYY3YCzEcf2f7VIiLooaN/lJZCr17w1Ve2\nCGbUKO0DIiKnUGD7Rd26tmf15ZfbQ0YRkdMosF1buBB27IAJE6B7d9fViIiPaQzbpT/+EW65xXrW\nZWWuqxERn1Ngu5KZCT/9qc0GycnRyeUiEpYC24Xf/AbuvRcGDYIlS6BBA9cViUgAKLBdqFvXpu3N\nnw8XXeS6GhEJCAV2rHge/P3v9vmECTB3rg4aEJGzosCOhYoKO1+xUyf4/HN7TXOsReQsKbCjrbTU\ndtqbPh1+8Qto3dp1RSISUJqHHU3HjsFtt8HixfDcc/DQQ64rEpEAU2BH00sv2SyQ3/3OZoWIiNSA\nAjuaHnwQOneGtDTXlYhIHAg7hn3gwAEGDRpEv379GDp0KKWlpbGoK7j27oWhQ2HPHjshRmEtIrUk\nbGDPmTOHiRMnkpOTQ0pKCkuXLo1FXcG0c6ftuLd8uZ3BKCJSi8IOiYwbN+6bzwsKCmjWrFlUCwqs\nzz6Dvn3h4EFbat6tm+uKRCTORDyGvW7dOoqLi0lNTT3l9czMTDIzMwEL9IT0ySdw441QWQm5udCx\no+uKRCQORTQPu6ioiAkTJpCVlXXGe+np6eTl5ZGXl0fTRD0d5fLLLaRXrlRYi0jUhO1hl5aWMmzY\nMKZOnUqrVq1iUVNw5OdDu3Zw2WWgsX0RibKwPezp06eTn5/PU089RVpaGtnZ2bGoy/8WLbIDByZN\ncl2JiCSIJM/zvNr6ZqFQiLy8vNr6dv41bx6MHAnXXWc96yZNXFckIgEWaXZqL5GztXGjHZCbmgor\nViisRSRmFNhnw/NgzBgbs37zTUhOdl2RiCQQLU0/G0lJMGsWFBVBos6IERFnFNiR2rkTWraEtm1d\nVyIiCUpDIpHYudMeME6Z4roSEUlgCuxwysttRkhFhR1EICLiiIZEwpkyBVavhtmz4aqrXFcjIglM\nPezqrFoFkyfDHXdYL1tExCEFdnUOH7YDCF56yXUlIiIK7GoNGgTr1kGDBq4rERFRYFcpKwuef962\nS01Kcl2NiAigwD7TJ5/A+PF2aoyIiI8osE/29ddw++1wySUwYwacp+YREf/QtL6TPfwwbN4MixdD\n8+auqxEROYW6kMdt3QoZGfDAA3DTTa6rERE5g3rYx333u7BmDVx/vetKRESqpB52ZSWsX2+fp6ZC\nvXpu6xER+RYK7GeftaDesMF1JSIi1UrswP7gA3j0UbjtNgiFXFcjIlKtxA3sAwdg+HBo0QJ+/3st\nkBER30vMh46eB+PGwY4dsHIlNGrkuiIRkbASM7ABunaFDh2ge3fXlYiIRCTxAtvzbPhj/HjXlYiI\nnJXEGsMuLYX+/WH+fNeViIictcQK7EmT4J139IBRRAIpcQJ7+XJ47jkYOxaGDHFdjYjIWUuMwP7y\nSzvm69pr4be/dV2NiMg5SYyHjtnZsH8/5OTARRe5rkZE5JwkRg97wgTYsgXat3ddiYjIOYvvwN60\nyfa3BrjqKre1iIjUUHwH9n/8B9x8M5SXu65ERKTG4jewv/gCli2Dn/4Uzk+MoXoRiW/xG9gzZ9pe\n13fe6boSEZFaEZ+BXVkJWVmQlqaxaxGJG/EZ2Fu3wu7dMGaM60pERGpNfA7uXnMN7Nmj475EJK7E\nX2BXVsJ550HDhq4rERGpVfE3JPLKK3bcV1GR60pERGpVRIG9b98+evbsGe1aakdWFpSUQOPGrisR\nEalVYQO7uLiY0aNHc+TIkVjUUzMffwzr18Ndd2kLVRGJO2EDu06dOmRnZ5OcnByLemrm1Vfhggts\nsYyISJwJ+9AxXFBnZmaSmZkJQEFBQe1UdS5KS22xzL/9GzRt6q4OEZEoqfEskfT0dNLT0wEIhUI1\nLuiceR48+6xN6RMRiUPxM62vXj0tQxeRuBYf0/p274YXX7RDCkRE4lTEgZ2bmxvFMmpoxgx44AEo\nLHRdiYhI1AS/h+15Nve6d2+4+mrX1YiIRE3wA3vVKti2zeZei4jEseAHdlYWNGgAt97quhIRkagK\ndmB7no1bjxgBF1/suhoRkagK9rS+pCRYtAgqKlxXIiISdcHuYX/1lf1Zp47bOkREYiC4gb1lC6Sk\nwPz5risREYmJ4Ab2q6/an927u61DRCRGghnYJ2/01KyZ62pERGIimIG9eDEUFGjutYgklGAGdlYW\n/Mu/wIABrisREYmZYE7ry8iw1Y3nB7N8EZFzEczEu/JK+xARSSDBGhLxPLjnHnjvPdeViIjEXLAC\ne/Vq+MMfYPt215WIiMRcsAJbGz2JSAILTmAfPAjz5sHtt0P9+q6rERGJueAE9rx5cPSo5l6LSMIK\nTmBffDHcfDN06eK6EhERJ4IT2CNGwMKFtqWqiEgCCkZgb9wIX3/tugoREaf8H9hlZdC/P4wZ47oS\nERGn/B/YixfDl1/C8OGuKxERccr/gZ2VBc2bw8CBrisREXHK34G9Zw8sWQKjR2ujJxFJeP4O7AUL\n7IDdO+90XYmIiHP+Dux774VNm+B733NdiYiIc/4O7KQkaN/edRUiIr7g38B+4AF46CHXVYiI+IY/\nA/vQIdtG9cAB15WIiPiGPwN73jw4ckQbPYmInMSfgZ2VBW3aQGqq60pERHzDf4H96aewdq0tRddG\nTyIi3/BfYF94Idx3H4wa5boSERFf8d/ywdatYdo011WIiPiOv3rY+fl20K7nua5ERMR3/BXYTz4J\nt90G5eWuKxER8R3/BPbevbaV6ujRcMEFrqsREfGdiAJ7zJgxdOvWjSlTpkSvkpkztdGTiEg1wgb2\nm2++SUVFBWvXrmX37t1s3bq19qvwPJt73b07XHNN7X9/EZE4EDawc3NzGTZsGAA33ngjq1evrv0q\ndu6EwkIdAyYiUo2w0/qOHDlCixYtAEhOTmbbtm2nvJ+ZmUlmZiYABQUF51ZFq1awa9e5fa2ISIII\n28O+5JJLOHbsGACHDx+msrLylPfT09PJy8sjLy+Ppk2bnnsl9erZh4iIVClsYHfq1OmbYZCNGzfS\nunXraNckIiJVCDskMmTIEHr27Mnu3bt5++23ef/992NRl4iInCZsDzs5OZnc3FxSU1N59913adiw\nYSzqEhGR00S0l0jjxo2/mSkiIiJu+Gelo4iIVEuBLSISEApsEZGAUGCLiAREkufV3ubTTZo08e08\n7YKCgpot7Ikyv9cH/q9R9dWM6quZmtS3fft2CgsLw15Xq4HtZ6FQiLy8PNdlfCu/1wf+r1H11Yzq\nq5lY1KchERGRgFBgi4gERJ3HH3/8cddFxEqnTp1cl1Atv9cH/q9R9dWM6quZaNeXMGPYIiJBpyER\nEZGAiKvAPnDgAIMGDaJfv34MHTqU0tLSM64pLy/nyiuvJC0tjbS0NDZv3uygUn96+eWXv2mXjh07\ncu+9955xjdqvavv27aNnz55AZPchqC1Pd3IbRnIvQgK2oRdHMjIyvOXLl3ue53ljx471FixYcMY1\n+fn53sMPPxzr0jzP87yysjKvZcuWXu/evb3evXt7mzZtqvK6u+66y+vatas3efLkGFd4wvjx470N\nGzac8bqr9tu7d6/Xo0ePb/4eaRvFoi2Lioq8AQMGeNdff73neZHdh54X+7Y8uQ0jvRc9z00bnuzb\n7kXPi00b7t+/3xs4cKDXt29fb8iQIV5JSYmz+y+uetjjxo2jX79+gE1ib9as2RnXvP/++8yfP58e\nPXowcuRIysvLY1bfpk2bGD58OLm5ueTm5tK+ffszronJocdh7Nq1i7179xIKhc54z0X7FRcXM3r0\naI4cOQJE3kaxass6deqQnZ1NcnIyENl9CLFty9PbMJJ7Edy14XHV3YsQmzacM2cOEydOJCcnh5SU\nFObOnevs/ourwD5u3bp1FBcXk5qaesZ7nTt35r333mP16tU0atSIJUuWxKyuSG6umBx6HEZGRgb3\n3Xdfle+5aL/Tf5gjbaNYtWVycnKV+8RXdx9CbNvy9DaMNOhct2F19yLEpg1P/wU8e/ZsZ/df3AV2\nUVEREyZMICsrq8r3O3ToQPPmzQFo06ZNTHuwkdxcpx96vG/fvpjVB1BZWcmKFSvo06dPle+7aL/T\nf5gjbSOXbRnuPoTYtuXpbRhp0Llsw3D3IsS2DY//Am7ZsqWz+y+uAru0tJRhw4YxdepUWrVqVeU1\no0aNYuPGjVRUVDB//nyuu+66mNUXyc0V7tDjaFu1ahWpqakkJSVV+b7L9jsu0jZy1ZaR3Ifg/3sR\n3N6P4e5FiF0bnvwL2OX9F1eBPX36dPLz83nqqadIS0vjiSeeYNKkSadc89hjjzFq1Cg6duxI165d\n6du3b8zqi+Tmcn3o8bJly+jVqxcAn3zyia/a77hI28hVW55+H2ZnZ/uuLSMNOpf348n3Iri7H0//\nBez0/quVR5cSkc2bN3vt27f32rVr5z3yyCPeli1bvF//+tenXHPgwAGvQ4cO3oMPPui1adPG279/\nv6Nq/ad3796e51XdRmrLyBxvw9PvRc/z1IbfYtq0aV6jRo2+mVHz2muvObv/tNLRh4qLi8nJyaFX\nr16kpKS4LseXIm0jtWXNqQ3P5Or+U2CLiAREXI1hi4jEMwW2iEhAKLBFRAJCgS0iEhAKbBGRgPh/\nf0Vqe6vtunoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e073e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "lineX = np.linspace(1,20, 15)\n",
    "y=np.log2(lineX)\n",
    "\n",
    "plt.figure(1, facecolor='white')\n",
    "plt.plot(lineX, y, 'r--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们现在需要关心的是什么时候相等。如下图所示的凸函数:![images](../images/06/01.png)\n",
    "\n",
    "无论是凸函数还是凹函数，当且仅当自变量X是常数的时候，等式成立"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们令$\\varphi=\\frac{p(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})}$，则$log\\sum_{z^{{i}}}Q_i(z^{(i)})*\\frac{p(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})}=log\\sum_{z^{{i}}}Q_i(z^{(i)}) \\bullet \\varphi$，因为$Q_i(z^{(i)})$是关于z的某一个分布,那么$\\sum_{z^{{i}}}Q_i(z^{(i)}) \\bullet \\varphi$其实就是在$\\varphi$上关于$Q_i(z^{(i)})$分布求期望$E_Q(\\varphi)$，所以可以变化为$logE_Q(\\varphi)$，那么根据Jenson不等式，我们知道，log是凹函数，所以必有$logE_Q(\\varphi) \\geq E_Qlog(\\varphi)$\n",
    "\n",
    "$\\therefore log\\sum_{z^{{i}}}Q_i(z^{(i)}) \\bullet \\varphi \\geq \\sum_{z^{{i}}}Q_i(z^{(i)}) \\bullet log(\\varphi)$，由于任意i都有这个特性，那么则有\n",
    "\n",
    "$\\sum_i^mlog\\sum_{z^{{i}}}Q_i(z^{(i)})*\\frac{p(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})} \\geq \\sum_i^m\\sum_{z^{{i}}}Q_i(z^{(i)}) \\bullet log(\\frac{p(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})})$\n",
    "\n",
    "这样我们求$\\sum_i^m\\sum_{z^{{i}}}Q_i(z^{(i)}) \\bullet log(\\frac{p(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})})$的极大值就可以了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3.3 推导\n",
    "那么上面的式子什么时候可以取等号呢？由于log函数是一个严格意义上的凹函数，那么只要$x_1 \\neq x_2$，那么$log(x_1) \\neq log(x_2)$，所以只有$x_1=x_2$，才有可能是的log值相等\n",
    "\n",
    "对于$\\sum_i^m\\sum_{z^{{i}}}Q_i(z^{(i)}) \\bullet log(\\frac{p(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})})$而言，$Q$是一个随机分布，那么必须要求$log(\\frac{p(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})})$取定值，才能相等，去掉log，也就是相当于$\\frac{p(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})}$取定值才能保证等号成立\n",
    "\n",
    "设置$\\frac{p(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})}=C$，$\\Rightarrow p(x^{(i)}, z^{(i)}; \\theta)=Q_i(z^{(i)})C \\Rightarrow \\sum_{z=1}^np(x^{(i)}, z^{(i)}; \\theta)=\\sum_{z=1}^nQ_i(z^{(i)})C$\n",
    "\n",
    "$\\therefore \\sum_{z=1}^np(x^{(i)}, z^{(i)}; \\theta)=C*\\sum_{z=1}^nQ_i(z^{(i)})$\n",
    "\n",
    "$\\because \\sum_zQ_i(z^{(i)})=1$\n",
    "\n",
    "$\\therefore \\sum_{z=1}^np(x^{(i)}, z^{(i)}; \\theta)=C$\n",
    "\n",
    "$\\because \\frac{p(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})}=C$\n",
    "\n",
    "$\\therefore Q_i(z^{(i)})=\\frac{p(x^{(i)}, z^{(i)}; \\theta)}{C} \\Rightarrow Q_i(z^{(i)})=\\frac{p(x^{(i)}, z^{(i)}; \\theta)}{\\sum_{z=1}^np(x^{(i)}, z^{(i)}; \\theta)}$\n",
    "\n",
    "根据全概率公式，有：$\\sum_{z=1}^np(x^{(i)}, z^{(i)}; \\theta)=p(x^{(i)};\\theta)$\n",
    "\n",
    "$\\therefore Q_i(z^{(i)})=\\frac{p(x^{(i)}, z^{(i)}; \\theta)}{p(x^{(i)};\\theta)}$\n",
    "\n",
    "又根据条件概率公式，有$ Q_i(z^{(i)})=p(z^{(i)}|x^{(i)};\\theta)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3.4 EM算法\n",
    "就有了EM算法的伪代码如下:\n",
    "- 初始化分布参数$\\theta$\n",
    "- 重复以下步骤并收敛\n",
    "> - E步:$Q_i(z^{i}) := \\frac{p(x^{(i)}, z^{(i)}; \\theta)}{p(x^{(i)};\\theta)}$,固定$\\theta$优化Q\n",
    "> - M步：$\\theta := argmax_{\\theta}\\sum_i\\sum_{z^{(i)}}Q_i(z^{(i)})log\\frac{p(x^{(i)},z^{(i)}; \\theta)}{Q_i(z^{(i)})}$,固定Q优化$\\theta$\n",
    "\n",
    "M步具体的$p(x^{(i)})$，需要根据具体的模型来计算\n",
    "\n",
    "高斯混合模型可以看作M个高斯密度函数的线性组合\n",
    "\n",
    "E步要得到隐变量的分布，具体到身高的例子，就是要得到每一个样本是男性的概率是多少，是女性的概率是多少，注意归一化\n",
    "\n",
    "M步就是要得到结果，具体到身高的例子，就是要得到高斯分布的均值和标准差\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#4 证明EM算法是收敛的\n",
    "假定$\\theta^t$和$\\theta^{t+1}$是EM第t次和t+1次迭代后的结果，如果我们证明了$\\varphi(\\theta^t) \\leq \\varphi(\\theta^{t+1})$，也就是说极大似然估计单调增加，那么最终我们会达到最大似然估计的最大值。\n",
    "\n",
    "证明:选定$\\theta^t$后，我们得到E步\n",
    "![images](../images/06/02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#5 EM算法应用与混合高斯模型GMM\n",
    "高斯混合模型可以看作M个高斯密度函数的线性组合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##5.1 问题给出\n",
    "随机变量X是有K个高斯分布混合而成，取各个高斯分布的概率为$\\varphi_1,\\varphi_2,...,\\varphi_K$(先验概率)，第i个高斯分布的均值为$\\mu_i$，方差为$\\sum_i$。若观测到随机变量X的一系列样本$x_1,x_2,...,x_n$，试估计参数$\\varphi,\\mu,\\sum$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##5.2 E-Step\n",
    "这一步计算条件概率$\\omega_j^{(i)}=Q_i(z^{(i)}=j)=P(z^{(i)}=j|x^{(i)};\\phi,\\mu,\\sum)$，其中$\\mu,\\phi,\\sum$是给定的初始值，这个就是要计算在给定样本$x^{(i)}$的情况下，计算第i个隐变量$z^{(i)}$属于某一个组份的概率，比如身高的例子中，样本198cm中可能有90%的概率是男的，10%的概率是女的，那么$\\omega_男^{1.98}=0.9,\\omega_女^{1.98}=0.1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##5.3 M-Step\n",
    "将多项分布和高斯分布的参数带入:$\\sum_{i=1}^m\\sum_{z^{(i)}}Q_i(z^{(i)})log\\frac{p(x^{(i)},z^{(i)};\\phi,\\mu,\\sum)}{Q_i(z^{(i)})}$，其中$Q_i(z^{(i)})$就是E步算出来的$\\omega$.\n",
    "\n",
    "M步需要计算的是当$\\phi,\\mu,\\sum$取什么值的时候，上述的式子可以取到最大值。在上面的式子中，$z^{(i)}$是从1到K的，$p(x,z)=p(x|z) \\bullet p(z)$，$\\therefore p(x^{(i)},z^{(i)};\\phi,\\mu,\\sum)=p(x^{(i)}|z^{(i)};\\mu,\\sum) \\bullet p(z^{(i)};\\phi)$。其中$p(x^{(i)}|z^{(i)};\\mu,\\sum)$，当给定了属于某一个组份后，它是符合高斯分布的，所以它只跟$\\mu,\\sum$有关；$p(z^{(i)};\\phi)$是先验概率，只跟$\\phi$有关，所以$p(z^{(i)};\\phi)=\\phi_j$\n",
    "\n",
    "$\\sum_{i=1}^m\\sum_{z^{(i)}}Q_i(z^{(i)})log\\frac{p(x^{(i)},z^{(i)};\\phi,\\mu,\\sum)}{Q_i(z^{(i)})}=\\sum_{i=1}^m\\sum_{j=1}^kQ_i(z^{(i)}=j)log\\frac{p(x^{(i)}|z^{(i)}=j;\\mu,\\sum) \\bullet p(z^{(i)}=j;\\phi)}{Q_i(z^{(i)}=j)}$\n",
    "\n",
    "带入高斯密度函数，$\\Rightarrow \\sum_{i=1}^m\\sum_{j=1}^k\\omega_j^{(i)}log\\frac{\\frac{1}{(2\\pi)^{\\frac{n}{2}}|\\sum_j|^{\\frac{1}{2}}}   e^{[-\\frac{1}{2}(x^{(i)}-\\mu_j)^T\\sum_j^{-1}(x^{(i)}-\\mu_j)]}   \\bullet \\phi_j}{\\omega_j^{(i)}}$，令这个式子为$\\xi(\\mu,\\phi,\\sum)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###5.3.1 对$\\mu_i$求偏导\n",
    "$\\frac{\\partial{\\xi(\\mu,\\phi,\\sum)}}{\\partial{\\mu_l}}=-\\frac{\\partial{\\xi(\\mu,\\phi,\\sum)}}{\\partial{\\mu_l}}\\sum_{i=1}^m\\sum_{j=1}^k\\omega_j^{(i)} \\bullet \\frac{1}{2} \\bullet (x^{(i)}-\\mu_j)^T \\bullet \\sum_j^{-1} \\bullet (x^{(i)}-\\mu_j)$\n",
    "\n",
    "如果$A$是对称阵，即$A=A^T$，那么$\\frac{\\partial{X^TAX}}{\\partial{X}}=2AX$，而$\\sum_j$是一个协方差矩阵，协方差矩阵是对称阵，那么$\\sum_j^{-1}$这个是协方差矩阵的逆矩阵，也是对称阵。所以有\n",
    "\n",
    "$\\Rightarrow \\frac{1}{2}\\sum_{i=1}^m\\omega_l^{(i)} \\bullet \\frac{\\partial{\\xi(\\mu,\\phi,\\sum)}}{\\partial{\\mu_l}}2\\mu_l^T\\sum_l^{-1}[x^{(i)}-\\mu_l]=\\sum_l^{-1}\\sum_{i=1}^m\\omega_l^{(i)}(x^{(i)}-\\mu_l)=0$\n",
    "\n",
    "$\\Rightarrow \\mu_l := \\frac{\\sum_{i=1}^m\\omega_l^{(i)}x^{(i)}}{\\sum_{i=1}^m\\omega_l^{(i)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###5.3.2 对$\\sum_j$求偏导\n",
    "$\\sum_j=\\frac{\\sum_{i=1}^m\\omega_j^{(i)}(x^{(i)}-\\mu_j)(x^{(i)}-\\mu_j)^T}{\\sum_{i=1}^m\\omega_j^{(i)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###5.3.3 对$\\phi_j$求偏导\n",
    "删除常数项后原式子得到$\\sum_{i=1}^m\\sum_{j=1}^k\\omega_j^{(i)}log\\phi_j$，但是直接对$\\phi_j$求偏导令其为0是不对的，因为$\\phi_j$的自由度没有那么大，有一个条件在约束着它$\\sum_{j=1}^k\\phi_j=1$，这个时候必须使用拉格朗日乘子法来计算\n",
    "\n",
    "$L(\\phi)=\\sum_{i=1}^m\\sum_{j=1}^k\\omega_j^{(i)}log\\phi_j + \\beta(\\sum_{j=1}^k\\phi_j-1)$，此时不需要加上约束$\\phi_j \\geq 0$，因为$log\\phi_j$已经保证了最后的$\\phi > 0$\n",
    "\n",
    "$\\frac{\\partial{L}}{\\partial{\\phi}}=\\sum_{i=1}^m\\omega_j^{(i)}+\\beta\\phi_j=0, j=1,2,...,k$\n",
    "\n",
    "$\\Rightarrow \\sum_{j=1}^k\\sum_{i=1}^m\\omega_j^{(i)}+\\beta\\sum_{j=1}^k\\phi_j=0$\n",
    "\n",
    "$\\Rightarrow \\sum_{j=1}^k\\sum_{i=1}^m\\omega_j^{(i)}+\\beta=0 \\Rightarrow \\sum_{i=1}^m\\sum_{j=1}^k\\omega_j^{(i)}+\\beta=0$\n",
    "\n",
    "$\\because \\sum_{j=1}^k\\omega_j^{(i)}=1 \\therefore \\beta=-m$，带入$\\sum_{i=1}^m\\omega_j^{(i)}+\\beta\\phi_j=0$，有\n",
    "\n",
    "$\\sum_{i=1}^m\\omega_j^{(i)}-m\\phi_j=0 \\therefore \\phi_j=\\frac{\\sum_{i=1}^m\\omega_j^{(i)}}{m}$"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}