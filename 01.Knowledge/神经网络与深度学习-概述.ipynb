{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1 概述与架构图\n",
    "![images](images/DeepLearning/01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1.1 传统的机器学习与深度学习的区别\n",
    "传统的机器学习流程是：\n",
    "- 数据预处理:归一化、降维和去噪\n",
    "- 特征提取:人为的找到一种模式，对数据进行一种提炼。图像：SIFT、LBP、Fisher、Gabor、Hog。语言：MFCC、小波、Word2Vec\n",
    "- 选择分类器:包括SVM、决策树、随机森林、贝叶斯网络、线性回归或者聚类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深度学习的流程是：\n",
    "- 数据准备\n",
    "- 设计模型：CNN、RNN或CNN+RNN\n",
    "- 训练：调结构、损失函数、训练参数\n",
    "\n",
    "CNN：卷积神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1.2 深度学习的应用\n",
    "指挥交通、预测股票、诊断疾病、写小说、编笑话、打游戏、心理咨询、买菜砍价"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![images](images/deeplearning/02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2 基本概念与流程\n",
    "![images](images/deeplearning/03.png)\n",
    "其中x是输入，w是权重,我们的目的是要计算出来w，来最好的拟合x，使得损失函数最小。\n",
    "\n",
    "计算损失函数的方向是前向传播.根据损失函数来更新w的过程，叫做反向传播。一旦损失函数不会在下降了，那么w就调整好了\n",
    "\n",
    "一般分为三个过程：\n",
    "- 前向传播：根据x，w计算出损失函数Loss\n",
    "- 反向传播：根据Loss，计算出w，看看每个w对于最终的Loss产生多大的贡献\n",
    "- 调整w，使得Loss最小\n",
    "\n",
    "这就是神经网络基本的流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![images](images/deeplearning/04.png)\n",
    "神经网络的结构，其中圆圈代表神经元，粉色或蓝色的叫做层次。所以神经网络是由层堆叠而成，每个层都有若干神经元，层与层之间通过全连接进行连接。连接代表的是权重参数，中间的层叫做隐层。可以看出这里就相当于多个线性分类组合在一起。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3 传统神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3.1 线性回归\n",
    "线性关系来描述输入到输出的映射关系，应用场景包括网络分析，银行风险分析，基金股价分析，天气预报\n",
    "\n",
    "但是为何要引入非线性以及为何要引入非线性的激励函数，因为不使用激励函数的话，神经网络的每层都只是做线性变换，多层输入叠加后也还是线性变换。因为线性模型的表达能力不够，激励函数可以引入非线性因素"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3.2 从线性到非线性\n",
    "添加一个非线性的激励函数,在一个线性函数的外面，套上一个非线性的激励函数，从而实现一个非线性的拟合\n",
    "\n",
    "选择激励函数的两个考量：\n",
    "- 正向对输入的调整\n",
    "- 反向梯度损失"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###3.2.1 常用非线性激励函数-Sigmoid\n",
    "$y(x)=\\frac{1}{1+e^{-x}}$,$y'(x)=y(x)(1-y(x))$，这个函数的优点是输出数据可以映射到[0,1]之间,缺点是梯度下降明显，至少减少75%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###3.2.2 常用非线性激励函数-tahn\n",
    "$f(x)=tanh(x)=\\frac{2}{1+e^{-2x}}-1$,$f'(x)=1-f(x)^2$。这个函数的有点是输出数据可以映射到[-1,1]，缺点也是梯度下降明显。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###3.2.3 常用非线性激励函数-ReLU(Rectified linear unit)\n",
    "$f(x)=\\begin{cases}\n",
    "0 & x \\le 0\\\\\n",
    "x & x \\geq 0\n",
    "\\end{cases}$，$f'(x)=\\begin{cases}\n",
    "0 & x \\le 0\\\\\n",
    "1 & x \\geq 0\n",
    "\\end{cases}$,这个函数正向截断负值，损失大量特征，反向梯度没有损失。由于特征特别多，所以损失一些特征没有关系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###3.2.4 常用非线性激励函数-Leaky ReLU\n",
    "$f(x)=\\begin{cases}\n",
    "0.01x & x \\le 0\\\\\n",
    "x & x \\geq 0\n",
    "\\end{cases}$,$f'(x)=\\begin{cases}\n",
    "0.01 & x \\le 0\\\\\n",
    "1 & x \\geq 0\n",
    "\\end{cases}$，优点是保留更多参数，少量梯度反向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你甚至可以创造自己的激励函数来处理自己的问题, 不过要确保的是这些激励函数必须是可以微分的, 因为在误差反向传递的时候, 只有这些可微分的激励函数才能把误差传递回去."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}