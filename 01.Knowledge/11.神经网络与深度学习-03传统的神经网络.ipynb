{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 传统神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1 线性回归\n",
    "线性关系来描述输入到输出的映射关系，应用场景包括网络分析，银行风险分析，基金股价分析，天气预报，传统的感知机只能进行线性可分，对于线性不可分的情况就会无法收敛，比如异或问题，所以要引入非线性的因素\n",
    "\n",
    "但是为何要引入非线性以及为何要引入非线性的激励函数，因为不使用激励函数的话，神经网络的每层都只是做线性变换，多层输入叠加后也还是线性变换。因为线性模型的表达能力不够，激励函数可以引入非线性因素\n",
    "\n",
    "目标方程:$y=ax_1+bx_2+cx_3+d$\n",
    "\n",
    "参数:$m=[a,b,c,d]$\n",
    "\n",
    "数据:$[(x_{11},x_{21},x_{31}),(x_{12},x_{22},x_{32}),...,(x_{1n},x_{2n},x_{3n})] [y_1,y_2,...,y_n]$\n",
    "\n",
    "预测:$\\hat{y_t}=ax_{1t}+bx_{2t}+cx_{3t}+d$\n",
    "\n",
    "目标:$minimize(\\hat{y_t}-y_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2 从线性到非线性\n",
    "添加一个非线性的激励函数,在一个线性函数的外面，套上一个非线性的激励函数，从而实现一个非线性的拟合。神经元和感知器本质上是一样的，只不过我们说感知器的时候，它的激活函数是阶跃函数；而当我们说神经元时，激活函数往往选择为sigmoid函数或tanh函数\n",
    "\n",
    "比如对于感知机中无法分割的异或问题，其实两条线就可以分开，那么我们其实可以这么考虑，给定两组权重，可以分别得到两条直线，然后两条之间做and操作，就可以分开。这就形成了一个简单的神经网络\n",
    "\n",
    "选择激励函数的两个考量：\n",
    "- 正向对输入的调整\n",
    "- 反向梯度损失(反向传播算法，是神经网络的训练算法)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2.1 常用非线性激励函数-Sigmoid\n",
    "- 正向看，函数$y(x)=\\frac{1}{1+e^{-x}}$，优点是输出数据可以映射到[0,1]之间,很好 \n",
    "- 反向看，$y'(x)=y(x)(1-y(x))$。缺点是梯度下降明显，至少减少75%。这是因为$y'$最大值是0.25，如果梯度原来是1，那么1 * 0.25 = 0.25，那么就是梯度损失了1-25%=75%\n",
    "\n",
    "所以在深度学习中，最后一层可以使用Sigmoid函数。中间层不可使用，因为梯度损失太厉害"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2.2 常用非线性激励函数-tanh\n",
    "- 正向看，函数$f(x)=tanh(x)=\\frac{2}{1+e^{-2x}}-1$，优点是输出数据可以映射到[-1,1]\n",
    "- 反向看，$f'(x)=1-f(x)^2$。最大值为1,也就是说只有f(x)=0的时候，反向梯度才是1，但是正向为0，也就没有什么激励过程了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2.3 常用非线性激励函数-ReLU(Rectified linear unit)\n",
    "$f(x)=\\begin{cases}\n",
    "0 & x \\le 0\\\\\n",
    "x & x \\geq 0\n",
    "\\end{cases}$，$f'(x)=\\begin{cases}\n",
    "0 & x \\le 0\\\\\n",
    "1 & x \\geq 0\n",
    "\\end{cases}$,这个函数正向截断负值，损失大量特征，反向梯度没有损失。由于特征特别多，所以损失一些特征没有关系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2.4 常用非线性激励函数-Leaky ReLU\n",
    "$f(x)=\\begin{cases}\n",
    "0.01x & x \\le 0\\\\\n",
    "x & x \\geq 0\n",
    "\\end{cases}$,$f'(x)=\\begin{cases}\n",
    "0.01 & x \\le 0\\\\\n",
    "1 & x \\geq 0\n",
    "\\end{cases}$，优点是保留更多参数，少量梯度反向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你甚至可以创造自己的激励函数来处理自己的问题, 不过要确保的是这些激励函数必须是可以微分的, 因为在误差反向传递的时候, 只有这些可微分的激励函数才能把误差传递回去.\n",
    "\n",
    "传统神经网络，一般使用Sigmoid或者tanh来做激活函数，而卷积神经网络一般使用ReLU和Leaky ReLU来说激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3 神经网络构建\n",
    "从第一层神经网络到最终输出，每一个神经元的数值由前一层神经元数值，神经元参数W，b以及激励函数共同决定第n+1层第k个神经元的方程可由公式表示为:$Z_{n+1,k}=\\sum_{i=1}^mW_{n,k,i} \\bullet x_{n,i} + b_{n,k}, y_{n+1,k}=\\frac{1}{1+e^{-z_{n+1,k}}}$,在这里，m表示第n层神经网络 的宽度，n为当前神经网络的深度.\n",
    "![images](images/DeepLearning/06.png)\n",
    "这也是神经网络正向的计算\n",
    "\n",
    "![images](images/DeepLearning/07.png)对于反向计算，则是求梯度.第n-1层的梯度，是用第n层的梯度除以参数得到的.所以真正计算的时候，都是从Loss出发，往前一步一步计算梯度。这就是链式法则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#4 神经网络配件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4.1 损失函数-Loss\n",
    "损失函数是影响深度学习性能最重要的因素之一。是外部世界对神经网络模型训练的直接指导。合适的损失函数能够保证深度学习模型收敛"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###4.1.1 Softmax-用于分类问题\n",
    "$\\sigma(Z)_j=\\frac{e^{Z_j}}{\\sum_{k=1}^Ke^{Z_k}}, j=1,2,...,K$\n",
    "\n",
    "比如特征的目标值是[1,2,3,4,1,2,3],那么不同值之间的距离比较相近,但是经过损失函数之后变成了[0.024,0.064,0.175,0.475,0.024,0.064,0.175]，这样差别就会很大，这样分类问题的预测结果更明显"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###4.1.2 Cross entropy-用于回归问题\n",
    "$L(w)=\\frac{1}{N}\\sum_{n=1}^NH(p_n,q_n)=-\\frac{1}{N}\\sum_{n=1}^N[y_nlog\\hat{y_n}+(1-y_n)log(1-\\hat{y_n})]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###4.1.3 自定义损失函数\n",
    "- 看中某一个属性\n",
    "单独讲某一些预测值取出活赋予不同大小的参数\n",
    "- 合并多个loss\n",
    "多目标训练任务，设置合理的loss结合方式\n",
    "- 神经网络融合\n",
    "不同神经网络loss结合，共同对网络进行训练指导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4.2 学习率 Learning rate\n",
    "- 数值大：收敛速度快\n",
    "- 数值小：精度高\n",
    "\n",
    "选用合适的学习率的办法\n",
    "- 固定一个\n",
    "- 设置一个step不停迭代\n",
    "- Adagrad\n",
    "- RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4.3 动量\n",
    "正常$x += -learning_rate * dx$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4.4 过拟合\n",
    "模型的大部分参数能够参与运算，那么过拟合的程度就低\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###4.4.1 应对过拟合-正则化\n",
    "没有加正则化，就是\n",
    "$$Loss=\\hat{y}-y\\\\\n",
    "\\Delta{w}=\\frac{d(Loss)}{d(w)}\\\\\n",
    "w := w - \\eta\\Delta{w}$$\n",
    "假如正则化以后，\n",
    "$$Loss'=\\hat{y}-y+\\lambda \\bullet ||w^2||\\\\\n",
    "\\Delta{w}=\\frac{d(Loss)}{d(w)} + 2\\lambda \\bullet w\\\\\n",
    "w := w-\\eta\\Delta{w}-2\\eta\\lambda{w}$$\n",
    "其中$2\\eta\\lambda{w}$叫做weight decay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###4.4.2 应对过拟合-Dropout\n",
    "每次随机选择一些神经元进行计算，剩下的不进行计算，这样就可以应对过拟合，因为只有大部分神经元都的参数都接近，才能每次选取不同的神经元才会有好的结果。一般最后两个layers用一下Dropout\n",
    "\n",
    "Pooling-对于原始数据进行区域求最大值或者均值的过程，本质就是降维"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#5 传统神经网络的输出\n",
    "神经网络实际上就是一个输入向量$\\overrightarrow{x}$到输出向量$\\overrightarrow{y}$的函数，即：$\\overrightarrow{y}=f_{network}(\\overrightarrow{x})$,根据输入计算神经网络的输出，需要首先将输入向量$\\overrightarrow{x}$的每个元素$x_i$的值赋给神经网络的输入层的对应神经元，然后根据式1依次向前计算每一层的每个神经元的值，直到最后一层输出层的所有神经元的值计算完毕。最后，将输出层每个神经元的值串在一起就得到了输出向量$\\overrightarrow{y}$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##5.1 举例说明\n",
    "![images](images/DeepLearning/20.png)\n",
    "如上图，输入层有三个节点，我们将其依次编号为1、2、3；隐藏层的4个节点，编号依次为4、5、6、7；最后输出层的两个节点编号为8、9。因为我们这个神经网络是全连接网络，所以可以看到每个节点都和上一层的所有节点有连接。比如，我们可以看到隐藏层的节点4，它和输入层的三个节点1、2、3之间都有连接，其连接上的权重分别为$\\omega_{41},\\omega_{42},\\omega_{43}$。那么，我们怎样计算节点4的输出值$\\alpha_4$呢？\n",
    "为了计算节点4的输出值，我们必须先得到其所有上游节点（也就是节点1、2、3）的输出值。节点1、2、3是输入层的节点，所以，他们的输出值就是输入向量$\\overrightarrow{x}$本身。按照上图画出的对应关系，可以看到节点1、2、3的输出值分别是$x_1,x_2,x_3$。我们要求输入向量的维度和输入层神经元个数相同，而输入向量的某个元素对应到哪个输入节点是可以自由决定的，你偏非要把$x_1$赋值给节点2也是完全没有问题的，但这样除了把自己弄晕之外，并没有什么价值\n",
    "\n",
    "这样我们就可以计算出$\\alpha_4 = sigmoid(\\overrightarrow{\\omega}^T \\bullet \\overrightarrow{x})=sigmoid(\\omega_{41}x_1+\\omega_{42}x_2+\\omega_{43}x_3+\\omega_{4b})$\n",
    "\n",
    "同样，我们可以继续计算出节点5、6、7的输出值$\\alpha_5,\\alpha_6,\\alpha_7$。这样，隐藏层的4个节点的输出值就计算完成了，我们就可以接着计算输出层的节点8的输出值$y_1=sigmoid(\\overrightarrow{\\omega}^T \\bullet \\overrightarrow{x})=sigmoid(\\omega_{84}\\alpha_4+\\omega_{85}\\alpha_5+\\omega_{86}\\alpha_6+\\omega_{87}\\alpha_7+\\omega_{8b})$\n",
    "\n",
    "同理，我们还可以计算出$y_2$的值。这样输出层所有节点的输出值计算完毕，我们就得到了在输入向量$\\overrightarrow{x}=\\begin{bmatrix}\n",
    "x_1\\\\\n",
    "x_2\\\\\n",
    "x_3\n",
    "\\end{bmatrix}$时，神经网络的输出向量$\\overrightarrow{y}=\\begin{bmatrix}\n",
    "y_1\\\\\n",
    "y_2\n",
    "\\end{bmatrix}$。这里我们也看到，输出向量的维度和输出层神经元个数相同。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#6 传统神经网络的训练-反向传播算法\n",
    "神经网络是一个模型，权值就是模型的参数，也就是模型需要学习的东西。而神经网络的连结方式、网络的层次、每层的节点数这些参数是人为事先设置的，这些参数叫做超参数。\n",
    "\n",
    "假设每个训练样本为$(\\overrightarrow{x}, \\overrightarrow{t})$，其中向量$\\overrightarrow{x}$是训练样本的特征，而$\\overrightarrow{t}$是样本的目标值![images](images/DeepLearning/21.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##6.1 计算$y_i$\n",
    "用样本的特征$\\overrightarrow{x}$，计算出神经网络中每个隐藏层节点的输出$\\alpha_i$，以及输出层每个节点的输出$y_i$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##6.2 计算每个节点的误差项$\\delta_i$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##6.2.1 输出层节点\n",
    "对于输出层节点i，有$\\delta_i=y_i(1-y_i)(t_i-y_i)$，其中，$\\delta_i$是节点i的误差项，$y_i$是节点的输出值，$t_i$是样本对应于节点i的目标值。举个例子，根据上图，对于输出层节点8来说，它的输出值是$y_1$，而样本的目标值是$t_1$，带入上面的公式得到节点8的误差项应该是$\\delta_8=y_1(1-ky_1)(t_1-y_1)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###6.2.2 隐藏层节点\n",
    "$\\delta_i=\\alpha_i(1-\\alpha_i)\\sum_{k \\in outputs}\\omega_{ki}\\delta_k$,其中，$\\alpha_i$是节点i的输出值，$\\omega_{ki}$是节点i到它的下一层节点k的连接的权重，$\\delta_k$是节点i的下一层节点k的误差项。例如，对于隐藏层节点4来说，计算方法如下：$\\delta_4=\\alpha_4(1-\\alpha_4)(\\omega_{84}\\delta_8+\\omega_{94}\\delta_9)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###6.2.3 更新每个连接上的权值\n",
    "$\\omega_{ji} \\leftarrow \\omega_{ij} + \\eta\\delta_jx_{ji}$,其中，$\\omega_{ji}$是节点i到节点j的权重，$\\eta$是一个成为学习速率的常数，$\\delta_j$是节点j的误差项，$x_{ji}$是节点i传递给节点j的输入。例如，权重$\\omega_{84}$的更新方法如下:$\\omega_{84} \\leftarrow \\omega_{84} + \\eta\\delta_8\\alpha_4$,类似的，权重$\\omega_{41}$的更新方法如下$\\omega_{41} \\leftarrow \\omega_{41} + \\eta\\delta_4x_1$,偏置项的输入值永远为1，例如节点4的偏置项$\\omega_{4b}$应该按照下面的方法计算$\\omega_{4b} \\leftarrow \\omega_{4b} + \\eta\\delta_4$\n",
    "\n",
    "显然，计算一个节点的误差项，需要先计算每个与其相连的下一层节点的误差项。这就要求误差项的计算顺序必须是从输出层开始，然后反向依次计算每个隐藏层的误差项，直到与输入层相连的那个隐藏层。这就是反向传播算法的名字的含义。当所有节点的误差项计算完毕后就可以更新所有的权重"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}