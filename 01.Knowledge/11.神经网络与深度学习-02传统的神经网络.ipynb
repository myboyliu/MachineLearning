{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 传统神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1 线性回归\n",
    "线性关系来描述输入到输出的映射关系，应用场景包括网络分析，银行风险分析，基金股价分析，天气预报\n",
    "\n",
    "但是为何要引入非线性以及为何要引入非线性的激励函数，因为不使用激励函数的话，神经网络的每层都只是做线性变换，多层输入叠加后也还是线性变换。因为线性模型的表达能力不够，激励函数可以引入非线性因素\n",
    "\n",
    "目标方程:$y=ax_1+bx_2+cx_3+d$\n",
    "\n",
    "参数:$m=[a,b,c,d]$\n",
    "\n",
    "数据:$[(x_{11},x_{21},x_{31}),(x_{12},x_{22},x_{32}),...,(x_{1n},x_{2n},x_{3n})] [y_1,y_2,...,y_n]$\n",
    "\n",
    "预测:$\\hat{y_t}=ax_{1t}+bx_{2t}+cx_{3t}+d$\n",
    "\n",
    "目标:$minimize(\\hat{y_t}-y_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2 从线性到非线性\n",
    "添加一个非线性的激励函数,在一个线性函数的外面，套上一个非线性的激励函数，从而实现一个非线性的拟合\n",
    "\n",
    "选择激励函数的两个考量：\n",
    "- 正向对输入的调整\n",
    "- 反向梯度损失"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2.1 常用非线性激励函数-Sigmoid\n",
    "- 正向看，函数$y(x)=\\frac{1}{1+e^{-x}}$，优点是输出数据可以映射到[0,1]之间,很好 \n",
    "- 反向看，$y'(x)=y(x)(1-y(x))$。缺点是梯度下降明显，至少减少75%。这是因为$y'$最大值是0.25，如果梯度原来是1，那么1 * 0.25 = 0.25，那么就是梯度损失了1-25%=75%\n",
    "\n",
    "所以在深度学习中，最后一层可以使用Sigmoid函数。中间层不可使用，因为梯度损失太厉害"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2.2 常用非线性激励函数-tanh\n",
    "- 正向看，函数$f(x)=tanh(x)=\\frac{2}{1+e^{-2x}}-1$，优点是输出数据可以映射到[-1,1]\n",
    "- 反向看，$f'(x)=1-f(x)^2$。最大值为1,也就是说只有f(x)=0的时候，反向梯度才是1，但是正向为0，也就没有什么激励过程了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2.3 常用非线性激励函数-ReLU(Rectified linear unit)\n",
    "$f(x)=\\begin{cases}\n",
    "0 & x \\le 0\\\\\n",
    "x & x \\geq 0\n",
    "\\end{cases}$，$f'(x)=\\begin{cases}\n",
    "0 & x \\le 0\\\\\n",
    "1 & x \\geq 0\n",
    "\\end{cases}$,这个函数正向截断负值，损失大量特征，反向梯度没有损失。由于特征特别多，所以损失一些特征没有关系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2.4 常用非线性激励函数-Leaky ReLU\n",
    "$f(x)=\\begin{cases}\n",
    "0.01x & x \\le 0\\\\\n",
    "x & x \\geq 0\n",
    "\\end{cases}$,$f'(x)=\\begin{cases}\n",
    "0.01 & x \\le 0\\\\\n",
    "1 & x \\geq 0\n",
    "\\end{cases}$，优点是保留更多参数，少量梯度反向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你甚至可以创造自己的激励函数来处理自己的问题, 不过要确保的是这些激励函数必须是可以微分的, 因为在误差反向传递的时候, 只有这些可微分的激励函数才能把误差传递回去."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3 神经网络构建\n",
    "从第一层神经网络到最终输出，每一个神经元的数值由前一层神经元数值，神经元参数W，b以及激励函数共同决定第n+1层第k个神经元的方程可由公式表示为:$Z_{n+1,k}=\\sum_{i=1}^mW_{n,k,i} \\bullet x_{n,i} + b_{n,k}, y_{n+1,k}=\\frac{1}{1+e^{-z_{n+1,k}}}$,在这里，m表示第n层神经网络 的宽度，n为当前神经网络的深度.\n",
    "![images](images/DeepLearning/06.png)\n",
    "这也是神经网络正向的计算\n",
    "\n",
    "![images](images/DeepLearning/07.png)对于反向计算，则是求梯度.第n-1层的梯度，是用第n层的梯度除以参数得到的.所以真正计算的时候，都是从Loss出发，往前一步一步计算梯度。这就是链式法则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#4 神经网络配件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4.1 损失函数-Loss\n",
    "损失函数是影响深度学习性能最重要的因素之一。是外部世界对神经网络模型训练的直接指导。合适的损失函数能够保证深度学习模型收敛"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###4.1.1 Softmax-用于分类问题\n",
    "$\\sigma(Z)_j=\\frac{e^{Z_j}}{\\sum_{k=1}^Ke^{Z_k}}, j=1,2,...,K$\n",
    "\n",
    "比如特征的目标值是[1,2,3,4,1,2,3],那么不同值之间的距离比较相近,但是经过损失函数之后变成了[0.024,0.064,0.175,0.475,0.024,0.064,0.175]，这样差别就会很大，这样分类问题的预测结果更明显"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###4.1.2 Cross entropy-用于回归问题\n",
    "$L(w)=\\frac{1}{N}\\sum_{n=1}^NH(p_n,q_n)=-\\frac{1}{N}\\sum_{n=1}^N[y_nlog\\hat{y_n}+(1-y_n)log(1-\\hat{y_n})]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###4.1.3 自定义损失函数\n",
    "- 看中某一个属性\n",
    "单独讲某一些预测值取出活赋予不同大小的参数\n",
    "- 合并多个loss\n",
    "多目标训练任务，设置合理的loss结合方式\n",
    "- 神经网络融合\n",
    "不同神经网络loss结合，共同对网络进行训练指导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4.2 学习率 Learning rate\n",
    "- 数值大：收敛速度快\n",
    "- 数值小：精度高\n",
    "\n",
    "选用合适的学习率的办法\n",
    "- 固定一个\n",
    "- 设置一个step不停迭代\n",
    "- Adagrad\n",
    "- RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4.3 动量\n",
    "正常$x += -learning_rate * dx$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4.4 过拟合\n",
    "模型的大部分参数能够参与运算，那么过拟合的程度就低\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###4.4.1 应对过拟合-正则化\n",
    "没有加正则化，就是\n",
    "$$Loss=\\hat{y}-y\\\\\n",
    "\\Delta{w}=\\frac{d(Loss)}{d(w)}\\\\\n",
    "w := w - \\eta\\Delta{w}$$\n",
    "假如正则化以后，\n",
    "$$Loss'=\\hat{y}-y+\\lambda \\bullet ||w^2||\\\\\n",
    "\\Delta{w}=\\frac{d(Loss)}{d(w)} + 2\\lambda \\bullet w\\\\\n",
    "w := w-\\eta\\Delta{w}-2\\eta\\lambda{w}$$\n",
    "其中$2\\eta\\lambda{w}$叫做weight decay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###4.4.2 应对过拟合-Dropout\n",
    "每次随机选择一些神经元进行计算，剩下的不进行计算，这样就可以应对过拟合，因为只有大部分神经元都的参数都接近，才能每次选取不同的神经元才会有好的结果。一般最后两个layers用一下Dropout\n",
    "\n",
    "Pooling-对于原始数据进行区域求最大值或者均值的过程，本质就是降维"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}