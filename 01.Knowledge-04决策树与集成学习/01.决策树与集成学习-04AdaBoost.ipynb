{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1 算法描述\n",
    "- 设训练数据集$T=\\{(\\overrightarrow{x_1}, y_1),(\\overrightarrow{x_2},y_2),...,(\\overrightarrow{x_N},y_N)\\}$，如果相对它们进行分类，能不能加入权值。我们的指导思想是对预测错误的样本假如大的权值，一遍下一次迭代的时候更多的考虑错误的样本。\n",
    "- 初始化训练数据的权值分布$D_1=\\{\\omega_{11},\\omega_{12},...,\\omega_{1i},...,\\omega_{1N}\\}$，其中$\\omega_{1i}=\\frac{1}{N},i=1,2,3,...,N$，也就是说初始化的时候每个样本的权值是一样的\n",
    "- 接下来对于m从1到M，开始使用具有权值分布$D_m$的训练数据集学习，得到基本分类器$G_m(x):\\chi \\in \\{-1,+1\\}$\n",
    "- 计算$G_m(x)$在训练数据集上的分类误差率$e_m=P(G_m(x_i)\\neq y_i)=\\sum_{i=1}^N\\omega_{mi}I(G_m(x_i)\\neq y_i),I(G_m(x_i)\\neq y_i)=\\begin{cases}\n",
    "1 & G_m(x_i)\\neq y_i = true\\\\\n",
    "0 & G_m(x_i)\\neq y_i = false\n",
    "\\end{cases}$\n",
    "- 计算$G_m(x)$的系数$\\alpha_m=\\frac{1}{2}log\\frac{1-e_m}{e_m}$.这里的$\\alpha_m$就是当前这个分类器的权值\n",
    "- 更新训练数据集的权值分布$D_{m+1}=\\{\\omega_{m+1,1},\\omega_{m+1,2},...,\\omega_{m+1,i},...,\\omega_{m+1,N}\\}$, $\\omega_{m+1,i}=\\frac{\\omega_{mi}}{Z_m}e^{-\\alpha_my_iG_m(x_i)},i=1,2,...,N$，这里除以$Z_m$是为了归一化\n",
    "- 这里的$Z_m$是规范化因子，$Z_m=\\sum_{i=1}^N\\omega_{mi}e^{-\\alpha_my_iG_m(x_i)}$\n",
    "- 得到若干个$\\alpha_m$和$G_m(x)$，构建基本分类器的线性组合$f_(x)=\\sum_{m=1}^M\\alpha_mG_m(x)$\n",
    "- 得到最终的分类器$G(x)=sign(f(x))=sign[\\sum_{m=1}^M\\alpha_mG_m(x)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2. 算法解释\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2.1 $e_m$和$\\alpha_m$\n",
    "首先需要明白的是如果没有任何基本分类器，单凭肉眼去分类，得到的分类误差率应该是0.5，那么有了基本分类器，$e_m$应该是小余0.5的，否则假如分类器没有任何意义，这也就是说$1-e_m > 0.5$，所以$\\alpha_m > 0$的数，每次$e_m$越小，那么$\\alpha_m$就越大。如果$e_m > 0.5$，那么得到的$\\alpha_m$就是一个负值，那么相当于对于此时的分类器$G_m$，我们需要反向考虑。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2.2 更新$\\omega$\n",
    "- 如果第i个样本我们分类错误了，也就是说$G_m(x_i) \\neq y_i$，其中$G_m(x_i)$和$y_i$都是-1或者+1，如果分类错误了，那么$y_iG_m(x_i)$就是-1。\n",
    "- 一般情况下,$\\alpha_m > 0$\n",
    "- 所以$-\\alpha_my_iG_m(x_i)$是一个大于0的数，那么$e^{-\\alpha_my_iG_m(x_i)} > 1$。\n",
    "- 这就是说如果第i个样本分错了，下一个$\\omega_{m+1,i}$就是上一个$\\omega_{mi}$乘以一个大于1的数，相当于权值升高了。\n",
    "- 反之如果分类分对了，权值就会下降\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3 举例说明\n",
    "给定下列训练样本，试用AdaBoost算法学习一个强分类器。![images](images/10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假定我们给定一个线性分类器，是无法把它分开的\n",
    "1. 首先初始化训练数据的权值分布$D_1=\\{\\omega_{11},\\omega_{12},...,\\omega_{1i},...,\\omega_{1N}\\},\\omega_{1i}=\\frac{1}{N},i=1,2,...,N$，由于有10个样本，所以$\\omega_{1i}=0.1$\n",
    "2. 对于m=1，我们需要根据样本训练出来一个基本分类器。由于样本很少，我们可以遍历以下，比如我们选择阀值V取2.5是误差率最低，那么基本分类器为$G_1(x)=\\begin{cases}\n",
    "1, & x < 2.5\\\\\n",
    "-1, & x > 2.5\n",
    "\\end{cases}$，那么我们可以看到只有样本6，7，8做错了\n",
    "3. $G_1(x)$训练集上的误差率$e_1=P(G_1(x_i)\\neq y_i)=P(x=6,7,8)=0.1+0.1+0.1=0.3$。\n",
    "4. 计算$G_1$的系数$\\alpha_1=\\frac{1}{2}log\\frac{1-e_1}{e_1}=0.4236$，那么$f_1(x)=0.4236G_1(x)$，分类器$sign[f_1(x)]$在训练数据集上有3个错误分类点\n",
    "5. 更新$D_{m+1}$和$\\omega_{m+1}$，其中$D_2=(0.0715,0.0715,0.0715,0.0715,0.0715,0.0715,,0.1666,0.1666,0.1666,0.0715)$\n",
    "6. 对于m=2，阀值V为8.5误差率最低，故基本分类器为$G_2(x)=\\begin{cases}\n",
    "1, & x < 8.5\\\\\n",
    "-1, & x > 8.5\n",
    "\\end{cases}$，发现样本3，4，5做错了，那么$e_2=0.0715 * 3 =0.2143$，那么$\\alpha_2=0.6496$\n",
    "7. 更新$D_3=(0.0455,0.0455,0.0455,0.1667,0.1667,0.1667,0.1060,0.1060,0.1060,0.0455)$，这时$f_2(x)=0.4236G_1(x)+0.6496G_2(x)$，发现$sign[f_2(x)]$在训练集上有3个错误分类点\n",
    "8. 对于m=3，阀值V为5.5是误差率最低，故基本分类器为$G_3(x)=\\begin{cases}\n",
    "1, & x > 5.5\\\\\n",
    "-1, & x < 5.5\n",
    "\\end{cases}$，发现样本0,1,2,9做错了，错误率就是$e_3=0.0455*4=0.1820$，计算$\\alpha_3=0.7514$\n",
    "9. 更新$D_4=(0.125,0.125,0.125,0.102,0.102,0.102,0.065,0.065,0.065,0.125)$，这时$f_3(x)=0.4236G_1(x)+0.6496G_2(x)+0.7514G_3(x)$。这是发现sign[f_3(x)]在训练集上有0个错误分类点。完美"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#4 收敛性\n",
    "![images](images/11.png)\n",
    "![images](images/12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#5 算法解释与前向分布算法\n",
    "AdaBoost算法是模型为加法模型、损失函数为指数函数、学习算法为前向分布算法时的二类学习方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#6 总结\n",
    "- AdaBoost算法可以看做是采用指数瞬时函数的提升方法，其每个基函数的学习算法为前向分布算法\n",
    "- AdaBoost的训练误差是以指数速率下降的\n",
    "- AdaBoost算法不需要实现知道下界$\\gamma$，具有自适应性，它能自适应弱分类器的训练误差率"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
