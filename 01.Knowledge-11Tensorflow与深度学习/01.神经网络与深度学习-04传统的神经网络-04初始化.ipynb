{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络的初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1 权重初始化\n",
    "- 全零初始化:错误的初始化。因为如果全零初始化，那么所有的输出都是一样的，那么在反向传播中就会算出同样的梯度，从而进行同样的参数更新，那么神经元之间也就失去了不对称性的源头.\n",
    "- 小随机数初始化:权重要非常接近于0但是不能是0，解决办法就是生成小随机数，以此来打破对称性。方法就是$\\omega=0.01 * np.random.randn(D,H)$。$randn$函数是基于零均值和标准差的一个高斯分布来生成随机数，这样每个神经元的权重向量都被初始化为一个随机向量，而这些随机向量又服从一个多变量高斯分布，这样在输入空间中，所有的神经元的指向是随机的。但是这个方法有个问题，就是随着输入数据量的增长，随机初始化的神经元的输出数据的分布中的方差也在增大。那么我们可以使用$\\frac{1}{\\sqrt{n}}$来校准方差，也就是$\\omega=\\frac{np.random.randn(n)}{\\sqrt{n}}$，其中n是输入数据的数量，这样就保证了网络中所有神经元起始时有近似同样的输出分布，这样可以提高收敛速度\n",
    "- 对于ReLU神经元的特殊初始化:$\\omega=\\frac{np.random.randn(n)}{\\sqrt{\\frac{2}{n}}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2 稀疏初始化\n",
    "另一个处理非标定方差的方法是将所有权重矩阵设为0，但是为了打破对称性，每个神经元都同下一层固定数目的神经元随机连接(其权重数值由一个小的好似分布生成)。一个比较典型的连接数目是10个"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3 偏置初始化\n",
    "通常将偏置初始化为0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#4 批量归一化\n",
    "其做法是让激活数据在训练开始前通过一个网络，网络处理数据使其服从标准高斯分布。在实现层面，应用这个技巧通常意味着全连接层(或者卷积层)与激活函数之间添加一个BatchNorm层。\n",
    "\n",
    "https://ask.julyedu.com/question/7706\n",
    "\n",
    "https://asdf0982.github.io/2017/06/06/CS231n学习笔记（批量归一化）/"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
