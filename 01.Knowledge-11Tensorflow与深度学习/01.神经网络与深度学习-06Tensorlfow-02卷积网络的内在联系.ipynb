{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 卷积网络各个参数对于网络的影响\n",
    "我们主要参考简单卷积网络里面的例子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1 卷积核的数量对于网络的影响\n",
    "修改参数conv1_kernel_num可以改变卷积核的数量，通过多次的比较，可以得到下面的图![images](images/35.png)可以看出，随着卷积核数量的增大，损失下降的更快![images](images/36.png)可以看出，随着卷积核数量的增大，正确率上升的更快"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2 激活函数的变化对于网络的影响\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2.1 卷积层激活函数\n",
    "我们会固定全连接层的激活函数是线性的，然后变化卷积层的激活函数\n",
    "![images](images/37.png)可以看出除了sigmoid函数的损失下降很慢之外，别的激活函数都基本差不多，然后我们将下半部分放大![images](images38.png)可以看到relu6激活函数的效果最好![images](images/39.png)正确率曲线，可以看到正好是损失曲线图的翻转。至于在验证集上的曲线，跟训练集是一样的，只不过曲线更加平滑，所以可以看到，激活函数为relu6,relu,elu比较好\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2.2 全连接层激活函数\n",
    "我们会固定卷积层的激活函数为ReLU，然后变化全连接层的激活函数![images](images/40.png)，可以看出linear，softplus和elu这三个激活函数表现最好![images](images/41.png)正确率曲线同样跟损失曲线正好反向"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3 学习率变化对于网络的影响\n",
    "学习率越大，损失下降的越快，正确率上升的越快，受权重初始化的影响越大，曲线比较激进，抖动厉害。学习率的选择应该遵循稳中求进的原则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#4 池化层变化对于网络的影响\n",
    "使用MaxPool和AvgPool对于网络的影响不大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#5 权重初始化对于网络的影响\n",
    "初始化权重我们使用的是random_normal，当标准差设置为$1e^{-14}$的时候的损失曲线与正确率曲线如下:![images](images/42.png)![images](images/43.png)，可以看到当标准差接近于0的时候，在训练集上损失函数抖动的很厉害，而且不收敛，并且验证集上的正确率维持在0.11，这是因为预测输出的正确率近似于随机猜测(10个类随机猜测的准确率为10%)。随着权重的标准差的增大，图像逐渐正常，一般情况下，标准差我们设置为0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当损失曲线中，训练集曲线和验证集曲线基本应该是重合的，如果验证集曲线和训练曲线中间的空隙很大，那就说明有过拟合的趋势(验证集在上，训练集在下)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#6 优化器对于网络的影响(10种优化器)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##6.1 GradientDescentOptimizer\n",
    "![images](images/44.png)![images](images/45.png)可以看到这个优化器对于学习率是很敏感的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##6.2 ProximalGradientDescentOptimizer\n",
    "![images](images/46.png)![images](images/47.png)可以看到这个优化器对于学习率是很敏感的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##6.3 AdagradOptimizer\n",
    "![images](images/48.png)![images](images/49.png)可以看到这个优化器对于学习率是很敏感的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##6.4 ProximalAdagradOptimizer\n",
    "![images](images/50.png)![images](images/51.png)可以看到这个优化器对于学习率是很敏感的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##6.5 AdagradDAOptimizer\n",
    "![images](images/52.png)![images](images/53.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##6.6 AdadeltaOptimizer\n",
    "![images](images/54.png)![images](images/55.png)效果最差的一个,而且还是线性下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##6.7 AdamOptimizer\n",
    "![images](images/56.png)![images](images/57.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##6.8 FtrlOptimizer\n",
    "![images](images/58.png)![images](images/59.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##6.9 MomentumOptimizer\n",
    "![images](images/60.png)![images](images/61.png)学习率很小的时候，也会下降，说明它对学习率不是很敏感"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##6.10 RMSPropOptimizer\n",
    "![images](images/62.png)![images](images/63.png)对于学习率非常的不敏感"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#7 非线性全连接层的神经元个数对于网络的影响\n",
    "训练集上的损失函数曲线![images](images/64.png)\n",
    "验证集上的损失函数曲线![images](images/65.png)\n",
    "可以看出神经元越多，损失收敛的越快，但是副作用就是越来越会过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#8 Dropout参数对于网络的影响\n",
    "训练集上的损失函数曲线![images](images/66.png)\n",
    "keep_prob是一个介于0到1之间的概率值，越大就代表随机失活的神经元越少，正则化强度越弱；keep_prob越小，代表随机失活的神经元越多，正则化强度越高"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#9 给非线性全连接层添加L2正则化损失对于网络的影响\n",
    "主要考虑正则化损失参数对于网络的影响\n",
    "\n",
    "参数为0.1\n",
    "![images](images/67.png)可以看到损失很大，主要都是L2的损失\n",
    "参数为0.001\n",
    "![images](images/68.png)train_loss和L2_loss的差异部分就是交叉熵损失，同样曲线更平滑，但是到了后期，出现了一些过拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
