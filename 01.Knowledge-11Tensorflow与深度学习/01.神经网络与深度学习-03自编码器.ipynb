{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#自编码器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1 传统机器学习的瓶颈\n",
    "传统的机器学习任务很大程度上依赖于好的特征工程，但是特征工程往往是非常耗时耗力的，在图像、语音、视频中提取有效的特征就更难了。更关键的是，工程师必须在某种数据处理专业领域有很深的造诣，比如CV工程师，NLP工程师，语音处理工程师，量化交易工程师"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2 无监督特征学习的特点\n",
    "- 无监督学习：即我们不需要标注数据就可以对数据进行一定程度的学习，这种学习是对数据内容的组织形式的学习，提取的是频繁出现的特征\n",
    "- 逐层抽象：特征是需要不断抽象的，就像人总是从简单基础的概念开始学习，在到复杂的概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3 特征的粒度与稀疏编码\n",
    "像素级别的特征没有任何意义，为了让机器可以从像素中提取出有效的特征，可以使用稀疏编码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3.1 稀疏编码\n",
    "- 选择一组S[k]，然后调整a[k]，使得$\\sum_k(a[k] * S[k])$最接近与T\n",
    "- 固定住a[k]，在400个碎片中，选择其它更适合的碎片S'[k]，替换原先的S[k]，使得$\\sum_k(a[k] * S'[k])$最接近T\n",
    "\n",
    "经过几次迭代后，发现最佳的S[k]组合，基本是照片上不同物体的边缘线，这些线段形状相似，区别在于方向。这就说明，复杂图形，往往是有一些基本结构组成的，可以按照线段的不同权重调和而成。同样声音也是这样的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3.2 Xaiver Initialization初始化\n",
    "如果深度学习模型的权重初始化的太小，那信号将在每层间传递时逐渐缩小而难以产生作用，如果权重初始的太大，那信号将在每层间传递时逐渐放大并导致发散和失效，如果有了Xaiver初始化，它就是让权重满足0均值，同时方差为$\\frac{1}{n_{in}+n_{out}}$，分布可以用均匀分布或高斯分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#4 AutoEncoder自动编码器\n",
    "Deep Learning最简单一种方法就是利用人工神经网络的特点。人工神经网络ANN本身就是具有层次结构的系统，如果给定一个神经网络，我们假设输出与输入是相同的，然后训练调整其参数，得到每一层中的权重，自然的我们就得到了输入I的集中不同表示，其中没一层代表一种表示，这些表示就是特征，自动编码起就是一种尽可能浮现输入信号的神经网络。为了实现这种复现，自动编码起就必须捕捉可以代表输入数据的最重要的因素，就像PCA那样，找到可以代表原信息的主要成分。\n",
    "\n",
    "- 输入一批无标签的数据，然后进行编码，紧接着进行解码，可以得到一批计算后的数据，然后跟原始数据进行比较，可以得到误差。我们需要做的就是调整编码和解码的参数，使得重构误差最小，这样就得到了特征\n",
    "- 训练好一层之后，需要进行下一层的输入，同样假如一个编码器和一个解码器，然后固定第一层的编码器，同时去掉第一层的解码器，继续训练...同样的办法可以训练多层\n",
    "- 这样就训练好了多层，最后我们需要假如一个分类器，比如SVM或者Logistic回归等，然后通过标准的多层神经网络的监督训练方法(梯度下降法)去训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4.1 Sparse AutoEncoder稀疏自动编码器\n",
    "在AutoEncoder的基础上记上L1正则化的限制(L1约束每一层节点中大部分都要为0，只有少数不为0，这也就是稀疏的名字来源)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4.2 Denoising AutoEncoders降噪自动编码器\n",
    "降噪自动编码器实在自动编码器的基础上，训练数据假如噪声，所以自动编码器必须学习去除这种噪声而获得真正的没有被噪声污染过的输入，因此他的繁华能力比一般编码器强，它可以通过梯度下降法来训练"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
