{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性回归总结\n",
    "线性回归的目的是要得到输出向量Y和输入特征X之间的线性关系，求出线性回归系数$\\theta$，也就是$Y=X\\theta$。其中Y的维度是$m * 1$，X的维度是$m * n$，而$\\theta$的维度是$n * 1$。m代表样本个数，n代表样本特征的维度。\n",
    "\n",
    "为了得到线性回归系数$\\theta$，我们需要定义一个损失函数，一个极小化损失函数的优化方法，以及一个验证算法的方法。损失函数的不同，损失函数的优化方法的不同，验证方法的不同，就形成了不同的线性回归算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1 线性回归\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1.1 损失函数\n",
    "损失函数：$J(\\theta)=\\frac{1}{2}(X\\theta-Y)^T(X\\theta-Y)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1.2 优化方法\n",
    "最小二乘法和梯度下降法最小二乘法可以解出回归系数$\\theta$为:$\\theta=(X^TX)^{-1}X^TY$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1.3 使用场景\n",
    "一般来说，只要我们觉得数据有线性关系，LinearRegression是我们的首先应该采用的。如果发现拟合或者预测的不好，再考虑用其他的线性回归库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2 岭回归\n",
    "由于单纯的线性回归没有考虑过拟合的问题，有可能泛化能力较差，这时损失函数可以加入正则化项，如果加入的是L2范数的正则化项，就是岭回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2.1 损失函数\n",
    "$J(\\theta)=\\frac{1}{2}(X\\theta-Y)^T(X\\theta-Y)+\\frac{1}{2}\\alpha{||\\theta||_2^2}$\n",
    "\n",
    "Ridge回归在不抛弃任何一个特征的情况下，缩小了回归系数，使得模型相对而言比较的稳定，不至于过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2.2 优化方法\n",
    "一般也是使用最小二乘法或梯度下降法来优化参数\n",
    "\n",
    "$\\theta=(X^TX+\\alpha{E})^{-1}X^TY$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2.3 使用场景\n",
    "一般来说，只要我们觉得数据有线性关系，用LinearRegression拟合的不是特别好，需要正则化，可以考虑用Ridge Regression。但是这个类最大的缺点是每次我们要自己指定一个超参数$\\alpha$，然后自己评估$\\alpha$的好坏，比较麻烦。一般情况下，需要配合GridSearchCV或者RandomSearchCV来一起使用，帮忙优化超参数$\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3 RidgeCV\n",
    "RidgeCV 的损失函数和损失函数的优化方法完全与 Ridge 类相同，区别在于验证方法。RidgeCV对超参数$\\alpha$使用了交叉验证，来帮忙我们选择一个合适的$\\alpha$。在初始化RidgeCV时候，我们可以传一组备选的$\\alpha$值，10个，100个都可以。RidgeCV类会帮我们选择一个合适的$\\alpha$。免去了我们自己去一轮轮筛选$\\alpha$的苦恼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3.1 使用场景\n",
    "一般来说，只要我们觉得数据有线性关系，用LinearRegression拟合的不是特别好，需要正则化，可以考虑用RidgeCV。如果输入特征的维度很高，而且是稀疏线性关系的话，RidgeCV类就不合适了。这时应该主要考虑下面要讲到的Lasso回归类家族"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#4 Lasso回归\n",
    "线性回归的L1正则化通常称为Lasso回归，它和Ridge回归的区别是在损失函数上增加了的是L1正则化的项，而不是L2正则化项。L1正则化的项也有一个常数系数$\\alpha$来调节损失函数的均方差项和正则化项的权重"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4.1 损失函数\n",
    "$J(\\theta)=\\frac{1}{2m}(X\\theta-Y)^T(X\\theta-Y)+\\alpha{||\\theta||_1}$\n",
    "\n",
    "Lasso回归可以使得一些特征的系数变小，甚至还是一些绝对值比较小的系数直接变为$\\theta$。增强模型的泛化能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#5 弹性网\n",
    "ElasticNet可以看做Lasso和Ridge的结合。它也是对普通的线性回归做了正则化，但是它的损失函数记不全是L1的正则化，也不全是L2的正则化，而是一个权重$\\rho$来平衡L1和L2正则化的比重"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##5.1 损失函数\n",
    "$J(\\theta)=\\frac{1}{2m}(X\\theta-Y)^T(X\\theta-Y)+\\alpha\\rho{||\\theta||_1} + \\frac{\\alpha(1-\\rho)}{2}||\\theta||_2^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##5.2 使用场景\n",
    "ElasticNet用在我们发现用Lasso回归太过（太多特征被稀疏为0），而用Ridge回归又正则化的不够（回归系数衰减的太慢）的时候。一般不推荐拿到数据就直接就ElasticNet"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}