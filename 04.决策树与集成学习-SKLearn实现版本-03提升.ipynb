{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1 XGBoost计算样本错误率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xgboost.core.DMatrix object at 0x108862fd0>\n<class 'xgboost.core.DMatrix'>\n[0]\teval-error:0.016139\ttrain-error:0.014433\teval-error:0.016139\ttrain-error:0.014433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\teval-error:0.016139\ttrain-error:0.014433\teval-error:0.016139\ttrain-error:0.014433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\teval-error:0.016139\ttrain-error:0.014433\teval-error:0.016139\ttrain-error:0.014433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\teval-error:0.016139\ttrain-error:0.014433\teval-error:0.016139\ttrain-error:0.014433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4]\teval-error:0.002483\ttrain-error:0.003071\teval-error:0.002483\ttrain-error:0.003071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\teval-error:0.002483\ttrain-error:0.003071\teval-error:0.002483\ttrain-error:0.003071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6]\teval-error:0.002483\ttrain-error:0.003071\teval-error:0.002483\ttrain-error:0.003071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  6.09937888e-06   9.84727502e-01   6.09937888e-06 ...,   9.99932647e-01\n   4.45600620e-07   9.99932647e-01]\n[ 0.  1.  0. ...,  1.  0.  1.]\n样本总数：\t 1611\n错误数目：\t   4\n错误率：\t0.24829%\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "# 定义f: theta * x\n",
    "def log_reg(y_hat, y):\n",
    "    p = 1.0 / (1.0 + np.exp(-y_hat))\n",
    "    g = p - y.get_label()\n",
    "    h = p * (1.0-p)\n",
    "    return g, h\n",
    "\n",
    "def error_rate(y_hat, y):\n",
    "    return 'error', float(sum(y.get_label() != (y_hat > 0.5))) / len(y_hat)\n",
    "\n",
    "# 读取数据\n",
    "data_train = xgb.DMatrix('data/agaricus_train.txt')\n",
    "data_test = xgb.DMatrix('data/agaricus_test.txt')\n",
    "print(data_train)\n",
    "print(type(data_train))\n",
    "\n",
    "# 设置参数\n",
    "param = {'max_depth': 3, 'eta': 1, 'silent': 1, 'objective': 'binary:logistic'} # logitraw\n",
    "# param = {'max_depth': 3, 'eta': 0.3, 'silent': 1, 'objective': 'reg:logistic'}\n",
    "watchlist = [(data_test, 'eval'), (data_train, 'train')]\n",
    "n_round = 7\n",
    "# bst = xgb.train(param, data_train, num_boost_round=n_round, evals=watchlist)\n",
    "bst = xgb.train(param, data_train, num_boost_round=n_round, evals=watchlist, obj=log_reg, feval=error_rate)\n",
    "\n",
    "# 计算错误率\n",
    "y_hat = bst.predict(data_test)\n",
    "y = data_test.get_label()\n",
    "print(y_hat)\n",
    "print(y)\n",
    "error = sum(y != (y_hat > 0.5))\n",
    "error_rate = float(error) / len(y_hat)\n",
    "print('样本总数：\\t', len(y_hat))\n",
    "print('错误数目：\\t%4d' % error)\n",
    "print('错误率：\\t%.5f%%' % (100*error_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2 Logisitic回归与XGBoost的比较-Wine实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic回归正确率： 0.943820224719\n[0]\teval-merror:0.011236\ttrain-merror:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\teval-merror:0\ttrain-merror:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost正确率： 1.0\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split   # cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 作业：尝试用Pandas读取试试？\n",
    "data = np.loadtxt('data/wine.data', dtype=float, delimiter=',')\n",
    "y, x = np.split(data, (1,), axis=1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=1, test_size=0.5)\n",
    "\n",
    "# Logistic回归\n",
    "lr = LogisticRegression(penalty='l2')\n",
    "lr.fit(x_train, y_train.ravel())\n",
    "y_hat = lr.predict(x_test)\n",
    "print('Logistic回归正确率：', accuracy_score(y_test, y_hat))\n",
    "\n",
    "# XGBoost\n",
    "y_train[y_train == 3] = 0\n",
    "y_test[y_test == 3] = 0\n",
    "data_train = xgb.DMatrix(x_train, label=y_train)\n",
    "data_test = xgb.DMatrix(x_test, label=y_test)\n",
    "watch_list = [(data_test, 'eval'), (data_train, 'train')]\n",
    "params = {'max_depth': 3, 'eta': 1, 'silent': 0, 'objective': 'multi:softmax', 'num_class': 3}\n",
    "bst = xgb.train(params, data_train, num_boost_round=2, evals=watch_list)\n",
    "y_hat = bst.predict(data_test)\n",
    "print('XGBoost正确率：', accuracy_score(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3 Logisitic回归与XGBoost的比较-agaricus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic回归正确率： 1.0\n[0]\teval-merror:0.035687\ttrain-merror:0.040696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\teval-merror:0.007291\ttrain-merror:0.009982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\teval-merror:0.000767\ttrain-merror:0.000512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\teval-merror:0.000767\ttrain-merror:0.000512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost正确率： 0.999232540292\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def read_data(path):\n",
    "    y = []\n",
    "    row = []\n",
    "    col = []\n",
    "    values = []\n",
    "    r = 0       # 首行\n",
    "    for d in open(path):\n",
    "        d = d.strip().split()      # 以空格分开\n",
    "        y.append(int(d[0]))\n",
    "        d = d[1:]\n",
    "        for c in d:\n",
    "            key, value = c.split(':')\n",
    "            row.append(r)\n",
    "            col.append(int(key))\n",
    "            values.append(float(value))\n",
    "        r += 1\n",
    "    x = scipy.sparse.csr_matrix((values, (row, col))).toarray()\n",
    "    y = np.array(y)\n",
    "    return x, y\n",
    "\n",
    "x, y = read_data('data/agaricus_train.txt')\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=1, train_size=0.6)\n",
    "\n",
    "# Logistic回归\n",
    "lr = LogisticRegression(penalty='l2')\n",
    "lr.fit(x_train, y_train.ravel())\n",
    "y_hat = lr.predict(x_test)\n",
    "print('Logistic回归正确率：', accuracy_score(y_test, y_hat))\n",
    "\n",
    "# XGBoost\n",
    "data_train = xgb.DMatrix(x_train, label=y_train)\n",
    "data_test = xgb.DMatrix(x_test, label=y_test)\n",
    "watch_list = [(data_test, 'eval'), (data_train, 'train')]\n",
    "param = {'max_depth': 3, 'eta': 1, 'silent': 0, 'objective': 'multi:softmax', 'num_class': 3}\n",
    "bst = xgb.train(param, data_train, num_boost_round=4, evals=watch_list)\n",
    "y_hat = bst.predict(data_test)\n",
    "print('XGBoost正确率：', accuracy_score(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#4 使用XGBoost对鸢尾花数据计算正确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-merror:0.04\ttrain-merror:0.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\teval-merror:0.04\ttrain-merror:0.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\teval-merror:0.02\ttrain-merror:0.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\teval-merror:0.02\ttrain-merror:0.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4]\teval-merror:0.02\ttrain-merror:0.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\teval-merror:0.02\ttrain-merror:0.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正确率:\t 0.98\nEND.....\n\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split   # cross_validation\n",
    "\n",
    "def iris_type(s):\n",
    "    it = {'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2}\n",
    "    return it[s]\n",
    "\n",
    "data = pd.read_csv('data/iris.data', header=None)\n",
    "x = data.values[:,:4]\n",
    "y_prime = data.values[:, -1:]\n",
    "\n",
    "YY = []\n",
    "for index, vec in enumerate(y_prime):\n",
    "    YY.append(vec[0])\n",
    "\n",
    "y = pd.Categorical(YY).codes\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=1, test_size=50)\n",
    "\n",
    "data_train = xgb.DMatrix(x_train, label=y_train)\n",
    "data_test = xgb.DMatrix(x_test, label=y_test)\n",
    "watch_list = [(data_test, 'eval'), (data_train, 'train')]\n",
    "param = {'max_depth': 2, 'eta': 0.3, 'silent': 1, 'objective': 'multi:softmax', 'num_class': 3}\n",
    "\n",
    "bst = xgb.train(param, data_train, num_boost_round=6, evals=watch_list)\n",
    "y_hat = bst.predict(data_test)\n",
    "result = y_test.reshape(1, -1) == y_hat\n",
    "print('正确率:\\t', float(np.sum(result)) / len(y_hat))\n",
    "print('END.....\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#5 Logistic回归、随机森林和XGBoost的比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic回归：0.805%\n随机森林：0.983%\nXGBoost：0.983%\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "def show_accuracy(a, b, tip):\n",
    "    acc = a.ravel() == b.ravel()\n",
    "    acc_rate = 100 * float(acc.sum()) / a.size\n",
    "    print('%s正确率：%.3f%%' % (tip, acc_rate))\n",
    "    return acc_rate\n",
    "\n",
    "\n",
    "def load_data(file_name, is_train):\n",
    "    data = pd.read_csv(file_name)  # 数据文件路径\n",
    "\n",
    "    # 性别\n",
    "    data['Sex'] = data['Sex'].map({'female': 0, 'male': 1}).astype(int)\n",
    "\n",
    "    # 补齐船票价格缺失值\n",
    "    if len(data.Fare[data.Fare.isnull()]) > 0:\n",
    "        fare = np.zeros(3)\n",
    "        for f in range(0, 3):\n",
    "            fare[f] = data[data.Pclass == f + 1]['Fare'].dropna().median()\n",
    "        for f in range(0, 3):  # loop 0 to 2\n",
    "            data.loc[(data.Fare.isnull()) & (data.Pclass == f + 1), 'Fare'] = fare[f]\n",
    "\n",
    "    # 年龄：使用均值代替缺失值\n",
    "    if is_train:\n",
    "        # 年龄：使用随机森林预测年龄缺失值\n",
    "        data_for_age = data[['Age', 'Survived', 'Fare', 'Parch', 'SibSp', 'Pclass']]\n",
    "        age_exist = data_for_age.loc[(data.Age.notnull())]   # 年龄不缺失的数据\n",
    "        age_null = data_for_age.loc[(data.Age.isnull())]\n",
    "        # print age_exist\n",
    "        x = age_exist.values[:, 1:]\n",
    "        y = age_exist.values[:, 0]\n",
    "        rfr = RandomForestRegressor(n_estimators=1000)\n",
    "        rfr.fit(x, y)\n",
    "        age_hat = rfr.predict(age_null.values[:, 1:])\n",
    "        # print age_hat\n",
    "        data.loc[(data.Age.isnull()), 'Age'] = age_hat\n",
    "    else:\n",
    "        data_for_age = data[['Age', 'Fare', 'Parch', 'SibSp', 'Pclass']]\n",
    "        age_exist = data_for_age.loc[(data.Age.notnull())]  # 年龄不缺失的数据\n",
    "        age_null = data_for_age.loc[(data.Age.isnull())]\n",
    "        x = age_exist.values[:, 1:]\n",
    "        y = age_exist.values[:, 0]\n",
    "        rfr = RandomForestRegressor(n_estimators=1000)\n",
    "        rfr.fit(x, y)\n",
    "        age_hat = rfr.predict(age_null.values[:, 1:])\n",
    "        data.loc[(data.Age.isnull()), 'Age'] = age_hat\n",
    "\n",
    "    # 起始城市\n",
    "    data.loc[(data.Embarked.isnull()), 'Embarked'] = 'S'  # 保留缺失出发城市\n",
    "    embarked_data = pd.get_dummies(data.Embarked)\n",
    "    embarked_data = embarked_data.rename(columns=lambda x: 'Embarked_' + str(x))\n",
    "    data = pd.concat([data, embarked_data], axis=1)\n",
    "    data.to_csv('New_Data.csv')\n",
    "\n",
    "    x = data[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_C', 'Embarked_Q', 'Embarked_S']]\n",
    "    y = None\n",
    "    if 'Survived' in data:\n",
    "        y = data['Survived']\n",
    "\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # 思考：这样做，其实发生了什么？\n",
    "    x = np.tile(x, (5, 1))\n",
    "    y = np.tile(y, (5, ))\n",
    "    if is_train:\n",
    "        return x, y\n",
    "    return x, data['PassengerId']\n",
    "\n",
    "\n",
    "def write_result(c, c_type):\n",
    "    file_name = 'data/Titanic.test.csv'\n",
    "    x, passenger_id = load_data(file_name, False)\n",
    "\n",
    "    if type == 3:\n",
    "        x = xgb.DMatrix(x)\n",
    "    y = c.predict(x)\n",
    "    y[y > 0.5] = 1\n",
    "    y[~(y > 0.5)] = 0\n",
    "\n",
    "    predictions_file = open(\"Prediction_%d.csv\" % c_type, \"wb\")\n",
    "    open_file_object = csv.writer(predictions_file)\n",
    "    open_file_object.writerow([\"PassengerId\", \"Survived\"])\n",
    "    open_file_object.writerows(zip(passenger_id, y))\n",
    "    predictions_file.close()\n",
    "\n",
    "\n",
    "x, y = load_data('data/Titanic.train.csv', True)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=1)\n",
    "\n",
    "lr = LogisticRegression(penalty='l2')\n",
    "lr.fit(x_train, y_train)\n",
    "y_hat = lr.predict(x_test)\n",
    "lr_acc = accuracy_score(y_test, y_hat)\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "rfc.fit(x_train, y_train)\n",
    "y_hat = rfc.predict(x_test)\n",
    "rfc_acc = accuracy_score(y_test, y_hat)\n",
    "\n",
    "# XGBoost\n",
    "data_train = xgb.DMatrix(x_train, label=y_train)\n",
    "data_test = xgb.DMatrix(x_test, label=y_test)\n",
    "watch_list = [] #[(data_test, 'eval'), (data_train, 'train')]\n",
    "param = {'max_depth': 6, 'eta': 0.8, 'silent': 1, 'objective': 'binary:logistic'}\n",
    "bst = xgb.train(param, data_train, num_boost_round=100, evals=watch_list)\n",
    "y_hat = bst.predict(data_test)\n",
    "y_hat[y_hat > 0.5] = 1\n",
    "y_hat[~(y_hat > 0.5)] = 0\n",
    "xgb_acc = accuracy_score(y_test, y_hat)\n",
    "\n",
    "print('Logistic回归：%.3f%%' % lr_acc)\n",
    "print('随机森林：%.3f%%' % rfc_acc)\n",
    "print('XGBoost：%.3f%%' % xgb_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}