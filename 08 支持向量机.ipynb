{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#支持向量机"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1. 概述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "支持向量机的三种情况：线性可分、近似线性可分、非线性可分.不仅可以做二分类，也可以做多分类\n",
    "- 线性可分：在空间中可以找到一条直线，或一个平面，或一个超平面，把样本可以完美的分开，一类在一边，另一类在另一边，也就是$\\Theta{X} + b$\n",
    "- 近似线性可分：在空间中可以找到一条直线，或一个平面，或一个超平面，把样本可以近似的分开\n",
    "- 非线性可分：在空间中可以找到一条曲线，或一个曲面，把样本可以完美的分开\n",
    "\n",
    "逻辑回归是不能进行非线性可分的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2. 重要概念\n",
    "![images](images/08/28.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如上图所示，在空间中存在9个点，我们可以找到很多的平面来把它们完美的分为两组，比如中间的两个实线所在的平面，这两个面可以向左右进行平移，当与某个点相交后，可以形成两个不同的虚面，(蓝色的形成两个虚面，红色的形成两个虚面)，那么蓝色的两个虚面的距离和，就叫做D，同样两个红色虚面的距离和，也叫做D，那么使得D最大的分割面，就是最好的分割面,虚面叫做支撑平面，跟支撑平面相交的点，就叫做支撑向量,D叫做间隔.分割面的公式:$\\vec{w}\\bullet\\vec{x}+b=0$.两个支撑平面的公式：$\\vec{w}\\bullet\\vec{x}+b=-1$,$\\vec{w}\\bullet\\vec{x}+b=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w是平面的法向量(垂直于平面的向量)，-1和+1是分类标签(逻辑回归的分类标签是0和1).假设x'是支撑平面上的一点，那么支撑平面到分割平面的距离，就是这个点x'到分割平面的距离，那么就是:$\\frac{\\vec{w}x^{'}+b}{||w||}=\\frac{1}{||w||}$,那么$D=\\frac{2}{||w||}$，也就是2除以w的模长(w的模长就是w的长度)但是这里有个约束条件，那么就是样本点带入wx+b的公式中，要么结果是<=-1的，要么就是>=1的，总结一下就有如下约束$y*(\\vec{w}\\bullet{x}+b)>=1$,x就是样本点的坐标，等于1的时候，样本点正好落在支撑平面上,所以我们需要求解的问题，就变成了$argmax(D)=argmax(\\frac{2}{||w||})=argmin(\\frac{1}{2}||w||)=argmin(\\frac{1}{2}||w||^2)$，最后一步是为了将来好做处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3. 解法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3.1 拉格朗日乘子法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要找函数$z=f(x,y)$在条件$\\varphi(x,y)=0$下可能的极值点，那么我们可以构造函数$F(x,y)=f(x,y)+\\lambda\\varphi(x,y)$,其中$\\lambda$为某一常数，也叫做拉格朗日乘子。令$\\frac{\\partial{F}}{\\partial{x}}=0,\\frac{\\partial{F}}{\\partial{y}}=0,\\frac{\\partial{F}}{\\partial{\\lambda}}=0$,\n",
    "$\\begin{cases} \n",
    "f_x(x,y)+\\lambda\\varphi_x(x,y)=0\\\\ \n",
    "f_y(x,y)+\\lambda\\varphi_y(x,y)=0\n",
    "\\end{cases}$\n",
    ",$\\varphi(x,y)=0$,解出$x,y,\\lambda$,其中x,y就是可能的极值点。举例说明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例题：将正数12分成x,y,z三个正数，使得$\\mu_{max}=x^3y^2z$为最大，求解x,y,z以及$\\mu_{max}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解：由题目可知$x+y+z=12$,所以$\\varphi(x,y,z)=x+y+z-12=0$,令$F(x,y,z)=x^3y^2z+\\lambda(x+y+z-12)$，则\n",
    "$\\begin{cases}\n",
    "\\frac{\\partial{F}}{\\partial{x}}=F_x^{'}=3y^2zx^2+\\lambda=0\\\\\n",
    "\\frac{\\partial{F}}{\\partial{y}}=F_y^{'}=2x^3zy+\\lambda=0\\\\\n",
    "\\frac{\\partial{F}}{\\partial{z}}=F_z^{'}=x^3y^2+\\lambda=0\\\\\n",
    "x+y+z=12\n",
    "\\end{cases}\n",
    "$\n",
    "$\\Rightarrow 2x=3y,y=2z$\n",
    "\n",
    "解得唯一驻点是(6,4,2)，故$\\mu_{max}=6^3*4^2*2=6912$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3.2 广义拉格朗日乘子法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设要求的极值函数为F(x),其中的条件有m个不等式:$[f_1(x)\\leq 0,f_2(x)\\leq 0,...,f_m(x)\\leq 0]$,p个等式:$[h_1(x)=0,h_2(x)=0,...,h_p(x)=0]$，那么可以分别给定m个不等式因子$\\lambda_{m}$，以及p个等式因子$\\nu_{p}$,形成广义拉格朗日函数为:\n",
    "$L(x,y,\\lambda,\\nu)=F(x)+\\sum_{i=1}^p\\nu_ih_i(x)+\\sum_{j=1}^m\\lambda_jf_j(x)$,\n",
    "\n",
    "$\\because h(x)=0 \\Rightarrow \\sum_{j=1}^p\\nu_jh_j(x)=0$\n",
    "\n",
    "$\\therefore L(x,y,\\lambda,\\nu)=L(x,y,\\lambda)=F(x)+\\sum_{i=1}^m\\lambda_if_i(x)$\n",
    "\n",
    "$\\because \\lambda > 0, f(x)\\leq 0$\n",
    "\n",
    "$\\therefore L(x,y,\\lambda)\\leq F(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KKT条件，就是使得$L(x,y,\\lambda)=F(x)$的条件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3.3 初始问题求解-拉格朗日函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最开始的问题，我们的目标是\n",
    "$\\begin{cases}\n",
    "min(\\frac{1}{2}||w||^2) \\\\\n",
    "y(w^Tx+b) >=1\n",
    "\\end{cases}\n",
    "$.通过上面的拉格朗日乘子法，我们就可以求解了。\n",
    "\n",
    "$$\\because y(w^Tx+b) >=1, \\therefore 1-y(w^Tx+b)\\leq 0$$\n",
    "\n",
    "加入拉格朗日因子$\\alpha$,变化为$-\\alpha(y(w^Tx+b)-1)\\leq 0$，那么拉格朗日函数就是$L(w,b,\\alpha)=\\frac{1}{2}||w||^2-\\alpha(y(w^Tx+b)-1)=\\frac{1}{2}||w||^2-\\sum_{i=1}^n\\alpha_i(y_i(w^Tx_i+b)-1)$,其中$x_i$就是第i个样本,$y_i$就是第i个样本对应的分类,$\\alpha_i$就是第i个样本对应的拉格朗日乘子,$\\alpha_i\\geq 0$，那么我们我们要求的是一个极小极大问题：\n",
    "\n",
    "$min_{w,b}max_{\\alpha_i}L(w,b,\\alpha)=min_{w,b}max_{\\alpha_i}[\\frac{1}{2}||w||^2-\\sum_{i=1}^n\\alpha_i(y_i(w^Tx_i+b)-1)]$,展开有\n",
    "\n",
    "$min_{w,b}max_{\\alpha_i}L(w,b,\\alpha)=[\\frac{1}{2}||w||^2-\\sum_{i=1}^n\\alpha_iy_i(w^Tx_i+b)+\\sum_{i=1}^n\\alpha_i],\\alpha_i\\geq 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是一个凸规划问题，意义是先对$\\alpha$求偏导数，令其等于0消掉$\\alpha$，然后再对w和b求偏导数以求得L的最小值。但是直接求解由难度，我们需要引入拉格朗日对偶函数来解决这个问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3.4 拉格朗日对偶函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$min_{w,b}max_{\\alpha_i}L(w,b,\\alpha)=max_{\\alpha_i}min_{w,b}L(w,b,\\alpha)$,这样我们可以先对w，b求偏导数，令其结果为0，消掉w,b，然后在对$\\alpha$求L的最大值。有：\n",
    "\n",
    "$\\frac{\\partial{L}}{\\partial{w}}=0 \\Rightarrow \\frac{1}{2}*2||w||-\\sum_{i=1}^n\\alpha_iy_ix_i=0 \\Rightarrow w=\\sum_{i=1}^n\\alpha_iy_ix_i$\n",
    "\n",
    "$\\frac{\\partial{L}}{\\partial{b}}=0 \\Rightarrow \\sum_{i=1}^n\\alpha_iy_i=0$\n",
    "\n",
    "这是两个非常重要的等式，然后我们展开原来的式子:\n",
    "\n",
    "$min_{w,b}max_{\\alpha_i}L(w,b,\\alpha)=max_{\\alpha_i}min_{w,b}L(w,b,\\alpha)=max_{\\alpha_i}min_{w,b}(\\frac{1}{2}||w||^2-\\sum_{i=1}^n\\alpha_iy_iw^Tx_i-\\sum_{i=1}^n\\alpha_iy_ib+\\sum_{i=1}^n\\alpha_i)$\n",
    "\n",
    "$\\therefore max_{\\alpha_i}min_{w,b}L(w,b,\\alpha)=max_{\\alpha_i}min_{w,b}=(\\frac{1}{2}||w||^2-w^T\\sum_{i=1}^n\\alpha_iy_ix_i-b\\sum_{i=1}^n\\alpha_iy_i+\\sum_{i=1}^n\\alpha_i)$\n",
    "\n",
    "将结果带入上式，有$max_{\\alpha_i}L(\\alpha)=max_{\\alpha_i}(\\frac{1}{2}||w||^2-w^T*w-b*0+\\sum_{i=1}^n\\alpha_i)=max_{\\alpha_i}(\\sum_{i=1}^n\\alpha_i-\\frac{1}{2}||w||^2)=max_{\\alpha_i}(\\sum_{i=1}^n\\alpha_i-\\frac{1}{2}(\\sum_{i=1}^n\\alpha_iy_ix_i)^2)=max_{\\alpha_i}(\\sum_{i=1}^n\\alpha_i-\\frac{1}{2}\\sum_{i,j=1}^n\\alpha_i\\alpha_jy_iy_jx_ix_j),\\alpha_i\\geq 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3.5 求解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\because min_{w,b}max_{\\alpha_i}L(w,b,\\alpha)=min\\frac{1}{2}||w||^2$\n",
    "\n",
    "$\\because L(w,b,\\alpha)=\\frac{1}{2}||w||^2-\\sum_{i=1}^n\\alpha_i(y_i(w^Tx_i+b)-1),\\alpha_i\\geq 0,y_i(w^Tx_i+b)\\geq 1$\n",
    "\n",
    "$\\therefore$ 只有每一个$\\alpha_i(y_i(w^Tx_i+b)-1)$都是0，结果才能为0从而达到才能取得最大值\n",
    "\n",
    "这样我们就能得到KKT条件:\n",
    "$\\begin {cases}\n",
    "\\alpha_i\\geq 0 \\\\\\\n",
    "y_i(w^Tx_i+b)-1\\geq 0 \\\\\\\n",
    "\\alpha_i(y_i(w^Tx_i+b)-1)=0\n",
    "\\end {cases}$\n",
    "\n",
    "根据KKT条件3，我们可以得出这样一个结论$\\alpha_i=0$或者$y_i(w^Tx_i+b)=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以如果一个样本是支持向量，则其对应的拉格朗日系数非零；如果一个样本不是支持向量，则其对应的拉格朗日系数一定为0。也就是说我们要找的支撑平面只跟支撑向量有关"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#4. 近似线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4.1 概述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "间隔代表着向量机的健壮性，完美分离的平面未必最好(间隔不够大，那么就不够健壮)，并且有时候有些数据是非线性可分的，所以这个时候我们就会找到一个平面，只要近似可分就可以了。\n",
    "- 硬间隔：完美分割所有数据样本的间隔，就是硬间隔\n",
    "- 软间隔：有部分数据样本分错的间隔，就是软间隔"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4.2 松弛变量与惩罚因子\n",
    "松弛变量代表着分错的样本(分到两个支撑平面里面的样本)距离最近的支撑平面的距离.C代表对分错的数据点的惩罚，C越大，惩罚越重，分错的数据点就越少，容易过拟合；C越小，惩罚越小，分错的数据点越多，精度下降，C趋近于无穷大的时候，就等价于硬间隔。C越小，支撑平面间隔就越大，反之，就会越小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4.3 目标函数与约束条件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假如松弛变量后，目标变为\n",
    "$\\begin {cases}\n",
    "min(\\frac{1}{2}||w||^2+C\\sum_{i=1}^n\\zeta_i) &\\zeta_i\\geq 0,i=1,2,...,n \\\\\\\n",
    "y_i(w^Tx_i+b)\\geq 1-\\zeta_i &i=1,2,...,n\n",
    "\\end {cases}$\n",
    ",也就是说，虽然可以允许一部分样本分错，但是不能离支撑平面太远，跟逻辑回归类似.对此函数求导，跟线性可分的求导结果一样，对偶函数的结果也一样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4.4 硬间隔与软间隔的求解方程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目标是：$max_\\alpha(\\sum_{i=1}^n\\alpha_i-\\frac{1}{2}\\sum_{i,j=1}^n\\alpha_i\\alpha_jy_iy_jx_i^Tx_j),\\alpha_i\\geq 0$,KKT条件是：\n",
    "$\\begin {cases}\n",
    "0\\leq \\alpha_i\\leq C \\\\\\\n",
    "y_i(w^Tx_i+b)-1\\geq 0 \\\\\\\n",
    "\\alpha_i(y_i(w^Tx_i+b)-1)=0 \\\\\\\n",
    "\\sum_{i=1}^n\\alpha_iy_i=0\n",
    "\\end {cases}$\n",
    ",其中$x_i^Tx_j$是向量$x_i$的转置与向量$x_j$的点积"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#5. 非线性可分与核函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##5.1 引入\n",
    "思路：对于在二维空间中的非线性可分，我们可以引入一个第三维，将二维空间扩展到三维空间，从而将低维度的非线性可分，转化为高维度的线性可分，因为在三维空间中的Z轴上，会有一个平面可以进行线性可分的，现在就有两个问题：\n",
    "- 在不知道数据规律的情况下，如果确定该映射到几维空间？\n",
    "- 新空间坐标如何根据现有维度来确定？\n",
    "\n",
    "根据非线性可分的求解方程，我们知道如果映射到高维空间，只有X的部分会发生变化，因为映射到高维空间，数据样本会发生变化，点积也必然会发生变化，这个时候的点积是高维空间的点积，所以如果我们能够这个点积的结果，那么就可以不用思考上面的两个问题了，这个时候，进引入了核函数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##5.2 核函数\n",
    "将低维空间的变量，带入一个函数，可以得到映射到高维空间后的点积，这个函数就是核函数.核函数只是一种运算技巧，用来计算映射到高维空间之后的点积的一种简便方法，实际中我们根本就不知道映射到底是什么形式的。常见的核函数有：\n",
    "- 高斯核(径向基核，RBF)：高斯核可以映射到无穷维\n",
    "- 指数核\n",
    "- 多项式核"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##5.3 引入核函数前后的做法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "引入核函数后，唯一变化的就是向量的点积不再是原始空间上的了，而是通过核函数计算出来的映射到高维空间上的点积。$max_\\alpha(\\sum_{i=1}^n\\alpha_i-\\frac{1}{2}\\sum_{i,j=1}^n\\alpha_i\\alpha_jy_iy_jK(x_i^T,x_j)),\\alpha_i\\geq 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#6. SMO算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##6.1 求解目标\n",
    "无论，我们是线性可分、近似线性可分或者是分线性可分，都可以通过加入松弛变量，或者是核函数，来映射成求$max_\\alpha(\\sum_{i=1}^n\\alpha_i-\\frac{1}{2}\\sum_{i,j=1}^n\\alpha_i\\alpha_jy_iy_jK(x_i^T,x_j)),0\\leq \\alpha_i\\leq C$,要解决的是在参数${\\alpha_1,\\alpha_2,...,\\alpha_n}$上求最大值的问题，至于$x_i$和$y_i$都是已知数。C由我们预先设定，也是已知数.所以我们需要求的其实就是所有的$\\alpha_i$使得上面的式子最大，求出以后，找到b，最后求出w.所以我们的思路必然是对于每个$\\alpha_i$求偏导数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##6.2 固定一个参数带来的问题\n",
    "如果我们固定除$\\alpha_1$之外的所有参数，然后在$\\alpha_1$上求极值，会有问题。因为如果别的参数固定了，那么a1也就固定了，这是因为有一个条件约束,$\\because \\sum_{i=1}^n\\alpha_iy_i=0, \\therefore \\alpha_1y_1=-\\sum_{i=2}^n\\alpha_iy_i$,所以这个思路有问题，我们只能最少选择两个参数进行优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##6.3 伪代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重复下面的过程知道收敛{\n",
    "- 选择两个拉格朗日乘子$\\alpha_i$和$\\alpha_j$\n",
    "- 固定其它拉格朗日乘子，只对$\\alpha_i$和$\\alpha_j$进行优化，会得到一个$\\alpha$的向量w($\\alpha$)\n",
    "- 根据优化后的$\\alpha_i$和$\\alpha_j$，更新截距b的值\n",
    "}\n",
    "\n",
    "迭代的停止条件是$\\alpha_i$和$\\alpha_j$基本没有改变，或者总得迭代次数达到了迭代次数上限"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##6.4 选择$\\alpha_i$和$\\alpha_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于$\\alpha_i$的选择，主要根据启发法。启发法主要是根据KKT条件来选择，需要选择那些违反KKT条件的$\\alpha_i$选择出来,我们根据KKT条件，可以得出结论：\n",
    "$\\begin {cases}\n",
    "y_i(w^Tx_i+b) > 1 &\\alpha_i=0 \\\\\\\n",
    "y_i(w^Tx_i+b) < 1 &\\alpha_i=C \\\\\\\n",
    "y_i(w^Tx_i+b) = 1 &0<\\alpha_i<C\n",
    "\\end {cases}$\n",
    ",两个支撑平面外的点，符合要求1；对于支撑平面之间的点，符合要求2；对于落在支撑平面上的点，符合要求3；如果我们的样本不满足上面的三个条件，那么就是违反了KKT的条件,一般情况下，我们会根据是否违反KKT条件来选择$\\alpha_i$，那么$\\alpha_j$的选择就是随机选择一个\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##6.5 优化\n",
    "既然选择了$\\alpha_i$和$\\alpha_j$，固定了其它的$\\alpha$，那么我们的对偶函数就变成了一个关于$\\alpha_i$和$\\alpha_j$的二元二次方程，具体推导如下：假设$K_{ij}=K(x_i^T,x_j)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sum_{i,j=1}^n\\alpha_i\\alpha_jy_iy_jK_{ij}=\\alpha_1y_1\\sum_{j=1}^n\\alpha_jy_jK_{1j}+\\alpha_2y_2\\sum_{j=1}^n\\alpha_jy_jK_{2j}+\\sum_{i=3}^n\\sum_{j=1}^n\\alpha_i\\alpha_jy_iy_jK_{ij} \n",
    "$\n",
    "\n",
    "$=\\alpha_1y_1\\alpha_1y_1K_{11}+\\alpha_1y_1\\alpha_2y_2K_{12}+\\alpha_1y_1\\sum_{i=3}^n\\alpha_iy_iK_{1n}+\\alpha_2y_2\\alpha_1y_1K_{21}+\\alpha_2y_2\\alpha_2y_2K_{22}+\\alpha_2y_2\\sum_{i=3}^n\\alpha_iy_iK_{2n}+\\alpha_1y_1\\sum_{i=3}^n\\alpha_iy_iK_{n1}+\\alpha_2y_2\\sum_{i=3}^n\\alpha_iy_iK_{n2}+\\sum_{i=3}^n\\sum_{j=3}^n\\alpha_i\\alpha_jK_{ij}$\n",
    "\n",
    "$\\because y \\in \\{-1,1\\}, \\therefore y_1^2 = 1$\n",
    "\n",
    "$\\therefore \\Rightarrow K_{11}\\alpha_1^2+K_{22}\\alpha_2^2+2K_{12}y_1y_2\\alpha_1\\alpha_2+y_1\\alpha_1(\\sum_{i=3}^n\\alpha_iy_iK_{1n}+\\sum_{i=3}^n\\alpha_iy_iK_{n1})+y_2\\alpha_2(\\sum_{i=3}^n\\alpha_iy_iK_{2n}+\\sum_{i=3}^n\\alpha_iy_iK_{n2})+\\sum_{i=3}^n\\sum_{j=3}^n\\alpha_i\\alpha_jK_{ij}$\n",
    "\n",
    "$\\therefore \\psi=\\sum_{i=1}^n\\alpha_i-\\frac{1}{2}(K_{11}\\alpha_1^2+K_{22}\\alpha_2^2+2K_{12}y_1y_2\\alpha_1\\alpha_2+y_1\\alpha_1(\\sum_{i=3}^n\\alpha_iy_iK_{1n}+\\sum_{i=3}^n\\alpha_iy_iK_{n1})+y_2\\alpha_2(\\sum_{i=3}^n\\alpha_iy_iK_{2n}+\\sum_{i=3}^n\\alpha_iy_iK_{n2})+\\sum_{i=3}^n\\sum_{j=3}^n\\alpha_i\\alpha_jK_{ij})$\n",
    "\n",
    "$=\\alpha_1+\\alpha_2+\\sum_{i=3}^n\\alpha_i-\\frac{1}{2}K_{11}\\alpha_1^2-\\frac{1}{2}K_{22}\\alpha_2^2-K_{12}y_1y_2\\alpha_1\\alpha_2-\\nu_1y_1\\alpha_1-\\nu_2y_2\\alpha_2-\\varphi_{constant}$\n",
    "\n",
    "$\\psi_{min}=min_{\\alpha_1,\\alpha_2}(\\frac{1}{2}K_{11}\\alpha_1^2+\\frac{1}{2}K_{22}\\alpha_2^2+K_{11}y_1y_2\\alpha_1\\alpha_2+\\nu_1y_1\\alpha_1+\\nu_2y_2\\alpha_2+\\varphi_{constant}-\\alpha_1-\\alpha_2）$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\because \\alpha_1y_2+\\alpha_2y_2=-\\sum_{i=3}^n\\alpha_iy_i, \\because y_1y_1=1$\n",
    "\n",
    "$\\therefore \\alpha_1y_1y_1+\\alpha_2y_2y_1=-y_1\\sum_{i=3}^n\\alpha_iy_i$\n",
    "\n",
    "$\\therefore \\Rightarrow \\alpha_1+y_1y_2\\alpha_2=-y_1\\sum_{i=3}^n\\alpha_iy_i$\n",
    "\n",
    "设 $S=y_1y_2$, $\\omega=-y_1\\sum_{i=3}^n\\alpha_iy_i, \\therefore \\alpha_1+S\\alpha_2=\\omega=\\alpha_1^*+S\\alpha_2^*$, $\\alpha_i^*$是经过优化后的新的$\\alpha_i$的值\n",
    "\n",
    "$\\therefore \\psi=\\frac{1}{2}K_{11}(\\omega-S\\alpha_2)^2+\\frac{1}{2}K_{22}\\alpha_2^2+K_{12}y_1y_2(\\omega-S\\alpha_2)\\alpha_2+\\nu_1y_1(\\omega-S\\alpha_2)+\\nu_2y_2\\alpha_2+\\varphi_{constant}-(\\omega-S\\alpha_2)-\\alpha_2$\n",
    "\n",
    "$=\\frac{1}{2}K_{11}(\\omega-S\\alpha_2)^2+\\frac{1}{2}K_{22}\\alpha_2^2+K_{12}S(\\omega-S\\alpha_2)\\alpha_2-\\omega+S\\alpha_2+\\nu_1y_1(\\omega-S\\alpha_2)+\\nu_2y_2\\alpha_2+\\varphi_{constant}-\\alpha_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设$\\frac{\\partial{\\psi}}{\\partial{\\alpha_2}}=0$\n",
    "\n",
    "$-K_{11}\\omega{S}+K_{11}S^2\\alpha_2+K_{22}\\alpha_2+K_{12}S\\omega-2K_{12}S^2\\alpha^2+S-\\nu_1y_1S+\\nu_2y_2-1=0$\n",
    "$\\because S^2=1$\n",
    "\n",
    "$-SK_{11}(\\omega-S\\alpha_2)+K_{22}\\alpha_2+K_{12}S\\omega-2K_{12}\\alpha_2-\\nu_1y_1S+S+\\nu_2y_2-1=0$\n",
    "\n",
    "$\\alpha_2(K_{11}+K_{22}-2K_{12})=S(K_{11}-K_{12})\\omega+y_2(\\nu_1-\\nu_2)+1-S$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们需要的是不停的迭代$\\alpha_i$，所以我们需要找到$\\alpha_2$与$\\alpha_2^*$之间的关系，将$\\omega=\\alpha_1^*+S\\alpha_2^*$代入，则：\n",
    "\n",
    "$S(K_{11}-K_{12})\\omega=S(K_{11}-K_{12})(\\alpha_1^*+S\\alpha_2^*)$\n",
    "\n",
    "$=S(K_{11}\\alpha_1^*+K_{11}S\\alpha_2^*-K_{12}\\alpha_1^*-K_{12}S\\alpha_2^*)$\n",
    "\n",
    "$=SK_{11}\\alpha_1^*+K_{11}S^2\\alpha_2^*-SK_{12}\\alpha_1^*-K_{12}S^2\\alpha_2^*$\n",
    "\n",
    "$=SK_{11}\\alpha_1^*-SK_{12}\\alpha_1^*+K_{11}\\alpha_2^*-K_{12}\\alpha_2^*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\because \\nu_1=\\mu_1+b^*-y_1\\alpha_1^*K_{11}-y_2\\alpha_2^*K_{21}, \\nu_2=\\mu_2+b^*-y_2\\alpha_2^*K_{12}-y_2\\alpha_2^*K_{22}$\n",
    "\n",
    "代入$\\nu_1-\\nu_2$\n",
    "\n",
    "$\\mu_1+b^*-y_1\\alpha_1^*K_{11}-y_2\\alpha_2^*K_{21}-(\\mu_2+b^*-y_2\\alpha_2^*K_{12}-y_2\\alpha_2^*K_{22})$\n",
    "\n",
    "$=\\mu_1+b^*-y_1\\alpha_1^*K_{11}-y_2\\alpha_2^*K_{21}-\\mu_2-b^*+y_2\\alpha_2^*K_{12}+y_2\\alpha_2^*K_{22}$\n",
    "\n",
    "$=\\mu_1-\\mu_2-y_1\\alpha_1^*K_{11}-y_2\\alpha_2^*K_{21}+y_2\\alpha_2^*K_{12}+y_2\\alpha_2^*K_{22}$\n",
    "\n",
    "$\\therefore y_2(\\nu_1-\\nu_2)=y_2(\\mu_1-\\mu_2-y_1\\alpha_1^*K_{11}-y_2\\alpha_2^*K_{21}+y_1\\alpha_1^*K_{12}+y_2\\alpha_2^*K_{22})$\n",
    "\n",
    "$=y_2\\mu_1-y_2\\mu_2-S\\alpha_1^*K_{11}-K_{21}\\alpha_2^*+S\\alpha_1^*K_{12}+K_{22}\\alpha_2^*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$S(K_{11}-K_{12})\\omega+y_2(\\nu_1-\\nu_2)+1-S=SK_{11}\\alpha_1^*-SK_{12}\\alpha_1^*+K_{11}\\alpha_2^*-K_{12}\\alpha_2^*+y_2\\mu_1-y_2\\mu_2-S\\alpha_1^*K_{11}-K_{21}\\alpha_2^*+S\\alpha_1^*K_{12}+K_{22}\\alpha_2^*+1-S$\n",
    "\n",
    "$=\\alpha_1^*(SK_{11}-SK_{12}-SK_{11}+SK_{12})+\\alpha_2^*(K_{11}-K_{12}-K_{21}+K_{22})+y_2\\mu_1-y_2\\mu_2+1-S$\n",
    "\n",
    "$\\alpha_2^*(K_{11}-K_{12}-K_{21}+K_{22})+y_2\\mu_1-y_2\\mu_2+y_2y_2-y_1y_2$\n",
    "\n",
    "$\\therefore \\alpha_2^{new,unclipped}=\\alpha_2^{old}+\\frac{y_2(E_1-E_2)}{\\eta}$\n",
    "\n",
    "其中，$\\eta=K_{11}-2K_{12}+K_{22}$, $E_i=f(x_i)-y_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\because 0\\leq \\alpha_i\\leq C$\n",
    "\n",
    "$\\therefore$如果$y_1y_2=-1$,则$max=(0, \\alpha_i^{old}-\\alpha_j^{old})$;$min=(C,C+\\alpha_i^{old}-\\alpha_j^{old})$\n",
    "\n",
    "如果$y_1y_2=1$,则$max=(0, \\alpha_i^{old}+\\alpha_j^{old}-C)$;$min=(C, \\alpha_i^{old} + \\alpha_j^{old})$\n",
    "\n",
    "根据这两个条件，可以真正求出一个$\\alpha_i^{new, clipped}$,这个就是我们优化后的$\\alpha_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##6.6 具体做法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}